---
title: "Model_Assumptions_and_Impact_on_Decisions_Draft"
output: 
 learnr::tutorial:
  progressive: true
  allow_skip: true
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
#install.packages("remotes", repos = "https://cloud.r-project.org/")


options(tutorial.storage = list(
  save_object = function(...){},
  get_object = function(...) NULL,
  get_objects = function(...) list(),
  remove_all_objects = function(...){}))
options(repos = "https://cloud.r-project.org/")


#library(remotes)
#remotes::install_github("rstudio/learnr", force=TRUE)
#remotes::install_github("rstudio/gradethis", force =TRUE)

#install.packages("mplot")
library(tidyverse)
library(learnr)
library(ggplot2)
library(gradethis)
library(dplyr)
library(mplot)
library(gridExtra)


set.seed(123)
data(fev)
FEV <- fev
fev_model <- lm(fev ~ smoke + height, data = FEV)

```




## Introduction

Welcome to Module X: **Model Assumptions and Thier Impacts on Decision Making**. In this module, you will learn: 

* To use R for exploratory analysis to see predictor-response relationships.

* To use R to construct residual plots, test assumptions, and use findings to refit models.

* To use R to apply appropriate transformations to correct model assumptions.

* To use your judgement in knowing when to remove predictors even if p-values suggest predictors are statistically insignificant (**not sure about this one :p**)



Remember that if you get stuck on a coding exercise, you can click the **Hints** button for help. 

## Motivation

We have already seen how to test linear model assumptions and the consequences of violating them in a previous module.

However, moving along this module, you'll come to realise that data doesn't always work the way you'd want it to! But don't fret, there are methods such as **variable transformations** that help us resolve issues with **assumption violations**. If variable transformations correct assumptions, we can apply our linear modelling knowledge and construct valid models. 

As such, in this module we will explore how we can work with data offering non-linear relationships, apply variable tranformations while testing model assumptions, and how to correctly interpret coefficients in the transformed model.

## Section 0: EDA - Summarising Data & Understanding Context

For the purpose of this module, we will work with the Forced Expiratory Volume dataset (called `FEV`). For context, this dataset contains information about 654 individuals on 5 variables. The response variable of interest will be `fev` which is the forced expiratory volume (liters); roughly the amount of air an individual can exhale in the first second of a forceful breath.

We have a number of different possible predictors in our data:

 - `smoke`: An indicator/binary variable where a nonsmoker is 0, and smoker is 1.
 - `sex`: An indicator/binary variable where a female is 0, and male is 1.
 - `height`: Height (inches)
 - `age`: Age (years)
 
In the code chunk below, summarise the response variable `fev`.

```{r e0_1, exercise=TRUE, exercise.lines=2}
# add your code below
summary(FEV$___)
```

```{r e0_1-solution, eval=FALSE}
# add your code below
#opt1 ----
summary(FEV$fev)

#opt2 ----
fev %>% 
summarise(Count = n(), Mean = mean(fev), "25th" = quantile(fev, 0.25), 
"50th" = median(fev), "75th" = quantile(fev, 0.75))
```

```{r e0_1-check}
grade_this_code()
```

```{r e0_1-hint-1}
#---

# remember you can also summarise using tidyverse:

FEV %>% 
summarise(Count = n(), Mean = mean(fev), "25th" = quantile(fev, ___), 
"50th" = median(fev), "75th" = quantile(fev, ___))


#---
```

In the portion below, you can explore the dataset by reviewing the relationship between `fev` and the predictors, as well as the summary of the predictors themselves.

```{r, context="render", echo=FALSE}
fluidPage(
  titlePanel("Exploratory Data Analysis"),
  

  sidebarLayout(
    sidebarPanel(
      radioButtons("Predictor", "Select Predictor:",
                   choices = c("smoke", "age", "height", "sex"),
                   selected = "smoke"),
      br(),
      
    ), 
    mainPanel(
     tabsetPanel(
        tabPanel("Plot",
                 plotOutput("plot")),
        tabPanel("Summary",
                 verbatimTextOutput("summary"))
    ))))

```


```{r, context="server"}
 output$plot <- renderPlot({
    ggplot(FEV, aes_string(x = input$Predictor, y = "fev")) +
      geom_point(color = "deeppink1") + labs(x = input$Predictor, y = "Forced Expiratory Volume") + theme_minimal()
  })
  output$summary <- renderPrint({
    summary(fev[[input$Predictor]])
  })
```

While exploring you may have noticed some interesting trends among the predictors.

```{r q0_1, echo=FALSE}
question("Which response-predictor relationship(s) appears non-linear?",
     answer("The relationship with `smoke`.", message="Try again! While you are correct that this relationship appears non-linear, you may be missing something else."),
     answer("The relationship with `age`.", message="Try again! the relationship appears linear."),
     answer("The relationship with `height`.", message="While you are correct that this relationship appears non-linear, you may be missing something else."),
     answer("The relationship with `sex`.", message="Try again! the relationship appears linear."),
     answer("They all appear linear.", message="Try again! Not all the relationships are linear."),
     answer("More than 1.", message="Correct! Both relationships of `fev` with `smoking` and `height` appear non-linear. ", correct=T),
     allow_retry = T)
```

Despite the well-established history of smoking and its adverse health effects, our exploratory analysis challenges the intuitive expectation of a negative linear relationship between smoking (`smoke`) and Forced Expiratory Volume (`fev`). Contrary to expectations, individuals who smoke exhibit FEV values that are distributed across the 50th percentile and above, extending even to the 75th percentile of `fev`! Surprisingly, we also observe considerable variability in `fev` among non-smokers.

____________________________

*As a rule of thumb*, for non-linear response-predictor relationships, it is worth seeing if other predictors may have underlying influences in the relationship.

Therefore, create a scatterplot in the code chunk below between `height` and `fev` while colour coding for `smoke` status. This would help test if the relationship between the response and `height` varies by the smoker group.


```{r e0_2, exercise=TRUE, exercise.lines=4}
# add your code below
ggplot(FEV, aes(x = ___, y = fev, color = ___)) +
       geom_point(size = 1) + theme_minimal() + labs(color = "Smoking Status") + geom_smooth(method = "lm", se = FALSE, aes(group = factor(___), color = factor(___))) +
+ scale_color_manual(values = c("0" = "turquoise2", "1" = "deeppink1")) 
```

```{r e0_2-solution, eval=FALSE}
# add your code below
ggplot(FEV, aes(x = height, y = fev, color = factor(smoke))) +
       geom_point(size = 1) + theme_minimal() + labs(color = "Smoking Status") + scale_color_manual(values = c("0" = "turquoise2", "1" = "deeppink1")) + geom_smooth(method = "lm", se = FALSE, aes(group = factor(smoke), color = factor(smoke)))
```

```{r e0_2-check}
grade_this_code()
```

```{r e0_2-hint-1}
#---

#Given that we group by a binary variable, use:
color = factor(___)

#---
```

It appears that smoker status does influence the `fev ~ height` relationship as apparent in the plot above. If you were to draw lines of best fit separately for smokers and non smokers, the intercepts would appear different. This motivates us to build a model with `smoke` as an indicator, to capture a main effect.


## Section 1:
Start by building a linear model using fev as the response variable and smoke and height as predictors. This model will help us understand how smoking status and height can predict lung function, measured by forced expiratory volume (FEV).

```{r e1s2, exercise=TRUE, exercise.lines=4}
```

```{r e1s2-hint-1}
#---

# remember to use this form:

fev_model <- lm(response variable ~ predictor1 + predictor2, data = FEV)

# Summarizing the model
summary(fev_model)
#---
```

```{r e1s2-solution, eval=FALSE}
fev_model <- lm(fev ~ smoke + height, data = FEV)

# Summarizing the model
summary(fev_model)
```

```{r e1s2-check}
grade_this_code()
```

Great job! You've just built your first model! Understanding your model's summary can provide insights into how significant each predictor is for FEV. Now that you've built your model, it's crucial to check the linear regression assumptions. We'll start by plotting residuals versus fitted values. This plot helps us identify any patterns that might suggest our model is not fitting the data well (like a curving or fanning pattern).


```{r e2s2, exercise=TRUE, exercise.lines=4}
plot()
```

```{r e2s2-hint-1}
#---

# remember to include your model name in this form:

plot( _ $fitted.values, resid(), 
     xlab = "Fitted values", ylab = "Residuals",
     main = "Residuals vs Fitted Plot")
abline(h = 0, col = "red")
#---
```

```{r e2s2-solution, eval=FALSE}
plot(fev_model$fitted.values, resid(fev_model), 
     xlab = "Fitted values", ylab = "Residuals",
     main = "Residuals vs Fitted Plot")
abline(h = 0, col = "red")
```

```{r e2s2-check}
grade_this_code()
```

```{r q1_2, echo=FALSE}
question("Which pattern do you observe in the Residuals vs Fitted plot?",
     answer("There is a linear pattern.", message="Try again! While the red line is flat, this doesn't mean that the relationship is linear."),
     answer("There is a fanning pattern", message="Yes! We observe that as the x-variable increases, the y-variable starts to fan away in both vertical directions. We will see how we can apply transformations to fix this.", correct= T),
     answer("There is no relationship depicted", message="Try again, there is a specific pattern that you should notice in the plot."),
     allow_retry = T)
```


In your regression analysis, you observed a curving pattern in the plot of residuals versus fitted values, as well as in the scatter plot of height against FEV. This curvature suggests that the relationship between height and FEV is not simply linear – as height increases, the effect on FEV does not increase at a constant rate.

Including a squared term for height in your model (height²) allows you to model this curvature. Essentially, this squared term helps the model to adjust for changes in the slope of the relationship between height and FEV. It accounts for the possibility that FEV increases at a different rate when height changes, which a simple linear term (just height) cannot adequately capture.


```{r e3s2, exercise=TRUE, exercise.lines=10}
# Adding height squared to the model
FEV$height2 <- FEV$height^2
fev_model_2 <- lm(fev ~ smoke + height + height2, data = FEV)

# Summarizing the new model
summary(fev_model_2)

plot()

```

```{r e3s2-hint-1}
#---

FEV$height2 <- FEV$height^2
fev_model_2 <- lm(fev ~ smoke + height + __, data = FEV)

# Summarizing the new model
summary()

plot(__$fitted.values, resid(__), 
     xlab = "Fitted values", ylab = "Residuals",
     main = "Residuals vs Fitted Plot for Modified Model")
abline(h = 0, col = "red")

#---
```

```{r e3s2-solution, eval=FALSE}
FEV$height2 <- FEV$height^2
fev_model_2 <- lm(fev ~ smoke + height + height2, data = FEV)

# Summarizing the new model
summary(fev_model_2)

plot(fev_model_2$fitted.values, resid(fev_model_2), 
     xlab = "Fitted values", ylab = "Residuals",
     main = "Residuals vs Fitted Plot for Modified Model")
abline(h = 0, col = "red")
```

```{r e3s2-check}
grade_this_code()
```

Now, we'll use another method to address the fanning issue observed in the first residuals vs. fitted values plot. Fanning occurs when the variance of residuals changes with the level of the fitted values, which can violate the homoscedasticity assumption of linear regression. To correct this, we will use a Box-Cox transformation, which can help stabilize variance across different levels of fitted values.
#boxcox (won't show up when I run the document)
```{r}
#library(MASS) 
#boxcox_result <- boxcox(fev_model_2, lambda = seq(-2, 2, by = 0.1)) 
#plot(boxcox_result)
```
After transforming fev, rebuild your model and check the residuals vs. fitted values plot again. This step is crucial to ensure that the transformation has effectively addressed the issue of fanning. Well done! Applying transformations like these can greatly improve your model's accuracy and the reliability of your inferences. Continue to explore and refine as necessary.

```{r e4s2, exercise=TRUE, exercise.lines=10}
FEV$log_fev <- log(FEV$fev)

# Refit the model with transformed FEV
fev_model_3 <- lm(log_fev ~ smoke + height + I(height^2), data = FEV)

# Summarizing the new model
summary(fev_model_3)

plot()
```

```{r e4s2-hint-1}
FEV$log_fev <- log(FEV$fev)

# Refit the model with transformed FEV
fev_model_3 <- lm(__ ~ smoke + height + __, data = FEV)

# Summarizing the new model
summary(__)

plot(__$fitted.values, resid(__), 
     xlab = "Fitted values", ylab = "Residuals",
     main = "Residuals vs Fitted Plot for Transformed Model")
abline(h = 0, col = "red")
```

```{r e4s2-solution, eval=FALSE}
FEV$log_fev <- log(FEV$fev)

# Refit the model with transformed FEV
fev_model_3 <- lm(log_fev ~ smoke + height + I(height^2), data = FEV)

# Summarizing the new model
summary(fev_model_3)

plot(fev_model_3$fitted.values, resid(fev_model_3), 
     xlab = "Fitted values", ylab = "Residuals",
     main = "Residuals vs Fitted Plot for Transformed Model")
abline(h = 0, col = "red")

```

## Section 2:




## Section 3:

Outline:

Recap the transformed model. Interpreting coefficients in a transformed model may seem tricky, but it isn't so different from what we've been doing.

First, we will interpet the smoking coefficient in both the simple linear model and the model with smoking and height as predictors. Notice the simple model says smoking increases lung function. 

In the code chunk below, run summaries of the original model and the transformed model. 

< code chunk > 

< MCQ >: Which of the following statements are true? 
[statements about interpreting the smoking coefficient, in the context of height]


Our current models seem to be telling us that smoking either has a positive impact on lung function or no impact at all. However, this contradicts our prior knowledge about the negative effects of smoking on lung health. Sometimes when a model contradicts your beliefs, it means that there is sufficient evidence against that hypotheses. Tie this back to linear regression assumption that you are including all relevant variables, and not missing any important variables. (the Linearity assumption requires that we include predictors present in the true relationship in the linear model)

(think about model in the greater context: how strong is the existing evidence vs what the model is telling you -- use your brain, be critical)


Let's take another look at variables in our dataset, and see if there is anything else that we believe may impact lung function. We have prior knowledge that age can generally impact health, including lung function, so maybe this would be a good variable to include in our model. 

In the code chunk below, add age as a predictor into the transformed model. 
<code chunk> 

< MCQ > : what conclusions can you draw from this model? 

Wrap-up about context 


interactive: options to select transformations, highlighting what is happening to estimate/significance for smoking, show residual

apply transformation -- compare residual and what happens to the coefficient


## Section 4: Interactive Exercise


```{r, context="render", echo=FALSE}
fluidPage(
  titlePanel("Interaction"),
  
  # Main layout with split columns
  splitLayout(
    
    # Left side with variable plots
    cellWidths = c("50%", "50%"),
    plotOutput("variable_plot"),
    
    # Right side with residual plot
    plotOutput("residual_plot")
  ),
  
  # Model selection dropdown
  selectInput("model", "Select Model:", 
              choices = c("fev ~ height + smoke",
                          "fev ~ height+ height2 + smoke",
                          "log_fev ~ height + smoke",
                          "log_fev ~ height+ height2 + smoke")
  )
)
```

```{r, context="server"}
### needs work

```


