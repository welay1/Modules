---
title: "Model Assumptions and Impact on Decisions"
output: 
 learnr::tutorial:
  progressive: true
  allow_skip: true
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
install.packages("remotes", repos = "https://cloud.r-project.org/")


options(tutorial.storage = list(
  save_object = function(...){},
  get_object = function(...) NULL,
  get_objects = function(...) list(),
  remove_all_objects = function(...){}))
options(repos = "https://cloud.r-project.org/")


#library(remotes)
#remotes::install_github("rstudio/learnr", force=TRUE)
#remotes::install_github("rstudio/gradethis", force =TRUE)

#install.packages("mplot")
library(tidyverse)
library(learnr)
library(ggplot2)
library(gradethis)
library(dplyr)
library(mplot)
library(MASS)

set.seed(123)
data(fev)
FEV <- fev
fev_model <- lm(fev ~ smoke + height, data = FEV)

```




## Introduction

Welcome to Module X: **Model Assumptions and Thier Impacts on Decision Making**. In this module, you will learn: 

* To use R for exploratory analysis to see predictor-response relationships.

* To use R to construct residual plots, test assumptions, and use findings to refit models.

* To use R to apply appropriate transformations to correct model assumptions.

* To use your judgement in knowing when to remove predictors even if p-values suggest predictors are statistically insignificant (**not sure about this one :p**)



Remember that if you get stuck on a coding exercise, you can click the **Hints** button for help. 

## Motivation

We have already seen how to test linear model assumptions and the consequences of violating them in a previous module.

However, moving along this module, you'll come to realise that data doesn't always work the way you'd want it to! But don't fret, there are methods such as **variable transformations** that help us resolve issues with **assumption violations**. If variable transformations correct assumptions, we can apply our linear modelling knowledge and construct valid models. 

As such, in this module we will explore how we can work with data offering non-linear relationships, apply variable tranformations while testing model assumptions, and how to correctly interpret coefficients in the transformed model.

## Section 0: EDA - Summarising Data & Understanding Context

For the purpose of this module, we will work with the Forced Expiratory Volume dataset (called `FEV`). For context, this dataset contains information about 654 individuals on 5 variables. The response variable of interest will be `fev` which is the forced expiratory volume (liters); roughly the amount of air an individual can exhale in the first second of a forceful breath.

We have a number of different possible predictors in our data:

 - `smoke`: An indicator/binary variable where a nonsmoker is 0, and smoker is 1.
 - `sex`: An indicator/binary variable where a female is 0, and male is 1.
 - `height`: Height (inches)
 - `age`: Age (years)
 
In the code chunk below, summarise the response variable `fev`.

```{r e0_1, exercise=TRUE, exercise.lines=2}
# add your code below
summary(FEV$___)
```

```{r e0_1-solution, eval=FALSE}
# add your code below
#opt1 ----
summary(FEV$fev)

#opt2 ----
fev %>% 
summarise(Count = n(), Mean = mean(fev), "25th" = quantile(fev, 0.25), 
"50th" = median(fev), "75th" = quantile(fev, 0.75))
```

```{r e0_1-check}
grade_this_code()
```

```{r e0_1-hint-1}
#---

# remember you can also summarise using tidyverse:

FEV %>% 
summarise(Count = n(), Mean = mean(fev), "25th" = quantile(fev, ___), 
"50th" = median(fev), "75th" = quantile(fev, ___))


#---
```

In the portion below, you can explore the dataset by reviewing the relationship between `fev` and the predictors, as well as the summary of the predictors themselves.

```{r, context="render", echo=FALSE}
fluidPage(
  titlePanel("Exploratory Data Analysis"),
  

  sidebarLayout(
    sidebarPanel(
      radioButtons("Predictor", "Select Predictor:",
                   choices = c("smoke", "age", "height", "sex"),
                   selected = "smoke"),
      br(),
      
    ), 
    mainPanel(
     tabsetPanel(
        tabPanel("Plot",
                 plotOutput("plot")),
        tabPanel("Summary",
                 verbatimTextOutput("summary"))
    ))))

```


```{r, context="server"}
 output$plot <- renderPlot({
    ggplot(FEV, aes_string(x = input$Predictor, y = "fev")) +
      geom_point(color = "deeppink1") +
      labs(x = input$Predictor, y = "Forced Expiratory Volume") + theme_minimal()
  })
  output$summary <- renderPrint({
    summary(fev[[input$Predictor]])
  })
```

While exploring you may have noticed some interesting trends among the predictors.

```{r q0_1, echo=FALSE}
question("Which response-predictor relationship(s) appears non-linear?",
     answer("The relationship with `smoke`.", message="Try again! While you are correct that this relationship appears non-linear, you may be missing something else."),
     answer("The relationship with `age`.", message="Try again! the relationship appears linear."),
     answer("The relationship with `height`.", message="While you are correct that this relationship appears non-linear, you may be missing something else."),
     answer("The relationship with `sex`.", message="Try again! the relationship appears linear."),
     answer("They all appear linear.", message="Try again! Not all the relationships are linear."),
     answer("None of the above.", message="Correct! Both relationships of `fev` with `smoking` and `height` appear non-linear. ", correct=T),
     allow_retry = T)
```

Despite the well-established history of smoking and its adverse health effects, our exploratory analysis challenges the intuitive expectation of a negative linear relationship between smoking (`smoke`) and Forced Expiratory Volume (`fev`). Contrary to expectations, individuals who smoke exhibit FEV values that are distributed across the 50th percentile and above, extending even to the 75th percentile of `fev`! Surprisingly, we also observe considerable variability in `fev` among non-smokers.

____________________________

*As a rule of thumb*, for non-linear response-predictor relationships, it is worth seeing if other predictors may have underlying influences in the relationship.

Therefore, create a scatterplot in the code chunk below between `height` and `fev` while colour coding for `smoke` status. This would help test if the relationship between the response and `height` varies by the smoker group.


```{r e0_2, exercise=TRUE, exercise.lines=4}
# add your code below
ggplot(FEV, aes(x = ___, y = fev, color = ___)) +
       geom_point(size = 1) + theme_minimal() + labs(color = "Smoking Status") 
+ scale_color_manual(values = c("0" = "turquoise2", "1" = "deeppink1"))
```

```{r e0_2-solution, eval=FALSE}
# add your code below
ggplot(FEV, aes(x = height, y = fev, color = factor(smoke))) +
       geom_point(size = 1) + theme_minimal() + labs(color = "Smoking Status") + scale_color_manual(values = c("0" = "turquoise2", "1" = "deeppink1"))
```

```{r e0_2-check}
grade_this_code()
```

```{r e0_2-hint-1}
#---

#Given that we group by a binary variable, use:
color = factor(___)

#---
```

It appears that smoker status does influence the `fev ~ height` relationship as apparent in the plot above. If you were to draw lines of best fit separately for smokers and non smokers, the intercepts would appear different. This motivates us to build a model with `smoke` as an indicator, to capture a main effect.


## Section 1:
Build a linear model with `fev` as the response and `smoke` and `height` as predictors.
Name your model 'fev_model', and summarize it

```{r e1s2, exercise=TRUE, exercise.lines=4}
"fev_model <- lm(y ~ x1+ x2 ,data =FEV)"
```

```{r e1s2-solution, eval=FALSE}
# add your code below
fev_model <- lm(fev ~ smoke + height, data = FEV)

# Summarizing the model
summary(fev_model)
```

```{r e1s2-check}
grade_this_code()
```


```{r e2s2, exercise=TRUE, exercise.lines=4}
plot(fev_model$fitted.values, resid(fev_model), 
     xlab = "Fitted values", ylab = "Residuals",
     main = "Residuals vs Fitted Plot")
abline(h = 0, col = "red")
```

Plot the residuals versus fitted values to investigate assumptions
```{r e2s2-solution, eval=FALSE}
plot(fev_model$fitted.values, resid(fev_model), 
     xlab = "Fitted values", ylab = "Residuals",
     main = "Residuals vs Fitted Plot")
abline(h = 0, col = "red")
```

```{r e2s2-check}
grade_this_code()
```

*how do you link this plot to the height plot?*


## Section 2:

```{r, include=FALSE}
FEV$lnfev <- log(FEV$fev)
log_fev_model <- lm(lnfev ~ smoke + height, data = FEV)
summary(log_fev_model)
```

Recall the transformed model, which has the following summary: 

```{r, echo=FALSE}
summary(log_fev_model)
```

Interpreting the coefficients in this model may seem tricky, but the process is very similar to what you have been doing so far. 

```{r qTransform, echo=FALSE}
question("Which interpretation below is correct?",
     answer("For a one-unit increase in height, FEV decreases by 0.052.", message="Recall the transformation applied and the variable it was applied to."),
     answer("For a one-unit increase in smoke, FEV decreases by 0.007", message="Is the correct way to interpret a binary variable?"),
     answer("For a one-unit increase in ln(height), FEV increases by 0.052", message="Recall the transformation applied and the variable it was applied to."),
     answer("For participants that smoke, ln(FEV) decreases by 0.0068", message="", correct=T),
     allow_retry = T)
```

The important rule when interpreting transformed models is that you must refer to variables in their transformed state. For example, if $x$ has been squared, you must interpret the impact in the context of a "one unit increase in $x^2$", not $x$ alone. 

Now that we know how to interpret the coefficients in `log_fev_model`, we will compare this to the untransformed `fev_model` and the simple model `simple_model` to see what each tells us about the effect of smoking on FEV. 

Run the code chunk below to produce summaries of `log_fev_model` and `fev_model`. 

```{r modCompare, exercise=TRUE, exercise.lines=6}

fev_model <- lm(fev ~ smoke + height, data = FEV)

log_fev_model <- lm(lnfev ~ smoke + height, data = FEV)

simple_model <- ___
  
summary(fev_model)
summary(log_fev_model)
summary(simple_model)
```


```{r modCompare-solution, eval=FALSE}

fev_model <- lm(fev ~ smoke + height, data = FEV)

log_fev_model <- lm(lnfev ~ smoke + height, data = FEV)

simple_model <- lm(fev ~ smoke, data=FEV)
  
summary(fev_model)
summary(log_fev_model)
summary(simple_model)
```

```{r mcqCompare, echo=FALSE}
question("Which of the following statements is correct",
     answer("The simple model states that smoking statistically increases lung function.", message=""),
     answer("The transformed model states that in the presence of height, smoking decreases lung function, but this effect is not significant.", message=""),
     answer("The untransformed model states that in the presence of height, smoking increases lung function, but this effect is not significant.", message=""),
     answer("All of the above are true", correct=T),
     allow_retry = T)
```

Another commonality is that all three models present findings that contradict what existing evidence overwhelmingly corroborates: that smoking has a **significant, negative impact** on lung function. <should I provide links?>

Sometimes when a model contradicts existing evidence or prior beliefs, it can be a sign that you have discovered a previously unknown relationship, and that there may be strong evidence against that preexisting information. However, when a model contradicts a relationship as well-studied and well-documented as smoking and lung function, this may be a sign that we should revisit our model and ensure that we are truly meeting all model assumptions. 

Recall the Linearity/Model Specification Assumption, which requires that the model as fitted to the sample is in fact the true relationship in the population, with no missing or extraneous predictors. If the model you are building is not corroborating very well-researched facts, this may be a sign that you are violating linearity by either including unnecessary predictors or excluding something important. 

Let's revisit the `FEV` dataset to see if there are any other variables we should include in this model. Run the chunk below to take a glimpse at the variables in `FEV`. 


```{r viewData, exercise=TRUE, exercise.lines=6}
head(FEV)
```

Using general knowledge, we can reasonably conclude that lung capacity is impacted by smoking and height, which means including this information in our model is probably important. Additionally, it is reasonable to assume that age also impacts lung capacity, as we know our health changes in many ways as we age. So, it is possible that `age` is a variable that should be included in our transformed model. 

In the code chunk below, add `age` to the transformed model and run a summary. 

```{r addAge, exercise=TRUE, exercise.lines=6}
log_fev_model_age <- lm()
summary(log_fev_model_age)
```


```{r addAge-solution, eval=FALSE}
log_fev_model_age <- lm(lnfev ~ smoke + height + age, data=FEV)
summary(log_fev_model_age)
```

Looking at the model summary, we can see that including age as a predictor means that smoking now has a significant, negative effect on the natural log of lung capacity, which corroborates what we would expect to find in existing research. 

<blurb about thikning about the model in the greater context: how strong is the existing evidence vs what the model is telling you -- use your brain, be critical >

