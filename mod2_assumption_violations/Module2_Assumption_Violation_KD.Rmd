---
title: 'STA302 Module 3: Assumption Violation'
output: learnr::tutorial
#css: "css/learnr_302.css"
runtime: shiny_prerendered
---

```{r setup, include = FALSE}
# These are the packages you will need for this activity
packages_needed <- c("tidyverse","learnr", "MASS")

package.check <- lapply(
  packages_needed,
  FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE, repos = "http://cran.us.r-project.org")
    }
  }
)

# Credit: package.check based on a helpful post from Vikram Baliga https://vbaliga.github.io/verify-that-r-packages-are-installed-and-loaded/

remotes::install_github("rstudio/gradethis")


library(tidyverse)
library(learnr)
gradethis::gradethis_setup()


knitr::opts_chunk$set(echo = FALSE)

options(tutorial.storage = list(
  save_object = function(...){},
  get_object = function(...) NULL,
  get_objects = function(...) list(),
  remove_all_objects = function(...){}))

set.seed(302)
```


## Introduction

In this module, you will: 

* Investigate the consequences of violating the assumptions of linear regression, specifically confidence intervals of coefficients. 

* Run a simulation to see how different samples produce different estimates. 

## Section 1 - Setting up a simulation

First, we will set up a simulation. We will set the simulation up such that all assumptions of linear regression will hold. We will then deliberately break each assumption and explore the effects. 

Usually, we do not know the true population relationship -- the whole purpose of linear regression is to *estimate* this relationship. However, for the purpose of this exercise, we will assume we do know this information, as we will be replicating drawing samples from some population. The steps of the simulation will be as follows: 

1. We will establish the "ground truth" -- the true population relationship between the predictors and response. 

2. Then, we take many samples from this population to obtain an estimate of the sampling distribution of each coefficient. 

3. Finally, we compute our confidence interval for each coefficient in each sample we take and calculate the **<mark>coverage</mark>**, or the proportion of intervals that contain the true parameter value. 

Why do we care about the coverage? Well, recall how we define a 95% confidence interval: *given that the assumptions of linear regression are met*, if we were to take many samples from a population and calculate a 95% confidence interval for each sample, then approximately 95% of the intervals would contain the true population parameter. Therefore, calculating the coverage for a simulation can tell us whether the linear regression assumptions are met. 

### Part 1.1: Sampling from the population

We will set up our first sample such that all linear regression assumptions are met. We will assume the following regression model is the true population relationship: 

$$Y = 1 + 2X_1 + 3X_2 + \epsilon, \text{     where   } \epsilon \sim N(0,1).$$
Our first step is generating a sample of size $n=50$ from this population and fitting a linear model to that sample. This will give us estimates of the coefficients of $X_1$ and $X_2$. 

Using the information above, fill in the blanks in the code to sample from the population and fit a linear model. 

```{r sim1-setup, include=F}
set.seed(302)
```

```{r sim1, exercise = TRUE, exercise.lines = 20}
# Set up parameters of the population errors
error_mean = 0
error_sd = 1
sample_size = 50

# Generate a sample from the true distribution of errors
e <- rnorm(___, ___, ___)

# Generate the predictors
x1<-  rnorm(sample_size, mean = 0, sd = 5)
x2<-  rnorm(sample_size, mean = 1, sd = 1)

# use the predictors and the errors to get your response
y <- ___

#Fit a linear regression model and summarize the output
model <- lm(___)
summary(model)
```

```{r sim1-hint-1}
# The syntax of the rnorm function (for getting Normal errors) is
rnorm(sample size, mean, sd)

# It's a good idea to use the names we gave to the parameters in the first lines of the code (see "set up parameters...")
```

```{r sim1-hint-2}
# The syntax of the rnorm function (for getting Normal errors) is
rnorm(sample size, mean, sd)

# It's a good idea to use the names we gave to the parameters in the first lines of the code (see "set up parameters...")

# Note that Y is generated using the relationship noted in the instructions: 
Y = 1 + 2*x1 + 3*x2 + e

```

```{r sim1-hint-3}
# The syntax of the rnorm function (for getting Normal errors) is
rnorm(sample size, mean, sd)

# It's a good idea to use the names we gave to the parameters in the first lines of the code (see "set up parameters...")

# Note that Y is generated using the relationship noted in the instructions: 
Y = 1 + 2*x1 + 3*x2 + e

# You may find the following function helpful when fitting linear regression: 
lm(response~predictors)
```

```{r sim1-solution}
# Set up parameters of the population errors
error_mean = 0
error_sd = 1
sample_size = 50

# Generate a sample from the true distribution of errors
e <- rnorm(sample_size, error_mean, error_sd)

# Generate the predictors
x1<-  rnorm(sample_size, mean = 0, sd = 5)
x2<-  rnorm(sample_size, mean = 1, sd = 1)

# use the predictors and the errors to get your response
y <- 1 + 2*x1 + 3*x2 + e

#Fit a linear regression model and summarize the output
model <- lm(y~x1+x2)
summary(model)

```

```{r sim1-code-check}
grade_code()
```

```{r q1, include=TRUE}
question("Based on the summary for your model, which of the following statements is correct?",
         answer("The residual standard error has 48 degrees of freedom.", message="The degrees of freedome for the residual standard error is 50-2-1 or 47."),
         answer(sprintf("We have no evidence against the claim that the coefficient for $X_1$ is 0."), message = sprintf("Since the p-value for $X_1 < 2e-16$, this tells us that we have evidence that the slope of $X_1$ is not 0.")),
         answer(sprintf("We have strong evidence that the true coefficient for $X_1$ is 0. "), message = sprintf("Since the p-value for $X_1 < 2e-16$, this tells us that we have evidence that the slope of $X_1$ is not 0.")),
         answer(sprintf("We have strong evidence against the claim that the coefficient for $X_2$ is 0."), correct = TRUE,
         message = sprintf("Since the p-value for $X_2 < 2e-16$, this tells us that $X_2$ is linearly related to $Y$ and so the true slope of $X_2$ is not 0.")), allow_retry = TRUE
)
```

### Part 1.2: Repeat Sampling from the Population Many Times

To understand how confidence intervals *should* behave if all assumptions hold, we will take 2000 samples from the population. We will then compute confidence intervals for $X_1$ and $X_2$ for each sample, resulting in 2000 intervals for each predictor. We can then see the *coverage* of the interval. 

We will generate 2000 samples using a *for loop*. This allows us to repeat the same process for a desired number of times. In each loop, we will take a sample, fit a model, and save the 95\% confidence interval for each coefficient. 

Fill in the gaps below to run 2000 iterations of the simulation and save the resulting confidence interval of each coefficient for each iteration.

```{r confint1-setup, include=F}
set.seed(302)
```

```{r confint1, exercise = TRUE, exercise.lines = 26}
# set up the parameters like before
error_mean = 0
error_sd = 1
sample_size = 50
# include how many samples you want to take below
num_runs = _____

# Initialize how you will store your results
# Each row of the matrix will store an interval that contains 2 numbers
# We will have as many rows as times we are sampling our population
Beta0CI <- Beta1CI <- Beta2CI <- matrix(NA, nrow=____, ncol = ____)

# the for loop will run the code below as many times as we have samples
for (i in 1:____){
  e <- rnorm(sample_size, mean = error_mean, sd=error_sd)
  x1<-  rnorm(sample_size, mean = 0, sd = 5)
  x2<-  rnorm(sample_size, mean = 1, sd = 1)
  y <- 1 + 2*x1 + 3*x2 + e
  # Fit the model and compute the confidence interval
  model <- lm(y ~ x1 + x2)
  Beta0CI[i,] <- confint(model)[1,]
  Beta1CI[i,] <- ____
  Beta2CI[i,] <- ____
}
```


```{r confint1-hint-1}
# We gave the number of samples a name, num_runs. You can call this in other parts of the code:

num_runs = 2000

for(i in 1:num_runs){
  
}
```

```{r confint1-hint-2}

# We gave the number of samples a name, num_runs. You can call this in other parts of the code:

num_runs = 2000

for(i in 1:num_runs){
  
}

# The following hint might be helpful if you are stuck on nrow and ncol:

# Number of rows (nrow) is the number of times the simulation is repeated

# Number of columns (ncol) is how many values we need to store to represent a confidence interval 
```

```{r confint1-hint-3}
# We gave the number of samples a name, num_runs. You can call this in other parts of the code:

num_runs = 2000

for(i in 1:num_runs){
  
}

# The following hint might be helpful if you are stuck on nrow and ncol:

# Number of rows (nrow) is the number of times the simulation is repeated

# Number of columns (ncol) is how many values we need to store to represent a confidence interval 

# The following command can be used to extract a confidence interval of a model
confint(model)
# and it will give us 2 values in each row, with as many rows as coefficients
```


```{r confint1-solution}
# set up the parameters like before
error_mean = 0
error_sd = 1
sample_size = 50
# include how many samples you want to take below
num_runs = 2000

# Initialize how you will store your results
# Each row of the matrix will store an interval that contains 2 numbers
# We will have as many rows as times we are sampling our population
Beta0CI <- Beta1CI <- Beta2CI <- matrix(NA, nrow=num_runs, ncol = 2)

# the for loop will run the code below as many times as we have samples
for (i in 1:num_runs){
  e <- rnorm(sample_size, mean = error_mean, sd=error_sd)
  x1<-  rnorm(sample_size, mean = 0, sd = 5)
  x2<-  rnorm(sample_size, mean = 1, sd = 1)
  y <- 1 + 2*x1 + 3*x2 + e
  # Fit the model and compute the confidence interval
  model <- lm(y ~ x1 + x2)
  Beta0CI[i,] <- confint(model)[1,]
  Beta1CI[i,] <- confint(model)[2,]
  Beta2CI[i,] <- confint(model)[3]
}
```


```{r confint-code-check}
grade_code()
```


### Part 1.3: Estimating the Confidence Interval Coverage

As a reminder, coverage here refers to the **proportion of confidence intervals based on samples from this population that contain the true value of the coefficient.** If we simulate many 95% confidence intervals from a population, the coverage should be about 95% if all assumptions are met. If we get a percentage very different from our confidence level, that is a clue that assumptions may be violated. 

By taking the 2000 samples from this population and computing confidence intervals each time, we can check if our 95\% confidence intervals actually capture the true value of the coefficient 95\% of the time. So for a set of confidence intervals $[a_1, b_1], [a_2, b_2], [a_3, b_3], ... , [a_{2000}, b_{2000}]$, we find the coverage by 
$$\frac{\sum_{i=1}^{2000}1_{\{\beta \in [a_i, b_i]\}}}{2000}$$, where $1_{\{\beta...}}$ is an indicator function taking $1$ if $\beta$ is in the interval, and $0$ otherwise. 

To code this process, we can break it up into four steps:

   1. Check if the true value of the coefficient is LARGER than the lower bound of the computed confidence interval (i.e. $a \leq \beta$ or equivalently $\beta \geq a$).
  2. Check if the true value of the coefficient is also SMALLER than the upper bound of the computed confidence interval (i.e. $b \geq \beta$ or equivalently $\beta \leq b$). 
   3. Count/Sum the number of simulated samples where the true value of the coefficient was BOTH larger than the lower bound and smaller than the upper bound of the confidence interval.
  4. Divide by the number of samples you took, i.e. the number of loops you did in your simulation.

Below is an example of how the coverage is computed for the confidence interval of the intercept. `print()` just prints the results, and `paste0()` just lets us add a label of what is being printed to the output. Use the code for the intercept to help you fill in the missing pieces for estimating the coverage for the slope of $X_1$ and the slope of $X_2$.

```{r confint1_1-setup, include=F}
set.seed(302)
error_mean = 0
error_sd = 1
sample_size = 50
# include how many samples you want to take below
num_runs = 2000

# Initialize how you will store your results
# Each row of the matrix will store an interval that contains 2 numbers
# We will have as many rows as times we are sampling our population
Beta0CI <- Beta1CI <- Beta2CI <- matrix(NA, nrow=2000, ncol = 2)

# the for loop will run the code below as many times as we have samples
for (i in 1:2000){
  e <- rnorm(sample_size, mean = error_mean, sd=error_sd)
  x1<-  rnorm(sample_size, mean = 0, sd = 5)
  x2<-  rnorm(sample_size, mean = 1, sd = 1)
  y <- 1 + 2*x1 + 3*x2 + e
  # Fit the model and compute the confidence interval
  model <- lm(y ~ x1 + x2)
  Beta0CI[i,] <- confint(model)[1,]
  Beta1CI[i,] <- confint(model)[2,]
  Beta2CI[i,] <- confint(model)[3,]
}
```

```{r confint1_1, exercise = TRUE, exercise.setup = "confint1_1-setup"}
# This is how we compute the coverage for the intercept:
print(paste0("Coverage for Intercept: ", sum(Beta0CI[,1] <=1 & Beta0CI[,2] >=1)/num_runs))

# use the same template as aboce to finish the next two lines
print(paste0("Coverage for Slope X1: ", sum(___[,1] <=2 & ___[,2] >=2)/num_runs))
print(paste0("Coverage for Slope X2: ", sum(Beta2CI[,1] <=___ & Beta2CI[,2] >___)/num_runs))
```

```{r confint1_1-hint-1}
# To get the coverage of the intervals, remember that we are checking:

# 1. if the lower value of our interval is less than the true value of the parameter, for example
Beta0CI[,1] <=1 
# 2. AND (&) if it's also true that the upper value of our interval is greater than the true value of the parameter (this is equivalent to then asking is the truth in my interval), for example
Beta0CI[,2]
# 3. then we sum up how many times the interval contains the truth
sum(Beta0CI[,1] <=1 & Beta0CI[,2] >=1)
# 4. and lastly divide by the number of samples we collected to get a proportion
sum(Beta0CI[,1] <=1 & Beta0CI[,2] >=1)/num_runs

```

```{r confint1_1-hint-2}
# Remember what we called the objects that store our confidence intervals:
Beta0CI
Beta1CI
Beta2CI
# each one has 2000 rows of information and 2 columns
```

```{r confint1_1-hint-3}
# The true values of the parameters were used to get the response as
y <- 1 + 2*x1 + 3*x2 + e
```


```{r q2, include=TRUE}
question(sprintf("Based on the output, which of the following ranges does the approximate coverage for the coefficient of $X_1$ fall in?"),
         answer("91%-93%", message="While we are getting random results, we would expect the proportion of intervals that capture the truth to be closer to our confidence level."),
         answer("93%-97%", correct = TRUE, message = "We would expect 95% of the CI captures the true value by definition"),
         answer("97%-99%", message="While we are getting random results, we would expect the proportion of intervals that capture the truth to be closer to our confidence level."),
         answer("99%-100%", message="While we are getting random results, we would expect the proportion of intervals that capture the truth to be closer to our confidence level."),
         allow_retry = TRUE
)
```


## Section 2 - Violated Homoscedasticity/Non-Constant Variance

In this part, we will examine the **<mark>homoscedasticity</mark>** (constant variance) assumption and how the violation of it could impact the confidence intervals we would build for the estimated coefficients.

Let's start by reviewing the definition for the homoscedasticity assumption.  

```{r q3, include=TRUE}
question("Which of the following statements is guaranteed to satisfy the homoscedasticity assumption?",
         answer(sprintf("$E(\\epsilon) = 0$"), message = "The homoscedasticity assumption refers to the variance, not the expectation."),
         answer(sprintf("$Var(\\epsilon \\mid X = 1) = Var(\\epsilon \\mid X = 2) = 1.$"), message = sprintf("This is close to the definition of homoscedasticity but only is true if $x$ only takes the values 1 and 2")),
         answer(sprintf("$Var(\\epsilon \\mid X = x) = \\sigma^2 x^2$ where $\\sigma^2$ is a constant."), message = sprintf("Since this variance contains the term $x^2$, it varies with different values of $x$ so it is not constant.")),
         answer(sprintf("$Var(\\epsilon \\mid X = x) = σ^2.$ where $\\sigma^2$ is a constant."), correct = TRUE,
         message = "The homoscedasticity assumption states that the error variance should not change with the different values of X."), allow_retry = TRUE
)
```


Now let's break the homoscedasticity assumption in our simulation. We will use the same simulation template as the previous section, but instead of having all assumptions hold, we will break constant variance. 

To do this, all we need to change is the distribution that dictates the population errors. Remember that, when all assumptions hold, the population relationship is $$Y = 1 + 2X_1 + 3X_2 + \epsilon, \text{     where   } \epsilon \sim N(0,1)$$ where the constant error variance is satisfied because the normal distribution has a variance of 1, a constant value that does *not* depend on the values of the predictors.

To create non-constant variance, we will make it so that the population relationship is now $$Y = 1 + 2X_1 + 3X_2 + \epsilon, \text{     where   } \epsilon \sim N(0,|X_1|).$$ Notice that as the values of $X_1$ change, so will the variance of the errors, thereby violating constant variance. We can add this into our simulation by setting `sd = abs(x1)` when we generate the errors below.

```{r confint2, exercise = TRUE, exercise.lines = 30}
# Set up parameters
error_mean = 0
num_runs = 2000
sample_size = 50

# Initialize how you will store your results
# Each row of the matrix represents 1 simulation loop
Beta0CI <- Beta1CI <- Beta2CI <- matrix(NA, nrow=num_runs, ncol = 2)

# repeat the commands num_run times
for (i in 1:num_runs){
  # sample the predictors
  x1<-  rnorm(sample_size, mean = 0, sd = 5)
  x2<-  rnorm(sample_size, mean = 1, sd = 1)
  # sample the errors
  # Let's break constant variance by making it depend on the value of x1
  e <- rnorm(sample_size, mean = error_mean, sd=____)
  # sample the responses
  y <- 1 + 2*x1 + 3*x2 + e
  # Fit the model and compute the confidence interval
  model <- lm(___ ~ ____)
  # Store the results so you won't lose it when we run the next run
  Beta0CI[i,] <- confint(model)[1,]
  Beta1CI[i,] <- confint(model)[2,]
  Beta2CI[i,] <- confint(model)[3,]
}
```

```{r confint2-hint-1}
# to create non-constant variance, set
sd = abs(x1)
# in the sampling of the errors on line 17
```

Now that you have your 2000 samples from the population and have fit a linear model to each one and computed the confidence intervals on each coefficient, we can look at whether the coverage of each confidence interval is changed due to the violation of constant variance. Fill in the blanks below to complete the coverage calculation for the CI of the slope of $X_1$ and $X_2$.

```{r confint2_1-setup, include=F}
# Set up parameters
error_mean = 0
num_runs = 2000
sample_size = 50

# Initialize how you will store your results
# Each row of the matrix represents 1 simulation loop
Beta0CI <- Beta1CI <- Beta2CI <- matrix(NA, nrow=num_runs, ncol = 2)

# repeat the commands num_run times
for (i in 1:num_runs){
  # sample the predictors
  x1<-  rnorm(sample_size, mean = 0, sd = 5)
  x2<-  rnorm(sample_size, mean = 1, sd = 1)
  # sample the errors
  # Let's break constant variance by making it depend on the value of x1
  e <- rnorm(sample_size, mean = error_mean, sd=abs(x1))
  # sample the responses
  y <- 1 + 2*x1 + 3*x2 + e
  # Fit the model and compute the confidence interval
  model <- lm(y ~ x1+x2)
  # Store the results so you won't lose it when we run the next run
  Beta0CI[i,] <- confint(model)[1,]
  Beta1CI[i,] <- confint(model)[2,]
  Beta2CI[i,] <- confint(model)[3,]
}
```

```{r confint2_1, exercise=TRUE, exercise.setup="confint2_1-setup"}
# Here is how the coverage is checked for the intercept CI

print(paste0("Coverage for Intercept: ", sum(Beta0CI[,1] <=1 & Beta0CI[,2] >=1)/num_runs))

# fill in the coverage for the two slopes:
print(paste0("Coverage for Slope X1: ", sum(___[,1] <= ___ & ___[,2] >= ___)/num_runs))
print(paste0("Coverage for Slope X2: ", sum(Beta2CI[,___] <= ___ & Beta2CI[,___] >= ___)/num_runs))
```

```{r confint2_1-hint-1}
# Remember what we called the objects that store our confidence intervals:
Beta0CI
Beta1CI
Beta2CI
# each one has 2000 rows of information and 2 columns
```

```{r confint2_1-hint-2}
# Remember that each storage object for the confidence intervals has
    # 2000 rows and 2 columns with lower bound in column 1 and upper in column 2
Beta0CI <- Beta1CI <- Beta2CI <- matrix(NA, nrow=num_runs, ncol = 2)
```

```{r confint2_1-hint-3}
# The true values of the parameters were used to get the response as
y <- 1 + 2*x1 + 3*x2 + e
```


Now that you've computed the coverage of each coefficient's confidence interval under non-constant variance, we need to compare that to what we saw when all assumptions held. Recall that when our assumptions held, we obtained approximately 95\% coverage when we computed a 95\% confidence interval.

```{r q4, include=TRUE}
question(sprintf("Compared to what you saw for the coverage of $X_1$ when all assumptions held, which of the following statements is true?"),
         answer("An error occurs when we try to compute the confidence interval when assumptions don't hold."),
         answer("The coverage is higher when assumptions don't hold."),
         answer("The coverage is lower when assumptions don't hold.", correct = TRUE),
         answer("The coverage is exactly the same when assumptions don't hold.", message = "Since the assumption no longer holds, we would expect to see some change in the coverage. Further, simulations inject randomness so it's very unlikely that the coverage would be exactly the same"),
         allow_retry = TRUE
)
```


## Section 3 - Violated Linearity/Model Misspecification
We just noticed that when we violate our assumption of constant error variance in linear regression, we end up with lower coverage in some of our confidence intervals than we should. In this section, we will examine the linearity/model specification assumption and see how its violation could impact the coverage of our confidence intervals.

Let's start by reviewing the definition for the linearity assumption.  

```{r q5, include=TRUE}
question("Which ONE of the following statements regarding the linearity assumption is incorrect?",
         answer(sprintf("The linearity assumption says the conditional mean of  $Y |X = x$ does not depend on $X$."), correct = TRUE, message="While it can happen that X is not related to the conditional mean of Y, the linearity assumption does not state that this must be true."),
         answer(sprintf("The linearity assumption says the conditional mean of  $Y |X = x$ is linear with respect to $X$. "), message="One phrasing of the linearity assumption is exactly this, that X is related to the conditional mean of Y in a linear fashion through the coefficients."),
         answer("The linearity assumption says that the errors in the population come from a distribution with a mean of zero.", message="This is an alternative phrasing of the linearity assumption, called the mean zero assumption, and is equivalent to option B through properties of linear combinations of Normal random variables."),
         answer("The linearity assumption states that the model as fitted to the sample is in fact the true relationship present in the population.", message="This is another alternative phrasing of the linearity assumption, which says that the fitted model is the true one too, and therefore is not missing any predictors, does not have any predictors that are not needed, and all predictors are in the correct format (e.g. none need to be squared to be the true relationship)."),
         allow_retry = TRUE
)
```

In order to see what happens when the linearity assumption is violated, we can create this violation in our simulation. There are a few ways we can do this:

  1. Have the true relationship contain both predictors but the fitted model contain only one predictor.
  2. Change the true relationship to contain a predictor in a different functional form, e.g. as $X^2$ instead of just $X$.

We will not only look at the impact of this violation on the coverage of our confidence intervals for the coefficients (as before), but also the average value of the estimated coefficients across all runs to better understand any impact we might see.

### Part 3.1: Omitting a predictor
We can start by violating the linearity assumption by omitting a predictor when we fit our model. We know (because we set it to be true) that the true relationship in the population is $E(Y | X) = 1 + 2X_1 + 3X_2$. 

To violate the assumption, we will keep the true relationship as above in our simulation, but when we fit the linear model, we will only use $X_1$ in the model (i.e not include $X_2$ in the fitted model). Then we can compute the estimated coverage as we did before. We will also store the estimated coefficients from each loop in our simulation so that we can compute the average $\beta$ from all 2000 samples.

First, set up the simulation with a true relationship of $y = 1 + 2*X_2 + 3*X_3 + e$ but a fitted linear relationship of $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x_2$.

```{r linearity1, exercise = TRUE, exercise.lines=26}
# Set up parameters
error_mean = 0
num_runs = 2000
sample_size = 50
error_sd = 1
betas <- matrix(NA, nrow=num_runs, ncol=2)
Beta0CI <- Beta1CI <- matrix(NA, nrow=num_runs, ncol = 2)

for (i in 1:num_runs){
  x1<-  rnorm(sample_size, mean = 1, sd = 5)
  x2<-  rnorm(sample_size, mean = 1, sd = 1)
  e <- rnorm(sample_size, mean = error_mean, sd = error_sd)
  # add the true population relationship here
  y <- _____
  #Let's break the linearity assumption by making it only depends on the value of x2
  model <- lm(__________)
  #Store the coefficients of the model in matrix betas
  betas[i,] <- as.numeric(__________)
  # Store the confidence intervals
  Beta0CI[i,] <- confint(model)[1,]
  Beta1CI[i,] <- confint(model)[2,]
}

```

```{r linearity1-hint-1}
# remember that the true relationship is not changing and is
y <- 1 + 2*x1 + 3*x2 + e
```

```{r linearity1-hint-2}
# the model we fit using lm contains the violation and should only have one predictor:
lm(y ~ x2)
```

```{r linearity1-hint-3}
# The following command can be used to obtain the coefficients of a model
coef(model)
# which will only give you the estimated coefficients from the model
```


Now we can compute the estimated coverage of the 95% confidence intervals that were calculated for each sample in our simulation. This is done the same way as the previous section:

 1. Check if the true value of the coefficient is LARGER than the lower bound of the computed confidence interval (i.e. $a \leq \beta$ or equivalently $\beta \geq a$).
  2. Check if the true value of the coefficient is also SMALLER than the upper bound of the computed confidence interval (i.e. $b \geq \beta$ or equivalently $\beta \leq b$). 
   3. Count/Sum the number of simulated samples where the true value of the coefficient was BOTH larger than the lower bound and smaller than the upper bound of the confidence interval.
  4. Divide by the number of samples you took, i.e. the number of loops you did in your simulation.
  
Let's do this below.

```{r confint3-setup, include=F}
# Set up parameters
error_mean = 0
num_runs = 2000
sample_size = 50
error_sd = 1
betas <- matrix(NA, nrow=num_runs, ncol=2)
Beta0CI <- Beta1CI <- matrix(NA, nrow=num_runs, ncol = 2)

for (i in 1:num_runs){
  x1<-  rnorm(sample_size, mean = 1, sd = 5)
  x2<-  rnorm(sample_size, mean = 1, sd = 1)
  e <- rnorm(sample_size, mean = error_mean, sd = error_sd)
  # add the true population relationship here
  y <- 1 + 2*x1 + 3*x2 + e
  #Let's break the linearity assumption by making it only depends on the value of x2
  model <- lm(y ~ x2)
  #Store the coefficients of the model in matrix betas
  betas[i,] <- as.numeric(coef(model))
  # Store the confidence intervals
  Beta0CI[i,] <- confint(model)[1,]
  Beta1CI[i,] <- confint(model)[2,]
}

```

```{r confint3_1, exercise=TRUE, exercise.setup="confint3-setup"}
# Here is how the coverage is checked for the intercept CI

print(paste0("Coverage for Intercept: ", sum(Beta0CI[,1] <=1 & Beta0CI[,2] >=1)/num_runs))

# Fill in the gaps for the coverage of the slope of X2
print(paste0("Coverage for Slope X2: ", sum(___[,___] <= 3 & ___[,___] >= 3)/___))
```

```{r confint3_1-hint-1}
# Remember what we called the objects that store our confidence intervals:
Beta0CI
Beta1CI
# each one has 2000 rows of information and 2 columns
```

```{r confint3_1-hint-2}
# Remember that each storage object for the confidence intervals has
    # 2000 rows and 2 columns with lower bound in column 1 and upper in column 2
Beta0CI <- Beta1CI <- matrix(NA, nrow=num_runs, ncol = 2)
```

```{r q6, include=TRUE}
question("Which confidence interval has reduced coverage (i.e. smaller than 95%)?",
         answer("Neither interval has reduced coverage", message="Check that you have computed the coverage correctly and that you have violated linearity as described."),
         answer(sprintf("The interval for the slope of $X_2$ has reduced coverage"), message="The coverage here should be close to 95%. Check that you have computed the coverage correctly and that you have violated linearity as described."),
         answer("The interval for the intercept has reduced coverage", correct = TRUE, message="You should see that the intercept CI is not quite capturing the truth as often as it should."),
         allow_retry = TRUE
)
```

Let's try to understand why we might be seeing reduced coverage in this situation. To do this, we will take the mean/average of the estimated coefficients from all samples in our simulation. This will give us an idea of whether any of our estimated coefficients appear to be **biased**.

```{r bias1, exercise=TRUE, exercise.setup="confint3-setup"}
# take the mean() of each COLUMN of our matrix of betas
print(paste0("Average value of the estimated intercept: ", mean(___)))
print(paste0("Average value of the estimated slope: ", mean(___)))
```

```{r bias1-hint-1}
# remember that we stored our estimated coefficients in
betas
# where column 1 stored all the intercepts and column 2 all the slopes
```

```{r q7, include=TRUE}
question("Based on these averages, which of the following statements is TRUE?",
         answer("The coefficients all appear to be unbiased, i.e. the average is roughly the same as the true parameter value.", message="Double check your simulation, as we should see an issue with bias since we saw an issue with coverage."),
         answer(sprintf("The average value of the estimated slope of $X_2$ is very different from the true value."), message="While it won't be exactly the same, you should see that it is somewhat near the true value of 3. If not, double check your simulation."),
         answer("The estimate of the intercept appears to be biased as it is not near the true value.", correct = TRUE, message="We saw an issue with the coverage of the 95% confidence interval for the intercept, and the cause of that is that the interval is not actually being centred at the correct place due to this bias in the estimates."),
        allow_retry = TRUE
)
```


### Part 3.2: Incorrect functional form
We can also violate this assumption by changing the functional form of at least one of the predictors. Recall that the linearity assumption states that a linear relationship exists between the response and predictors, and that it specifically includes predictors as they have been added to the fitted model. This means that if we fit a model assuming that the true relationship is $$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon$$ but in fact the true relationship is $$Y = \beta_0 + \beta_1 X_1^2 + \beta_2 X_2^2  + \epsilon,$$ we would violate the linearity assumption.

Therefore, we can also violate the linearity assumption by creating this situation. We will do this below by squaring each predictor when we generate the responses in our sample.

```{r linearity2, exercise = TRUE, exercise.lines = 25}
# Set up parameters
error_mean = 0
num_runs = 2000
sample_size = 50
error_sd = 1
betas <- matrix(NA, nrow=num_runs, ncol=3)

for (i in 1:num_runs){
  x1<-  rnorm(sample_size, mean = 1, sd = 5)
  x2<-  rnorm(sample_size, mean = 1, sd = 1)
  e <- rnorm(sample_size, mean = error_mean, sd = error_sd)
  # Break the linearity assumption by squaring each predictor
  y <- __________
  # fit a model as we have previously assuming that linearity holds
  model <- lm(__________)
  # Store the coefficients of the model in matrix betas
  betas[i,] <- as.numeric(__________)
}

# Print the mean of the estimated coefficients
print(paste0("Average value of the estimated intercept: ", mean(betas[,1])))
print(paste0("Average value of the estimated slope of X1: ", mean(betas[,2])))
print(paste0("Average value of the estimated slope of X2: ", mean(betas[,3])))
```

```{r linearity2-hint-1}
# we want to square the predictors in the true relationship like below
y <- 1 + 2*x1**2 + 3*x2**2 + e
```

```{r linearity2-hint-2}
# the fitted model would assume that the true relationship is
y <- 1 + 2*x1 + 3*x2 + e
```

```{r linearity2-hint-3}
# we can extract the estimated coefficients using
coef(model)
```


```{r q8, include=TRUE}
question("What do you notice about the average value of the estimated coefficients?",
         answer("On average, the estimated coefficients are similar to the true value", message="Double check how you've set up the true relationship that generates the data and the fitted model. They should be different."),
         answer("Only one of the estimated coefficients on average is different that the true value.", message="Double check how you've set up the true relationship that generates the data and the fitted model. They should be different."),
         answer("On average, the estimated coefficients are not estimating the correct true value.", correct = TRUE, message="This is because the true relationship has the preedictors squared but the fitted model doesn't. So the coefficients being estimated are not those that are part of the true relationship."),
         answer("The average values of the estimated coefficients suggest that the coverage of the confidence intervals should be close to 95%.", message="Because linearity is violated and thus the estimators are not unbiased, we would not expect the coverage of the intervals to align with 95%."),
         allow_retry = TRUE
)
```

## Section 4 - Violated Independent/Uncorrelated Errors
As we saw from the previous two sections, if we violate either the assumptions of linearity or the assumption of constant variance, we end up with estimates of coefficients that don't behave as we would expect them to. We end up with incorrect coverage of our confidence intervals and bias in our estimates of the coefficients.

In this part, we will examine the independence/uncorrelated errors assumption to see if violation of this assumptions will yield similar results. Let's start by reviewing the definition of the independence/uncorrelated errors assumption.  

```{r q9, include=TRUE}
question("Which statements is TRUE regarding the independent/uncorrelated errors assumption?",
         answer(sprintf("The relationship $\\epsilon_{k+1} = \\epsilon_{k}$ for $k=1,2,3,...$ satisfies the independence assumption."), message="The assumption does not require that all errors be equal, rather that they have no relationship to each other."),
         answer(sprintf("The independence assumption states that $Corr(\\epsilon_i, \\epsilon_j) = 0$ for all $i \\neq j$."), correct = TRUE, message="The assumption requires that all errors be uncorrelated with one another, namely have a correlation of 0."),
         answer(sprintf("The independence assumption states that the observed predictors $X_1, X_2, X_3...$ are independent/uncorrelated of each other."), message="The assumption is about the errors, not the predictors."),
         allow_retry=TRUE
)
```

### Part 4.1: Examining the Coverage when Assumptions Hold
We will study the impact of violating the uncorrelated/independent errors assumption by again looking at the coverage of the confidence intervals to understand what changes when this assumption no longer holds.

To begin, we can create a simulation when all assumptions hold where, for each run, we will save the confidence intervals. Fill in the blanks below to get an idea of how the sampling distribution behaves normally.

```{r corr1, exercise = TRUE, exercise.lines = 25}
# Set up parameters
num_runs = 2000
sample_size = 100
# setup storage
Beta0CI <- Beta1CI <- Beta2CI <- matrix(NA, nrow=num_runs, ncol = 2)

for(i in 1:num_runs){
  # generate the sample
  x1<- rnorm(sample_size, mean = 1, sd = 5)
  x2<- rnorm(sample_size, mean = 1, sd = 1)
  e <- ____
  y <- 1+2*x1+3*x2+e
  # fit the model
  model <- lm(y~x1+x2)
  # store each confidence interval
  Beta0CI[i,] <- confint(model)[1,]
  Beta1CI[i,] <- confint(model)[2,]
  Beta2CI[i,] <- confint(model)[3,]
}

# compute the coverage of the confidence intervals
print(paste0("Coverage for Intercept: ", sum(Beta0CI[,1] <=__ & Beta0CI[,2] >=__)/num_runs))
print(paste0("Coverage for Slope X1: ", sum(Beta1CI[,1] <=__ & Beta1CI[,2] >=__)/num_runs))
print(paste0("Coverage for Slope X2: ", sum(Beta2CI[,1] <=__ & Beta2CI[,2] >=__)/num_runs))
```

```{r corr1-hint-1}
# Remember that we are generating a sample of Normal errors with mean 0 and sd 1.
# we can use the same code as for generating the predictors, just with different values, e.g.
x2<- rnorm(sample_size, mean = 1, sd = 1)
```

```{r corr1-hint-2}
# remember that we had the following parameters set up in previous sections
error_mean = 0
error_sd = 1
```

```{r corr1-hint-3}
# our true relationship with the true intercept, slope of x1 and slope of x2 is
y <- 1+2*x1+3*x2+e
```


```{r q10, include=TRUE}
question("Is the coverage of the confidence intervals about what you expected?",
         answer("Yes", correct = TRUE, message="All our assumptions hold, so we should observe no issues with the coverage. It should be around 95%."),
         answer("No", message="All our assumptions hold, so we should observe no issues with the coverage. It should be around 95%. If this is not what you saw, double check that you have filled in the gaps to ensure the assumptions hold."),
         allow_retry = TRUE
)
```

### Part 4.2: Adding Correlation to the Errors
To violate uncorrelated errors, we will need to modify how the random errors are generated. The function we have been using to get independent errors from a Normal distribution, `rnorm()` always gives us independent errors. We will need to use a different random normal generator which can generate correlated/dependent random errors to violate this assumption. The function that will let us do this is `mvrnorm()` which can take in a covariance matrix to create correlated variables.

This means that we need to create a **covariance matrix**. A covariance matrix is a way to concisely describe the variance of multiple random variables along with their covariance. Recall that when we have independent random variables, the covariance between any two will be $Cov(X_i, X_j) = 0, i \neq j$. So we could write the covariance matrix between 3 independent random variables as

$$\Sigma = \begin{pmatrix}
\sigma_1^2 & 0 & 0\\
0 & \sigma_2^2 & 0\\
0 & 0 & \sigma_3^2
\end{pmatrix},$$

where the diagonal elements are the variances of each random variable. If the random variables have a covariance, then the off-diagonal elements will no longer be zero and then the random variables are no longer independent.

Since we will be generating a sample of 100 errors, we don't want to create a covariance matrix with 100 rows and columns. Instead we can make a covariance matrix of size 4x4 and just sample random variables in sets of 4. By doing this, we create a dependence between 4 random variables/errors at a time in order to create a situation that violates the uncorrelated errors assumption.

Our covariance matrix will be

$$\Sigma = \begin{pmatrix}
1 & 0.75 & 0.5 & 0.25\\
0.75 & 1 & 0.75 & 0.5\\
0.5 & 0.75 & 1 & 0.75\\
0.25 & 0.5 & 0.75 & 1
\end{pmatrix},$$
and we will need to also set a mean for the multivariate normal distribution that will create these errors. Since we only want to violate correlated errors, we will set the mean to be 0. Then we will be generating errors from a multivariate distribution that looks like
$$\begin{pmatrix} \epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \epsilon_4 \end{pmatrix} \sim N_4 \left(\begin{pmatrix} 0 \\ 0 \\ 0 \\ 0 \end{pmatrix}, \begin{pmatrix}
1 & 0.75 & 0.5 & 0.25\\
0.75 & 1 & 0.75 & 0.5\\
0.5 & 0.75 & 1 & 0.75\\
0.25 & 0.5 & 0.75 & 1
\end{pmatrix}\right). $$

Since this only gives us 4 errors at a time, and we want a sample of 100, we will sample this distribution 25 times (25 times 4 = 100). In the code below, fill in the diagonal elements of the covariance matrix like above, add in a value for the mean of the errors in `mvrnorm()` and tell this function what the covariance matrix is called.

```{r corr2-setup, include=F}
set.seed(1234)
```

```{r corr2, exercise = TRUE, exercise.lines = 30, exercise.setup="corr2-setup"}
# Set up parameters
num_runs = 5000
sample_size = 100
# set up the storage
Beta0CI <- Beta1CI <- Beta2CI <- matrix(NA, nrow=num_runs, ncol = 2)
# Build the covariance matrix by row
row1 <- c(___, 0.75, 0.5, 0.25)
row2 <- c(0.75, ___, 0.75, 0.5)
row3 <- c(0.5, 0.75, ___, 0.75)
row4 <- c(0.25, 0.5, 0.75, ___)
# Create covariance matrix such that all errors corresponding to unique values of x1 will be correlated
Sigma = matrix(rbind(row1, row2, row3, row4), nrow=4, ncol=4)
library(MASS)
for(i in 1:num_runs){
  x1<-  rnorm(sample_size, mean = 1, sd = 5)
  x2<-  rnorm(sample_size, mean = 1, sd = 1)
  e <- as.vector(mvrnorm(n=25, mu = rep(___, times=4), Sigma = ____))
  y <- 1+2*x1+3*x2+e
  model <- lm(y~x1+x2)
  Beta0CI[i,] <- confint(model)[1,]
  Beta1CI[i,] <- confint(model)[2,]
  Beta2CI[i,] <- confint(model)[3,]
}
  
# compute the coverage of the confidence intervals
print(paste0("Coverage for Intercept: ", sum(Beta0CI[,1] <=1 & Beta0CI[,2] >=1)/num_runs))
print(paste0("Coverage for Slope X1: ", sum(Beta1CI[,1] <=2 & Beta1CI[,2] >=2)/num_runs))
print(paste0("Coverage for Slope X2: ", sum(Beta2CI[,1] <=3 & Beta2CI[,2] >=3)/num_runs))
```

```{r corr2-hint-1}
# the covariance matrix should have diagonal elements equal to 1
row1 <- c(1, 0.75, 0.5, 0.25)
row2 <- c(0.75, 1, 0.75, 0.5)
row3 <- c(0.5, 0.75, 1, 0.75)
row4 <- c(0.25, 0.5, 0.75, 1)
```

```{r corr2-hint-2}
# the mean we want for the errors is 0 and since we generate 4 errors at once, we want to repeat this value 4 times.
# to repeat something many times we use the code
rep(0, times=4)
# which repeats the value 0 four times.
```

```{r corr2-hint-3}
# see line 12 in the code chunk for the label we gave to our covariance matrix:
Sigma = matrix(rbind(row1, row2, row3, row4), nrow=4, ncol=4)
```

```{r q11, include=TRUE}
question("What do you notice about the coverage?",
         answer("At least one confidence interval does not have the correct coverage.", message="We should notice that the intercept has lower coverage than it should, indicating that having correlated errors impacts this property.", correct = TRUE),
         answer("All of the confidence intervals do not have the correct coverage.", message="This type of violation does not usually impact all confidence intervals, and this should be supported by the values you compare between section 4.1 and 4.2."),
         answer("All of the confidence intervals have the correct coverage.", message="This type of violation usually impacts at least one of the confidence intervals, and this should be supported by the values you compare between section 4.1 and 4.2."),
         answer("I'm not sure what is happening to the coverage", message="That's ok. Be sure to double check that you've filled in the code correrctly by looking at the hints. Then compare to what you saw in section 4.1."),
         allow_retry = TRUE
)
```

## Section 5 - Violated Normality
In our final section, we will examine how the normality assumption relates to the sampling distributions of our estimators of the coefficients. Recall that a sampling distribution is a theoretical distribution that describes how the values we get for an estimate of each coefficient will change depending on the sample we used to compute that estimate. We can mimic the concept of a sampling distribution with our simulation, since we take many samples when we loop through our code.

### Part 5.1: Sampling Distribution of the Coefficient Estimators
Let's start by considering which elements of our linear regression model are assumed to be Normally distributed due to this assumption.  

```{r q12, include=TRUE}
question("Which quantity should NOT have a Normal distribution, assuming the Normality assumption is not violated?",
         answer("The population responses", message="The Normality assumption can equivalently be written for the responses or the errors."),
         answer("The population errors", message="The Normality assumption can equivalently be written for the responses or the errors."),
         answer("The estimators of the coefficients", message="If the Normality assumption holds, then by properties of Normal random variables, the distribution of the estimators will also be Normal."),
         answer("The population predictors", message="In linear regression, the predictors are fixed, not random, and therefore do not have a distribution.", correct=T),
         allow_retry = TRUE
)
```

To understand what the sampling distribution looks like when assumptions hold, we will modify our simulation to look at the different values of the coefficients that are estimated in each loop. We can store these, like we did the confidence intervals, and plot them to get a visual representation of an estimated sampling distribution.

```{r norm1, exercise = TRUE, exercise.lines = 25}
# Set up parameters
num_runs = 2000
sample_size = 25
# setup storage
betas <- matrix(NA, nrow=num_runs, ncol = ___)

for(i in 1:num_runs){
  # generate the sample
  x1<- rnorm(sample_size, mean = 1, sd = 5)
  x2<- rnorm(sample_size, mean = 1, sd = 1)
  e <- ____
  y <- 1+2*x1+3*x2+e
  # fit the model
  model <- lm(y~x1+x2)
  # store each coefficient
  betas[i,] <- as.numeric(___)
}

# plot a histogram of each coefficient
par(mfrow=c(1,3))
hist(betas[,1], main="Intercept Distribution", xlab="Estimated Intercepts")
hist(betas[,2], main="X1 Slope Distribution", xlab="Estimated X1 Slopes")
hist(betas[,3], main="X2 Slope Distribution", xlab="Estimated X2 Slopes")
```

```{r norm1-hint-1}
# the number of columns must be the same as the number of coefficients
```

```{r norm1-hint-2}
# remember that we had the following parameters set up in previous sections
error_mean = 0
error_sd = 1
# and that our assumption is that the errors are Normal like the predictors
```

```{r norm1-hint-3}
# To extract the estimated coefficients, use
coef(model)
```

```{r normq, include=TRUE}
question("What do you notice about the sampling distributions?",
         answer("The sampling distributions appear to be non-Normal shaped.", message="By properties of Normals, they should look approximately Normal."),
         answer("The sampling distributions appear to be centred at the true values of the coefficients.", message="If all assumptions hold, our estimates should be unbiased, meaning that the expected value is the true value.", correct=T),
         answer("The estimators of the coefficients appear to be biased.", message="If all assumptions hold, our estimates should be unbiased, meaning that the expected value is the true value."),
         answer("We would expect the coverage of the confidence intervals to be much less than 95%.", message="If all assumptions hold, as they do here, the coverage should be approximately 95%."),
         allow_retry = TRUE
)
```

### Part 5.2: Violating the Normality Assumption

To violate Normality, we simply need to change the distribution that is generating the errors to that is not a Normal distribution, i.e. we need to use a function different from `rnorm()`. While any distribution that is not Normal will violate this assumption, the effect is more prominent if we pick a distribution that is far from Normal, such as the log-Normal distribution. The log-Normal distribution is a highly skewed distribution that is based off the Normal. If a random variable X is log-Normally distributed, then then ln(X) will be Normally distributed.

Because of this relationship, the log-Normal distribution has both a mean and a standard deviation as parameters, like the Normal. We will use `rlnorm()` to generate errors from a log-Normal, and we will specify the mean to be 0 and the standard deviation to be 1. The `rlnorm()` function uses the same syntax as the `rnorm()` function, so you can copy the syntax from the code that generates the predictors. Fill in the blanks below to see how a violation of Normality affects the sampling distributions of the coefficients.

```{r norm2, exercise = TRUE, exercise.lines = 25}
# Set up parameters
error_mean = 0
num_run = 2000
sample_size = 25
error_sd = 1
betas <- matrix(NA, nrow=num_run, ncol=3)

for(i in 1:num_run){
  x1<- rnorm(sample_size, mean = 1, sd = 5)
  x2<- rnorm(sample_size, mean = 1, sd = 1)
  # Use a log-Normal distribution
  e <- __________
  y <- 1+2*x1+3*x2+e
  model <- lm(y~x1+x2)
  betas[i,] <- as.numeric(_____)
}

# plot the distribution of betas
par(mfrow=c(1,3))
hist(betas[,1], main="Intercept Distribution", xlab="Intercept")
hist(betas[,2], main="X1 Slope Distribution", xlab="X1 Slope")
hist(betas[,3], main="X2 Slope Distribution", xlab="X2 Slope")

```

```{r norm2-hint-1}
# The syntax of rlnorm function that generates log Normal data is
rlnorm(sample_size, meanlog = mean, sdlog = sd)
# and we want meanlog = 0 and sdlog = 1
```

```{r norm2-hint-2}
# To extract the estimated coefficients, use
coef(model)
```


```{r q13, include=TRUE}
question("What about the sampling distribution has changed now that Normality is violated?",
         answer("The sampling distributions are more bell-shaped now than when Normality held.", message="The opposite should be true, as we are generating our errors from a very highly skewed distribution. So you should see a less bell-shaped appearance than before."),
         answer("The mean of all sampling distributions are still around the true value of the coefficients.", message="We should observe a slightly different mean in at least one of the sampling distributions, most likely the intercept."),
         answer("The sampling distributions exhibit more variability in the estimated coefficients.", correct = TRUE, message="The axes have changed dramatically due to sampling from a highly skewed distribution leading us to occasionally get some very extreme estimates of the coefficients."),
         allow_retry = TRUE
)
```

## Conclusion

With any statistical tool you might use, it's important to recognize that they all depend on certain assumptions being true regarding where the data originated. Since we only see a sample of that population, we always need to check that these assumptions hold, in order for valid inference about that population to be made. The goal of this module was to get you to see that when assumptions do not hold, our tools do not tell us the correct information, which can lead us to make mistakes in how we use those tools. We focused primarily on looking at the impact on coverage and on the sampling distribution, but other areas of inference would likewise be affected.

```{r q14, include=TRUE}
question("What types of mistakes could we make if we use inferential tools without checking assumptions?",
         answer("We might conclude an effect is significant when it isn't.", message="This is true since p-values are directly linked to the sampling distribution. But it's not the only true option."),
         answer("We might obtain an estimate of a quantity that is far from being true.", message="This is true since we've seen that violated assumptions can lead to biased estimation. But it's not the only true option."),
         answer("We might have more confidence in our results and process than we should.", message="This is true since we saw that violated assumptions lead to lower coverage. meaning we will have more confidence than we should. But it's not the only true option."),
         answer("All of the options are potential mistakes.", correct = TRUE, message="Bias means the centre of the sampling distribution isn't in the correct place, which means our confidence intervals are less likely to capture the truth. Further, if sampling distributions are not as they should be, we will compute incorrect p-values and thus make decisions regarding significance that may be incorrect."),
         allow_retry = TRUE
)
```

Hopefully, you see how important it is to verify that assumptions hold in all inferential tools. There are many ways to verify the assumptions and to correct or at minimum improve them if needed. But it is most important to know that statistical tools depend on assumptions and to recognize that if assumptions do not hold, the statistical tool should not be used. 