---
title: "Impact of Multicollinearity"
output: 
 learnr::tutorial:
  progressive: true
  allow_skip: true
runtime: shiny_prerendered
---


```{r setup, include=FALSE}
#install.packages("remotes", repos = "https://cloud.r-project.org/")

# These are the packages you will need for this activity
packages_needed <- c("tidyverse","learnr", "MASS")

package.check <- lapply(
  packages_needed,
  FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE, repos = "http://cran.us.r-project.org")
    }
  }
)

# Credit: package.check based on a helpful post from Vikram Baliga https://vbaliga.github.io/verify-that-r-packages-are-installed-and-loaded/

#remotes::install_github("rstudio/gradethis")


library(tidyverse)
library(shiny)
library(ggplot2)
options(repos = c(CRAN = "https://cloud.r-project.org"))
install.packages("broom")
library(broom)
library(learnr)
library(remotes)
library(gradethis)

gradethis::gradethis_setup()


knitr::opts_chunk$set(echo = FALSE)

options(tutorial.storage = list(
  save_object = function(...){},
  get_object = function(...) NULL,
  get_objects = function(...) list(),
  remove_all_objects = function(...){}))
set.seed(302)
```

## Introduction


Welcome to Module 6: **Impact of Multicollinearity**. In this module, you will learn: 

  * To use R to generate both independent and correlated variables

  * To use R to visualize confidence intervals of coefficients from both simple and multiple linear regression models. 

  * To explain how highly correlated predictors change the widths of confidence intervals for both simple and multiple linear regression models. 

  * To explain how unobserved confounders might impact estimated coefficients in a linear regression model.  

Remember that if you get stuck on a coding exercise, you can click the **Hints** button for help! 

## Motivation

In this module, we will explore our ability to make inferences when we have two or more predictors that are correlated to one another. We will simulate a dataset in which we know the true relationship between our response ($Y$) and our predictors ($X_1, X_2, X_3$), and we will choose how correlated the predictors will be. 

When two or more predictors are correlated to each other, we say we have **multicollinearity** present in our model.

## Section 0: Understanding Correlation

First, we will review correlation and the corresponding correlation coefficient, $r$. 

Recall that the correlation coefficient measures the strength of linear relationship between two variables, $X$ and $Y$, and is calculated as $r=\frac{\Sigma[(x_i-\bar{x})(y_i-\bar{y})]}{\sqrt{\Sigma(x_i-\bar{x})^2*\Sigma(y_i-\bar{y})^2}}$.

The value of $r$ falls in the range of $[-1,1]$ with values near $-1$ denoting a strong negative linear relationship, and values near $1$ denoting a strong positive linear relationship. Values near 0 denote weaker linear relationships. 

```{r q0, include=TRUE}
question("Which statement is true regarding correlation between two measurements?",
         answer("The correlation coefficient tells us whether these measurements are linearly related.", message="We must know that two coefficients are linearly related to correctly interpret the correlation coefficient. "),
         answer("If the measurements are strongly linearly related, then a linear regression model can be used to represent this association.",correct = TRUE,message = "If the measurements are strongly linearly related, then a linear regression model can be used to represent this association."),
         answer("The correlation coefficient can be used to describe the association between 3 or more measurements.", message="The correlation coefficient can only be used to describe the association between two measurements. "),
         answer("None of these options are true."),
        allow_retry = TRUE
)
```

## Section 1: Creating the Dataset

To begin, we will simulate a dataset where we select the true values of the coefficients ($\beta$). 

First we will create a dataset such that our predictors will be uncorrelated to each other. We will be trying to estimate a model using 3 predictors, where the true relationship is $$Y = 1  + 2X_1 + 3X_2 + 0X_3 + \epsilon, \quad \epsilon \sim N(0, 2.25).$$ 

```{r q1, include=TRUE}
question("Which of the below statements is correct regarding this true relationship?",
         answer("$X_3$ has no linear association to $Y$.", message="You're on the right track, but remember we are interpreting this in the context of a multiple linear model."),
         answer("In the presence of the other predictors, a one-unit increase in $X_2$ yields a 3-unit increase in $Y$.", message="To interpret a single coefficient in a multiple linear model, we must hold all other predictors constant and express the relationship as an on average effect."),
         answer("On average, when other predictors are held fixed, a one-unit increase in $X_1$ increases $Y$ by 2 units.", correct = TRUE, message = "This is true since the coefficient in the true model of X1 is 2."),
         answer("All of the statements are true."),
        allow_retry = TRUE
)
```

To begin, we will set all our fixed values for how we would like to generate our data. This includes:

  * The error variance in the population as $\sigma^2 = 2.25$.
  
  * The true regression coefficients where $(\beta_0, \beta_1, \beta_2, \beta_3) = (1, 2, 3, 0)$.
  
  * The sample size as $n = 50$.
  
  * The covariance matrix for the 3 predictors we will create. 

The covariance matrix takes the form
$$\Sigma = \begin{pmatrix} 1 & 0 & 0\\
                          0 & 1 & 0\\
                          0 & 0 & 1 \end{pmatrix}$$
where the values on the main diagonal are the variances of each predictor in the population, and the off-diagonal elements are the covariances between any two predictors. **Note:** A value of 0 in the off-diagonal elements implies no instance of collinearity across the respective predictors.

Once these values are set up, we can "sample" our predictors from the population in which each predictor distribution has a mean of 0. When the predictors have been created, check the correlation between them to ensure we have nearly uncorrelated predictors.

Below, fill in the the lines of code such that we sample the data using the information above. The `diag()` function takes a vector of $m$ diagonal elements and creates an $m \times m$ matrix where all off-diagonal elements are 0. 

```{r sim1, exercise = TRUE, exercise.lines = 20}
# set the seed for reproducibility
set.seed(302)

# fill in the values from above:
n = ____
errorvar = ____
betas <- c(____,____,____,____)
covariance <- diag(c(____,____,____))

# generate/sample the predictors using a multivariate Normal distribution
X <- mvrnorm(n, mu = c(0,0,0), Sigma = ____)

# check the correlation between the predictors
cor(X)
```

```{r sim1-solution, eval=FALSE}
# set the seed for reproducibility
set.seed(302)

# fill in the values from above:
n = 50
errorvar = 2.25
betas <- c(1,2,3,0)
covariance <- diag(c(1,1,1))

# generate/sample the predictors using a multivariate Normal distribution
X <- mvrnorm(n, mu = c(0,0,0), Sigma = covariance)

# check the correlation between the predictors
cor(X)
```

```{r sim1-check}
grade_this_code()
```

```{r sim1-hint-1}
# ---

# The input for Sigma in mvrnorm function needs a covariance matrix of dimension 3*3, which we already created.

# ---
```

```{r q2, include=TRUE}
question("Why might it be possible to observe a sample correlation that is non-zero when we provided R with a covariance matrix with zero correlation?",
         answer("The population has uncorrelated predictors, but the sample may not.", correct = TRUE),
         answer("We did not set up our code correctly for the population."),
         answer("The sampling process did not function correctly."),
         answer("None of these options."), allow_retry = TRUE
)
```

Lastly, we can use this information to generate our errors and our response variable to be able to fit linear regression models in the next section. To do this, we use the true relationship that was given at the top of this section.

```{r confint1-setup, include=FALSE}
# set the seed for reproducibility
set.seed(302)

# fill in the values from above:
n = 50
errorvar = 2.25
betas <- c(1,2,3,0)
covariance <- diag(c(1,1,1))

# generate/sample the predictors using a multivariate Normal distribution
X <- mvrnorm(n, mu = c(0,0,0), Sigma = covariance)

# check the correlation between the predictors
cor(X)
```

```{r confint1, exercise = TRUE, exercise.lines = 30}
# set the seed for reproducibility
set.seed(302)

# generate the random errors for these data from a Normal(0, 2.25)
e <- rnorm(n, ____, ____)

# update your X vector to include an intercept column
Xmat <- cbind(1, X)

# use the above two components to generate a response variable
y <- Xmat %*% betas + e

# produce a summary of your response
summary(y)

```

```{r confint1-solution, eval=FALSE}
# set the seed for reproducibility
set.seed(302)

# generate the random errors for these data from a Normal(0, 2.25)
e <- rnorm(n, 0, sqrt(2.25))

# update your X vector to include an intercept column
Xmat <- cbind(1, X)

# use the above two components to generate a response variable
y <- Xmat %*% betas + e

# produce a summary of your response
summary(y)

```

```{r confint1-check}
grade_this_code()
```

```{r confint1-hint-1}
# ---

# Keep in mind that the last parameter in the rnorm function is standard deviation rather than variance.

# ---
```

## Section 2: Fitting Simple Linear Models 

Now that we have decided on our true population relationship between $Y$ and predictors $X_1, X_2$, and $X_3$ and "collected" a sample of data from the population, we can use this data to estimate the true relationship. To understand how each predictor individually relates to the response, we can fit three separate simple linear models, using each predictor at a time. 

```{r q4-setup, include=FALSE}
# set the seed for reproducibility
set.seed(302)

# fill in the values from above:
n = 50
errorvar = 2.25
betas <- c(1,2,3,0)
covariance <- diag(c(1,1,1))

# generate/sample the predictors using a multivariate Normal distribution
X <- mvrnorm(n, mu = c(0,0,0), Sigma = covariance)

# generate the random errors for these data from a Normal(0, 2.25)
e <- rnorm(n, 0, sqrt(2.25))

# update your X vector to include an intercept column
Xmat <- cbind(1, X)

# use the above two components to generate a response variable
y <- Xmat %*% betas + e
```

```{r q4, exercise = TRUE, exercise.lines = 30}
# set the seed for reproducibility
set.seed(302)

# Fit a simple linear model using X1 as the predictor
model1a <- lm(y ~ X[,1])

# Fit a simple linaer model using X2 as the predictor
model2a <- ____

# Fit a simple linear model using X3 as the predictor
model3a <- ____

summary(model1a)
summary(model2a)
summary(model3a)
```

```{r q4-solution, eval=FALSE}
# set the seed for reproducibility
set.seed(302)

# Fit a simple linear model using X1 as the predictor
model1a <- lm(y ~ X[,1])

# Fit a simple linaer model using X2 as the predictor
model2a <- lm(y ~ X[,2])

# Fit a simple linear model using X3 as the predictor
model3a <- lm(y ~ X[,3])

# Uncomment one at a time to view summaries
#summary(model1)
#summary(model2)
#summary(model3)
```

```{r q4-check}
grade_this_code()
```

```{r q4MCQ, include=TRUE}
question("Which statement is true regarding the three simple linear models?",
         answer("The estimated slope in each model is not equal to the corresponding slope in the true relationship", correct = TRUE,  message = "Option A is correct since the true relationship is (beta_0, beta_1, beta_2, beta_3) = (1, 2, 3, 0)"),
         answer("In the simple linear model using $X_3$, $X_3$ has no linear association with $Y$.", message = "While the relationship may be weak, it appears to be nonzero."),
         answer("The estimate of the error variance for the model using $X_1$ matches the true error variance in the population.", "Recall that the error variance in the population is 2.25."),
         answer("None of the options are true."),
        allow_retry = TRUE
)
```

## Section 3: Fit Multiple Linear Models & Visualize Significance of Estimates

To visualize the estimated coefficient for each of $X_1, X_2$ and $X_3$ from the 3 separate simple linear models, we can make a plot that displays the estimated coefficients as points and places a confidence interval around each in the vertical direction. We can then compare the results from the separate SLRs to an estimated relationship from a multiple linear model. Next, we will extract the 95% confidence interval of each slope using the confint() function. Finally, we will also extract the estimated coefficients using the coef() function.


```{r linearity1-setup, include=FALSE}
# set the seed for reproducibility
set.seed(302)

# fill in the values from above:
n = 50
errorvar = 2.25
betas <- c(1,2,3,0)
covariance <- diag(c(1,1,1))

# generate/sample the predictors using a multivariate Normal distribution
X <- mvrnorm(n, mu = c(0,0,0), Sigma = covariance)

# generate the random errors for these data from a Normal(0, 2.25)
e <- rnorm(n, 0, sqrt(2.25))

# update your X vector to include an intercept column
Xmat <- cbind(1, X)

# use the above two components to generate a response variable
y <- Xmat %*% betas + e

# Fit a simple linear model using X1 as the predictor
model1a <- lm(y ~ X[,1])

# Fit a simple linaer model using X2 as the predictor
model2a <- lm(y ~ X[,2])

# Fit a simple linear model using X3 as the predictor
model3a <- lm(y ~ X[,3])
```

```{r linearity1, exercise = TRUE, exercise.lines = 25}
# set the seed for reproducibility
set.seed(302)

# Fit a multiple linear model which includes all 3 predictors
MLRa <- lm(y ~ X)

# Extract the slope coefficients from all 4 models
SLRcoefs <- c(coef(model1a)[2], coef(model2a)[2], coef(model3a)[2])
MLRcoefs <- coef(MLRa)____

# relabel to values more intuitively
names(SLRcoefs) <- c("X1", "X2", "X3")
names(MLRcoefs) <- c("X1", "X2", "X3")

# Extract confidence intervals (95%) from each simple linear model
x1confint <- confint(model1a)[2,]
x2confint <- confint(model2a)[2,]
x3confint <- confint(model3a)[2,]

# extract the confidence intervals of the 3 slopes from the MLR
MLRconfints <- ____

# create a combined coefficients vector and combined CI matrix
coefs <- c(SLRcoefs, MLRcoefs)
confints <- rbind(x1confint, x2confint, x3confint, MLRconfints)
```

```{r, linearity1-solution,  eval=FALSE}
# set the seed for reproducibility
set.seed(302)

# Fit a multiple linear model which includes all 3 predictors
MLRa <- lm(y ~ X)

# Extract the slope coefficients from all 4 models
SLRcoefs <- c(coef(model1a)[2], coef(model2a)[2], coef(model3a)[2])
MLRcoefs <- coef(MLRa)[2:4]

# relabel to values more intuitively
names(SLRcoefs) <- c("X1", "X2", "X3")
names(MLRcoefs) <- c("X1", "X2", "X3")

# Extract confidence intervals (95%) from each simple linear model
x1confint <- confint(model1a)[2,]
x2confint <- confint(model2a)[2,]
x3confint <- confint(model3a)[2,]

# extract the confidence intervals of the 3 slopes from the MLR
MLRconfints <- confint(MLRa)[2:4,]

# create a combined coefficients vector and combined CI matrix
coefs <- c(SLRcoefs, MLRcoefs)
confints <- rbind(x1confint, x2confint, x3confint, MLRconfints)
```

```{r linearity1-check}
grade_this_code()
```

```{r linearity1-hint-1}
#---

# Be sure we are not extracting the intercept information for coefficients 
# and confidence intervals. You may find the following command helpful
coef(MLRa)[2:4]
confint(MLRa)[2:4,]

#---
```

Now we will plot the information we have extracted from the models. The estimates will be added as points with the confidence intervals bounds represented as endpoints of a line. Additionally, a dashed line is included for reference to a 0 coefficient. By merging the coefficients and the confidence intervals from all models into a single storage element, it allows us to draw this plot more efficiently by using a for loop.

As such, to draw this plot, you will need to add: 

  * the correct x-axis and y-axis elements to the plot function.
  
  * the horizontal line at 0 as a dashed line.
  
  * the vertical lines representing the confidence intervals for each slope.
  
  
```{r linearity2-setup, include=FALSE}
# set the seed for reproducibility
set.seed(302)

# fill in the values from above:
n = 50
errorvar = 2.25
betas <- c(1,2,3,0)
covariance <- diag(c(1,1,1))

# generate/sample the predictors using a multivariate Normal distribution
X <- mvrnorm(n, mu = c(0,0,0), Sigma = covariance)

# generate the random errors for these data from a Normal(0, 2.25)
e <- rnorm(n, 0, sqrt(2.25))

# update your X vector to include an intercept column
Xmat <- cbind(1, X)

# use the above two components to generate a response variable
y <- Xmat %*% betas + e

# Fit a simple linear model using X1 as the predictor
model1a <- lm(y ~ X[,1])

# Fit a simple linaer model using X2 as the predictor
model2a <- lm(y ~ X[,2])

# Fit a simple linear model using X3 as the predictor
model3a <- lm(y ~ X[,3])

MLRa <- lm(y ~ X)

# Extract the slope coefficients from all 4 models
SLRcoefs <- c(coef(model1a)[2], coef(model2a)[2], coef(model3a)[2])
MLRcoefs <- coef(MLRa)[2:4]

# relabel to values more intuitively
names(SLRcoefs) <- c("X1", "X2", "X3")
names(MLRcoefs) <- c("X1", "X2", "X3")

# Extract confidence intervals (95%) from each simple linear model
x1confint <- confint(model1a)[2,]
x2confint <- confint(model2a)[2,]
x3confint <- confint(model3a)[2,]

# extract the confidence intervals of the 3 slopes from the MLR
MLRconfints <- confint(MLRa)[2:4,]

# create a combined coefficients vector and combined CI matrix
coefs <- c(SLRcoefs, MLRcoefs)
confints <- rbind(x1confint, x2confint, x3confint, MLRconfints)
``` 

```{r linearity2, exercise = TRUE, exercise.lines = 25}
# set the seed for reproducibility
set.seed(302)

#plot the point estimates on a plot, where the position on the x-axis will be 
# 1, 2, 3 for the SLR values and 6,7,8 for the MLR values
xposition <- c(1,2,3,6,7,8)
plot(____,
     type="p", ylim=c(-3, 6), xlim=c(0,8), bty="n", xaxt="n",
     ylab="", xlab="")

# add the horizontal line of no effect at 0
abline(h=____, lty="dashed")

# adds a label to distinguish the points
text(2, 5.5, "Coefficients in SLRs")
text(7, 5.5, "Coefficients in MLR")

# loop to add confidence intervals and labels to plot
for(i in 1:6){
    # get the x position to plot at
    x <- xposition[i]
    # adds a label to distinguish the points
    text(x, -2.75, names(coefs)[i])
    # plots a vertical lines for the CI of each point
    lines(c(x,x), ____, lwd=3)
}
```

```{r, linearity2-solution, eval=FALSE}
# set the seed for reproducibility
set.seed(302)

#plot the point estimates on a plot, where the position on the x-axis will be 
# 1, 2, 3 for the SLR values and 6,7,8 for the MLR values
xposition <- c(1,2,3,6,7,8)
plot(coefs ~ xposition,
     type="p", ylim=c(-3, 6), xlim=c(0,8), bty="n", xaxt="n",
     ylab="", xlab="")

# add the horizontal line of no effect at 0
abline(h=0, lty="dashed")

# adds a label to distinguish the points
text(2, 5.5, "Coefficients in SLRs")
text(7, 5.5, "Coefficients in MLR")

# loop to add confidence intervals and labels to plot
for(i in 1:6){
    # get the x position to plot at
    x <- xposition[i]
    # adds a label to distinguish the points
    text(x, -2.75, names(coefs)[i])
    # plots a vertical lines for the CI of each point
    lines(c(x,x), confints[i,], lwd=3)
}
```

```{r, linearity2-check}
grade_this_code()
```

```{r linearity2-hint-1}
#---

# For the plot function, values on y are our estimated coefficients and values 
# on x are the positions
plot(coefs ~ xposition, ...)

#---
```

```{r linearity2-hint-2}
#---

# For the abline function h is the value that gives the horizontal straight line
abline(h=0, lty="dashed")

#---

# For the plot function, values on y are our estimated coefficients and values 
# on x are the positions
plot(coefs ~ xposition, ...)

#---
```

```{r linearity2-hint-3}
#---

# The second argument of the lines function needs a lower bound and the upper 
# bound of the line segment as a single vector input
confints[i,]

#---

# For the abline function h is the value that gives the horizontal straight line
abline(h=0, lty="dashed")

#---

# For the plot function, values on y are our estimated coefficients and values 
# on x are the positions
plot(coefs ~ xposition, ...)

#---
```

This plot allows us to visualise the relationship between individual predictors and the response, as well as compare the estimates from the simple linear models to those from the multiple linear model. It also allows us to quickly view the variation in each estimate as well as whether that slope is significantly different from 0.

```{r q7, include=TRUE}
question("Why are the confidence intervals on the coefficient of each predictor narrower in the multiple linear model compared to the separate simple linear models?",
         answer("The predictors we generated are strongly correlated.", message="This is correct, but consider the error variance and the estimated coefficients in the simple models. "),
         answer("The error variance is not estimated correctly in the simple linear models.", message="This is correct, but consider how the predictors are associated and the error variance of the simple linear models. "),
         answer("The true relationship is not estimated correctly in the simple linear models.", message = "This is correct, but consider how the predictors are associated and the estimated coefficients in the simple linear models."),
         answer("More than one option is correct.", correct = TRUE),
         message = "Both betas and sigma are not estimated correctly in the simple linear regression model"
)
```

## Section 4: Impact of Correlation Between $X_1$ and $X_2$ 

In this section, we will now introduce a correlation between two of our predictors, $X_1$ and $X_2$. 

To begin, we will consider only moderate correlation (0.5) between them and investigate the impact of this on the estimates and confidence interval widths (i.e. length of the bar) from simple linear models and the multiple linear model.

First we need to generate predictors with this correlation included. Since we already created a covariance matrix without correlation, we simply need to replace the values in positions [1,2] and [2,1] with our correlation. Then, generate our predictors (again) using the new covariance matrix. Continue generating the rest of the data as before (no changes need to be made). 

```{r q81, exercise = TRUE, exercise.lines = 25}
# set the seed for reproducibility
set.seed(302)

n = 50
betas <- c(1,2,3,0)
e <- rnorm(n, 0, sqrt(2.25))
# replace elements [1,2] and [2,1] with 0.5 to incorporate the correlation
covariance <- diag(c(1,1,1))
covariance[1,2] <- covariance[2,1] <- ____
# re-generate our predictors with this new covariance matrix
X <- mvrnorm(n, mu = c(0,0,0), Sigma = covariance)
# generate new responses
Xmat <- cbind(1, X)
y <- ____

# Now fit our models, like we did before (3 SLRs and 1 MLR), and extract the slope
# coefficients in all 4 models, as well as the 95% confidence intervals.
# fit the SLR and MLE models and extract coefficients
model1b <- lm(y ~ X[,1])
model2b <- lm(____)
model3b <- lm(y ~ X[,3])
MLRb <- lm(y ~ X)

coefs <- c(coef(model1b)[2], coef(model2b)[2], ____, ____)
names(coefs) <- c("X1", "X2", "X3", "X1", "X2", "X3")

# extract confidence intervals of slopes in all 4 models
confints <- rbind(confint(model1b)[2,], confint(model2b)[2,], confint(model3b)[2,],
                  confint(MLRb)[2:4,])

# Finally, create the same type of plot as before, but using the information we 
# extracted from models fit on the new data with moderately correlated X1 and X2.

#plot the point estimates on a plot, where the position on the x-axis will be 
# 1, 2, 3 for the SLR values and 6,7,8 for the MLR values
xposition <- c(1,2,3,6,7,8)
plot(coefs ~ xposition,
     type="p", ylim=c(-3, 6), xlim=c(0,8), bty="n", xaxt="n",
     ylab="", xlab="")

# add the horizontal line of no effect at 0
abline(h=0, lty="dashed")

# adds a label to distinguish the points
text(2, 5.5, "Coefficients in SLRs")
text(7, 5.5, "Coefficients in MLR")

# loop to add confidence intervals and labels to plot
for(i in 1:6){
    # get the x position to plot at
    x <- xposition[i]
    # adds a label to distinguish the points
    text(x, -2.75, names(coefs)[i])
    # plots a vertical lines for the CI of each point
    lines(c(x,x), ____, lwd=3)
}
```

```{r, q81-solution}
# set the seed for reproducibility
set.seed(302)

n = 50
betas <- c(1,2,3,0)
e <- rnorm(n, 0, sqrt(2.25))
# replace elements [1,2] and [2,1] with 0.5 to incorporate the correlation
covariance <- diag(c(1,1,1))
covariance[1,2] <- covariance[2,1] <- 0.5
# re-generate our predictors with this new covariance matrix
X <- mvrnorm(n, mu = c(0,0,0), Sigma = covariance)
# generate new responses
Xmat <- cbind(1, X)
y <- Xmat %*% betas + e
# fit the SLR and MLE models and extract coefficients
model1b <- lm(y ~ X[,1])
model2b <- lm(y ~ X[,2])
model3b <- lm(y ~ X[,3])
MLRb <- lm(y ~ X)

coefs <- c(coef(model1b)[2], coef(model2b)[2], coef(model3b)[2], coef(MLRb)[2:4])
names(coefs) <- c("X1", "X2", "X3", "X1", "X2", "X3")

# extract confidence intervals of slopes in all 4 models
confints <- rbind(confint(model1b)[2,], confint(model2b)[2,], confint(model3b)[2,],
                  confint(MLRb)[2:4,])

#plot the point estimates on a plot, where the position on the x-axis will be 
# 1, 2, 3 for the SLR values and 6,7,8 for the MLR values
xposition <- c(1,2,3,6,7,8)
plot(coefs ~ xposition,
     type="p", ylim=c(-3, 6), xlim=c(0,8), bty="n", xaxt="n",
     ylab="", xlab="")

# add the horizontal line of no effect at 0
abline(h=0, lty="dashed")

# adds a label to distinguish the points
text(2, 5.5, "Coefficients in SLRs")
text(7, 5.5, "Coefficients in MLR")

# loop to add confidence intervals and labels to plot
for(i in 1:6){
    # get the x position to plot at
    x <- xposition[i]
    # adds a label to distinguish the points
    text(x, -2.75, names(coefs)[i])
    # plots a vertical lines for the CI of each point
    lines(c(x,x), confints[i,], lwd=3)
}
```

```{r, q81-check}
grade_this_code()
```

```{r q81-hint-1}
#---

# The covariance matrix covariance[1,2], covariance[2,1] should set to the new correlation value
covariance[1,2] <- covariance[2,1] <- 0.5

#---
```

```{r q81-hint-2}
#---

# y needs to be generated from true relationship. Make sure to include the error.
y <- Xmat %*% betas + e

#---

# The covariance matrix covariance[1,2], covariance[2,1] should set to the new correlation value
covariance[1,2] <- covariance[2,1] <- 0.5

#---

```

```{r q81-hint-3}
#---

# The second argument of the lines function needs a lower bound and the upper 
# bound of the line segment as a single vector input and remember CI information 
# is stored in rows
confints[i,]

#---

# y needs to be generated from true relationship. Make sure to include the error.
y <- Xmat %*% betas + e

#---

# The covariance matrix covariance[1,2], covariance[2,1] should set to the new correlation value
covariance[1,2] <- covariance[2,1] <- 0.5

#---
```


```{r q8, include=TRUE}
question("Does there appear to be much difference in the widths of the confidence intervals compared to when there was no correlation present in the predictors?",
         answer("Yes."),
         answer("No."),
         answer("Not totally sure.", correct = TRUE),
         message = "With moderate correlation, we generally don't see too much of a difference in the widths of the confidence intervals compared to when no correlation was present (although it generally depends on your data - sometimes it can be noticeable and other times it won't). We do still observe that the MLR confidence intervals are narrower than the SLR ones due to the fact we are specifying the correct model and therefore the error variance is being estimated more precisely (because we are explaining more variation in the response with more predictors).", allow_retry = TRUE
)
```

What if we had very strong correlation present between $X_1$ and $X_2$? Update the R code below to add a correlation of 0.98 between these two predictors. Fill in the blanks below.
```{r corr1, exercise = TRUE, exercise.lines = 30}
# set the seed for reproducibility
set.seed(302)

n = 50
betas <- c(1,2,3,0)
e <- rnorm(n, 0, sqrt(2.25))
covariance <- diag(c(1,1,1))
covariance[1,2] <- covariance[2,1] <- ____
# re-generate our predictors with this new covariance matrix
X <- mvrnorm(____, mu = ____, Sigma = ____)
# generate new responses
Xmat <- cbind(1, X)
y <- Xmat %*% betas + e

# fit the SLR and MLE models and extract coefficients
model1c <- lm(y ~ X[,1])
model2c <- lm(y ~ X[,2])
model3c <- lm(y ~ X[,3])
MLRc <- lm(y ~ X)

coefs <- c(coef(model1c)[2], coef(model2c)[2], coef(model3c)[2], coef(MLRc)[2:4])
names(coefs) <- c("X1", "X2", "X3", "X1", "X2", "X3")

# extract confidence intervals of slopes in all 4 models
confints <- rbind(confint(model1c)[2,], confint(model2c)[2,], confint(model3c)[2,],
                  confint(MLRc)[2:4,])


#plot the point estimates on a plot, where the position on the x-axis will be 
# 1, 2, 3 for the SLR values and 6,7,8 for the MLR values
xposition <- c(1,2,3,6,7,8)
plot(coefs ~ xposition,
     type="p", ylim=c(-3, 6), xlim=c(0,8), bty="n", xaxt="n",
     ylab="", xlab="")

# add the horizontal line of no effect at 0
abline(h=0, lty="dashed")

# adds a label to distinguish the points
text(2, 6, "Coefficients in SLRs")
text(7, 6, "Coefficients in MLR")

# loop to add confidence intervals and labels to plot
for(i in ____){
    # get the x position to plot at
    x <- xposition[i]
    # adds a label to distinguish the points
    text(x, -2.75, names(coefs)[i])
    # plots a vertical lines for the CI of each point
    lines(____, ____, lwd=____)
}
```

```{r, corr1-solution, eval=FALSE}
# set the seed for reproducibility
set.seed(302)

covariance <- diag(c(1,1,1))
covariance[1,2] <- covariance[2,1] <- 0.98
# re-generate our predictors with this new covariance matrix
X <- mvrnorm(n, mu = c(0,0,0), Sigma = covariance)
# generate new responses
Xmat <- cbind(1, X)
y <- Xmat %*% betas + e

# fit the SLR and MLE models and extract coefficients
model1c <- lm(y ~ X[,1])
model2c <- lm(y ~ X[,2])
model3c <- lm(y ~ X[,3])
MLRc <- lm(y ~ X)

coefs <- c(coef(model1c)[2], coef(model2c)[2], coef(model3c)[2], coef(MLRc)[2:4])
names(coefs) <- c("X1", "X2", "X3", "X1", "X2", "X3")

# extract confidence intervals of slopes in all 4 models
confints <- rbind(confint(model1c)[2,], confint(model2c)[2,], confint(model3c)[2,],
                  confint(MLRc)[2:4,])


#plot the point estimates on a plot, where the position on the x-axis will be 
# 1, 2, 3 for the SLR values and 6,7,8 for the MLR values
xposition <- c(1,2,3,6,7,8)
plot(coefs ~ xposition,
     type="p", ylim=c(-3, 6), xlim=c(0,8), bty="n", xaxt="n",
     ylab="", xlab="")

# add the horizontal line of no effect at 0
abline(h=0, lty="dashed")

# adds a label to distinguish the points
text(2, 6, "Coefficients in SLRs")
text(7, 6, "Coefficients in MLR")

# loop to add confidence intervals and labels to plot
for(i in 1:6){
    # get the x position to plot at
    x <- xposition[i]
    # adds a label to distinguish the points
    text(x, -2.75, names(coefs)[i])
    # plots a vertical lines for the CI of each point
    lines(c(x,x), confints[i,], lwd=3)
}
```

```{r, corr1-check}
grade_this_code()
```

```{r corr1-hint-1}
#---

# The covariance matrix covariance[1,2], covariance[2,1] should set to the new correlation value
covariance[1,2] <- covariance[2,1] <- 0.98

#---
```

```{r corr1-hint-2}
#--- 

# The syntax of the mvrnorm function can be found in the link below
https://www.rdocumentation.org/packages/rockchalk/versions/1.8.157/topics/mvrnorm

#---

# The covariance matrix covariance[1,2], covariance[2,1] should set to the new correlation value
covariance[1,2] <- covariance[2,1] <- 0.98

#---
```

```{r corr1-hint-3}
#---

# The second argument of the lines function needs a lower bound and the upper 
# bound of the line segment as a single vector input and remember CI information 
# is stored in rows
confints[i,]

#--- 

# The syntax of the mvrnorm function can be found in the link below
https://www.rdocumentation.org/packages/rockchalk/versions/1.8.157/topics/mvrnorm

#---

# The covariance matrix covariance[1,2], covariance[2,1] should set to the new correlation value
covariance[1,2] <- covariance[2,1] <- 0.98

#---
```

```{r q9, include=TRUE}
question("Based on the outputs, what do you notice about the estimates of the slopes now?",
         answer("Some slope estimates in the simple linear models are very different from the true values. "),
         answer("The widths of the confidence intervals in the multiple linear model are much wider."),
         answer("$X_1$ no longer appears significantly different from 0 in the presence of $X_2$ and $X_3$."),
         answer("$X_1$ and $X_2$ appear to have the same estimated slope in the simple linear models. "),
         answer("More than one of these are true.", correct = TRUE),
         message = "All options listed here are true", allow_retry = TRUE
)
```

We have seen some drastic changes in how our models use the data to estimate the true population relationship. It seems as if when two predictors are highly correlated, our confidence intervals get wider and this can lead us to potentially conclude that $X_1$ is not significantly related to the response when in reality the only predictor not related to the response is $X_3$. We also notice that in the simple linear models, when two predictors are highly correlated, the simple models estimate the same slope for both predictors, even though these are not the true population values.

If we compare the standard errors we estimate from each model, we can see how the correlation changes the standard errors. This can be visualized with  Bar Chart using the BarChart() function in the lessR package. We need to define the groups (each model's slope estimate) and the value we wish to plot (the standard errors of each model's slope estimate).

```{r corr2-setup, include=FALSE}
# set the seed for reproducibility
set.seed(302)

n = 50
betas <- c(1,2,3,0)
e <- rnorm(n, 0, sqrt(2.25))
covariance <- diag(c(1,1,1))
covariance[1,2] <- covariance[2,1] <- 0.98
# re-generate our predictors with this new covariance matrix
X <- mvrnorm(n, mu = c(0,0,0), Sigma = covariance)
# generate new responses
Xmat <- cbind(1, X)
y <- Xmat %*% betas + e

# fit the SLR and MLE models and extract coefficients
model1c <- lm(y ~ X[,1])
model2c <- lm(y ~ X[,2])
model3c <- lm(y ~ X[,3])
MLRc <- lm(y ~ X)

coefs <- c(coef(model1c)[2], coef(model2c)[2], coef(model3c)[2], coef(MLRc)[2:4])
names(coefs) <- c("X1", "X2", "X3", "X1", "X2", "X3")

# extract confidence intervals of slopes in all 4 models
confints <- rbind(confint(model1c)[2,], confint(model2c)[2,], confint(model3c)[2,],
                  confint(MLRc)[2:4,])


#plot the point estimates on a plot, where the position on the x-axis will be 
# 1, 2, 3 for the SLR values and 6,7,8 for the MLR values
xposition <- c(1,2,3,6,7,8)
plot(coefs ~ xposition,
     type="p", ylim=c(-3, 6), xlim=c(0,8), bty="n", xaxt="n",
     ylab="", xlab="")

# add the horizontal line of no effect at 0
abline(h=0, lty="dashed")

# adds a label to distinguish the points
text(2, 6, "Coefficients in SLRs")
text(7, 6, "Coefficients in MLR")

# loop to add confidence intervals and labels to plot
for(i in 1:6){
    # get the x position to plot at
    x <- xposition[i]
    # adds a label to distinguish the points
    text(x, -2.75, names(coefs)[i])
    # plots a vertical lines for the CI of each point
    lines(c(x,x), confints[i,], lwd=3)
}

```

```{r corr2, exercise = TRUE, exercise.lines = 35}
# extract standard errors of each slope from MLR model and SLR models
ses <- c(summary(model1c)$coef[2,2], ____,
         summary(model2c)$coef[2,2], ____,
         summary(model3c)$coef[2,2], ____)

# create group names
names <- c("X1 (SLR)", "X1 (MLR)","X2 (SLR)", "X2 (MLR)", "X3 (SLR)", "X3 (MLR)")

# create the Bar Chart
library(lessR)
BarChart(names, ses, data=data.frame(cbind(names, ____)), fill=c("purple", "purple", "cyan", "cyan", "orange", "orange"))
```

```{r corr2-solution, eval=FALSE}
# extract standard errors of each slope from MLR model and SLR models
ses <- c(summary(model1c)$coef[2,2], summary(MLRc)$coef[2,2],
         summary(model2c)$coef[2,2], summary(MLRc)$coef[3,2],
         summary(model3c)$coef[2,2], summary(MLRc)$coef[4,2])

# create group names
names <- c("X1 (SLR)", "X1 (MLR)","X2 (SLR)", "X2 (MLR)", "X3 (SLR)", "X3 (MLR)")

# create the Bar Chart
library(lessR)
BarChart(names, ses, data=data.frame(cbind(names, ses)), fill=c("purple", "purple", "cyan", "cyan", "orange", "orange"))
```

```{r corr2-check}
grade_this_code()
```

```{r corr2-hint-1}
#--- 

# You may find the following hints helpful
# 1. Use same method to isolate SEs from MLR
# 2. Remember we only want slope and no intercept of MLR
# 3. Standard errors are stored in column 2

#--- 
```


```{r q10, include=TRUE}
question("Approximately how many times higher are the bars for the slopes of $X_1$ and $X_2$ in the MLR model compared to the bars for the same slopes in the SLR model?",
         answer("About 5 times higher.", correct = TRUE),
         answer("About 10 times higher."),
         answer("About 15 times higher."),
         answer("About 20 times higher."), allow_retry = TRUE
)
```

So when we have predictors in a model that are highly correlated, our model will incorrectly estimate the standard errors of those coefficients. We see that the standard errors are **inflated** by a certain amount relative to how correlated those predictors are.

## Section 5: Impact of All Predictors Being Correlated
In this section, we will now introduce a correlation between all predictors.

Our approach will be similar to the simple linear model setup. We can first start with generating data in which all 3 predictors have a correlation of 0.9 and then fitting separate simple linear models and a multiple linear model. We shall also extract the estimated slopes from all models as well as the confidence intervals for those slopes. Finally, we will make a plot that displays the confidence intervals from all models.

```{r q110, exercise = TRUE, exercise.lines = 30}
n = 50
betas <- c(1,2,3,0)
e <- rnorm(n, 0, sqrt(2.25))
# first create a matrix of 0.9 everywhere, then change the diagonal to be 1
covariance <- matrix(rep(____, ____), nrow=3, ncol=3)
diag(covariance) <- c(____,____,____)
# re-generate our predictors with this new covariance matrix
X <- mvrnorm(n, mu = c(0,0,0), Sigma = covariance)
# generate new responses
Xmat <- cbind(1, X)
y <- Xmat %*% betas + e

# fit the SLR and MLE models and extract coefficients
model1d <- lm(y ~ X[,1])
model2d <- lm(y ~ X[,2])
model3d <- lm(y ~ X[,3])
MLRd <- lm(y ~ X)

coefs <- c(coef(model1d)[2], coef(model2d)[2], coef(model3d)[2], coef(MLRd)[2:4])
names(coefs) <- c("X1", "X2", "X3", "X1", "X2", "X3")

# extract confidence intervals of slopes in all 4 models
confints <- rbind(confint(model1d)[2,], confint(model2d)[2,], confint(model3d)[2,],
                  confint(MLRd)[2:4,])


#plot the point estimates on a plot, where the position on the x-axis will be 
# 1, 2, 3 for the SLR values and 6,7,8 for the MLR values
xposition <- c(1,2,3,6,7,8)
plot(____ ~ ____,
     type="p", ylim=c(-3, 6), xlim=c(0,8), bty="n", xaxt="n",
     ylab="", xlab="")

# add the horizontal line of no effect at 0
abline(h=0, lty="dashed")

# adds a label to distinguish the points
text(2, 6, "Coefficients in SLRs")
text(7, 6, "Coefficients in MLR")

# loop to add confidence intervals and labels to plot
for(i in 1:6){
    # get the x position to plot at
    x <- ____
    # adds a label to distinguish the points
    text(x, -2.75, names(coefs)[i])
    # plots a vertical lines for the CI of each point
    lines(c(x,x), confints[i,], lwd=3)
}
```

```{r q110-solution, eval=FALSE}
n = 50
betas <- c(1,2,3,0)
e <- rnorm(n, 0, sqrt(2.25))
# first create a matrix of 0.9 everywhere, then change the diagonal to be 1
covariance <- matrix(rep(0.9, 9), nrow=3, ncol=3)
diag(covariance) <- c(1,1,1)
# re-generate our predictors with this new covariance matrix
X <- mvrnorm(n, mu = c(0,0,0), Sigma = covariance)
# generate new responses
Xmat <- cbind(1, X)
y <- Xmat %*% betas + e

# fit the SLR and MLE models and extract coefficients
model1d <- lm(y ~ X[,1])
model2d <- lm(y ~ X[,2])
model3d <- lm(y ~ X[,3])
MLRd <- lm(y ~ X)

coefs <- c(coef(model1d)[2], coef(model2d)[2], coef(model3d)[2], coef(MLRd)[2:4])
names(coefs) <- c("X1", "X2", "X3", "X1", "X2", "X3")

# extract confidence intervals of slopes in all 4 models
confints <- rbind(confint(model1d)[2,], confint(model2d)[2,], confint(model3d)[2,],
                  confint(MLRd)[2:4,])


#plot the point estimates on a plot, where the position on the x-axis will be 
# 1, 2, 3 for the SLR values and 6,7,8 for the MLR values
xposition <- c(1,2,3,6,7,8)
plot(coefs ~ xposition,
     type="p", ylim=c(-3, 6), xlim=c(0,8), bty="n", xaxt="n",
     ylab="", xlab="")

# add the horizontal line of no effect at 0
abline(h=0, lty="dashed")

# adds a label to distinguish the points
text(2, 6, "Coefficients in SLRs")
text(7, 6, "Coefficients in MLR")

# loop to add confidence intervals and labels to plot
for(i in 1:6){
    # get the x position to plot at
    x <- xposition[i]
    # adds a label to distinguish the points
    text(x, -2.75, names(coefs)[i])
    # plots a vertical lines for the CI of each point
    lines(c(x,x), confints[i,], lwd=3)
}
```

```{r q110-check}
grade_this_code()
```

```{r q110-hint-1}
#---

# When creating the covariance matrix, we need to add value of new correlation and want diagonals to represent variances of 1
covariance <- matrix(rep(0.9, 9), nrow=3, ncol=3)
diag(covariance) <- c(1,1,1)

#---
```

```{r q110-hint-2}
#---

# For the plot function, we want a plot of estimates at the position we've assigned
# for each model

#---

# When creating the covariance matrix, we need to add value of new correlation and want diagonals to represent variances of 1
covariance <- matrix(rep(0.9, 9), nrow=3, ncol=3)
diag(covariance) <- c(1,1,1)

#---
```

```{r q11, include=TRUE}
question("Which model(s) seem to be estimating the true association of each predictor with the response?",
         answer("The multiple linear model. ", correct = TRUE),
         answer("Simple model with $X_1$.", message = "Remember that the beta coefficients should be beta=c(1, 2, 3, 0)"),
         answer("Simple model with $X_2$. ", message = "Remember that the beta coefficients should be beta=c(1, 2, 3, 0)"),
         answer("Simple model with $X_3$. ", message = "Remember that the beta coefficients should be beta=c(1, 2, 3, 0)"), allow_retry = TRUE
)
```

It appears that even with a correlation of 0.9 among all three of the predictors, the multiple linear model still estimates the correct relationship (more or less) with slightly wider confidence intervals around the estimates. So we still see *the impact of correlation on inflating the standard errors* (in this case, by about a factor of 2).

Will this always be the case? If we increase the correlation to 0.98, will we still obtain estimates that appear to reflect the true association? Repeat the previous part but now set the correlation to 0.98.

```{r q120, exercise = TRUE, exercise.lines = 30}
n = 50
betas <- c(1,2,3,0)
e <- rnorm(n, 0, sqrt(2.25))
# first create a matrix of 0.98 everywhere, then change the diagonal to be 1
covariance <- matrix(rep(____, 9), nrow=3, ncol=3)
diag(covariance) <- ____
# re-generate our predictors with this new covariance matrix
X <- mvrnorm(n, mu = c(0,0,0), Sigma = covariance)
# generate new responses
Xmat <- cbind(1, X)
y <- Xmat %*% betas + e

# fit the SLR and MLE models and extract coefficients
model1d <- lm(y ~ X[,1])
model2d <- lm(y ~ X[,2])
model3d <- lm(y ~ X[,3])
MLRd <- lm(y ~ X)

coefs <- c(coef(model1d)[2], coef(model2d)[2], coef(model3d)[2], coef(MLRd)[2:4])
names(coefs) <- c("X1", "X2", "X3", "X1", "X2", "X3")

# extract confidence intervals of slopes in all 4 models
confints <- rbind(confint(model1d)[2,], confint(model2d)[2,], confint(model3d)[2,],
                  confint(MLRd)[2:4,])


#plot the point estimates on a plot, where the position on the x-axis will be 
# 1, 2, 3 for the SLR values and 6,7,8 for the MLR values
xposition <- c(1,2,3,6,7,8)
plot(coefs ~ xposition,
     type="p", ylim=c(-3, 6), xlim=c(0,8), bty="n", xaxt="n",
     ylab="", xlab="")

# add the horizontal line of no effect at 0
abline(h=____, lty="dashed")

# adds a label to distinguish the points
text(2, 6, "Coefficients in SLRs")
text(7, 6, "Coefficients in MLR")

# loop to add confidence intervals and labels to plot
for(i in 1:6){
    # get the x position to plot at
    x <- xposition[i]
    # adds a label to distinguish the points
    text(x, -2.75, names(____)[i])
    # plots a vertical lines for the CI of each point
    lines(c(x,x), confints[i,], lwd=3)
}
```

```{r q120-solution}
n = 50
betas <- c(1,2,3,0)
e <- rnorm(n, 0, sqrt(2.25))
# first create a matrix of 0.9 everywhere, then change the diagonal to be 1
covariance <- matrix(rep(0.98, 9), nrow=3, ncol=3)
diag(covariance) <- c(1,1,1)
# re-generate our predictors with this new covariance matrix
X <- mvrnorm(n, mu = c(0,0,0), Sigma = covariance)
# generate new responses
Xmat <- cbind(1, X)
y <- Xmat %*% betas + e

# fit the SLR and MLE models and extract coefficients
model1d <- lm(y ~ X[,1])
model2d <- lm(y ~ X[,2])
model3d <- lm(y ~ X[,3])
MLRd <- lm(y ~ X)

coefs <- c(coef(model1d)[2], coef(model2d)[2], coef(model3d)[2], coef(MLRd)[2:4])
names(coefs) <- c("X1", "X2", "X3", "X1", "X2", "X3")

# extract confidence intervals of slopes in all 4 models
confints <- rbind(confint(model1d)[2,], confint(model2d)[2,], confint(model3d)[2,],
                  confint(MLRd)[2:4,])


#plot the point estimates on a plot, where the position on the x-axis will be 
# 1, 2, 3 for the SLR values and 6,7,8 for the MLR values
xposition <- c(1,2,3,6,7,8)
plot(coefs ~ xposition,
     type="p", ylim=c(-3, 6), xlim=c(0,8), bty="n", xaxt="n",
     ylab="", xlab="")

# add the horizontal line of no effect at 0
abline(h=0, lty="dashed")

# adds a label to distinguish the points
text(2, 6, "Coefficients in SLRs")
text(7, 6, "Coefficients in MLR")

# loop to add confidence intervals and labels to plot
for(i in 1:6){
    # get the x position to plot at
    x <- xposition[i]
    # adds a label to distinguish the points
    text(x, -2.75, names(coefs)[i])
    # plots a vertical lines for the CI of each point
    lines(c(x,x), confints[i,], lwd=3)
}
```

```{r q120-check}
grade_this_code()

```

```{r q120-hint-1}
#---

# When creating the covariance matrix, we need to add value of new correlation and want diagonals to represent variances of 1
covariance <- matrix(rep(0.98, 9), nrow=3, ncol=3)
diag(covariance) <- c(1,1,1)

#---
```

```{r q120-hint-2}
#---

# The abline function will let you draw a horizontal line with input h 

#---

# When creating the covariance matrix, we need to add value of new correlation and want diagonals to represent variances of 1
covariance <- matrix(rep(0.98, 9), nrow=3, ncol=3)
diag(covariance) <- c(1,1,1)

#---
```

```{r q12, include=TRUE}
question("Do any of these models appear to estimate the correct association of each predictor to the response?",
         answer("Yes"),
         answer("No", correct = TRUE),
         message = "So when all predictors are correlated to one another, it appears as if, in addition to the inflated variances visible in the wide confidence intervals, we also struggle to capture the true relationships present in the population.", allow_retry = TRUE
)
```

## Section 6: Impact of Correlation Between $X_1$ and $X_3$ 
In this section, we will now introduce a correlation between two of our predictors, $X_1$ and $X_3$. We have seen the general impact of strongly correlated predictors on the standard errors of slopes in a multiple linear model. Now we can see what happens in a slightly different scenario: we will still have two strongly correlated predictors ($X_1$ and $X_3$) but here we will forget to include $X_1$ in our multiple linear model. By doing this, we are misspecifying our model (estimating the wrong population relationship) and therefore automatically introducing bias into our estimates. However, we will see something else interesting occurs.

Recall that the true population relationship between the response and the three predictors is $$Y = 1  + 2X_1 + 3X_2 + 0X_3 + \epsilon, \quad \epsilon \sim N(0, 2.25).$$ This tells us that only $X_3$ has no association with the response in the presence of the other two predictors. 

For this section, we will set elements [1,3] and [3,1] in the covariance matrix to be 0.9, giving us predictors that are generated with a strong correlation between $X_1$ and $X_3$. We will still fit 3 separate simple linear models, but will now fit a multiple linear model using only $X_2$ and $X_3$. We will extract the slopes and confidence intervals from each model again. At the end, print the summary of the simple linear model with $X_2$ and the simple linear model with $X_3$.


```{r q13, exercise = TRUE, exercise.lines = 30}
n = 50
betas <- c(1,2,3,0)
e <- rnorm(n, 0, sqrt(2.25))
# edit the covariance matrix to introduce the correlation
covariance <- diag(c(1,1,1))
covariance[1,3] <- covariance[3,1] <- ____

# re-generate our predictors with this new covariance matrix
X <- mvrnorm(n, mu = c(0,0,0), Sigma = covariance)
# generate new responses
Xmat <-  ____
y <- Xmat %*% betas + e

# fit the SLR and MLE models and extract coefficients
model1e <- lm(y ~ ____)
model2e <- lm(y ~ ____)
model3e <- lm(y ~ ____)
MLRe <- lm(y ~ ____)

coefs <- c(coef(model1e)[2], coef(model2e)[2], coef(model3e)[2], coef(MLRe)[2:3])
names(coefs) <- c("X1", "X2", "X3", "X2", "X3")

# extract confidence intervals of slopes in all 4 models
confints <- rbind(confint(model1e)[2,], confint(model2e)[2,], confint(model3e)[2,],
                  confint(MLRe)[2:3,])

# print the model summaries for model2e and model3e
summary(model2e)
summary(model3e)

```

```{r q13-solution}
n = 50
betas <- c(1,2,3,0)
e <- rnorm(n, 0, sqrt(2.25))
# edit the covariance matrix to introduce the correlation
covariance <- diag(c(1,1,1))
# edit the covariance matrix to introduce the correlation
covariance <- diag(c(1,1,1))
covariance[1,3] <- covariance[3,1] <- 0.9

# re-generate our predictors with this new covariance matrix
X <- mvrnorm(n, mu = c(0,0,0), Sigma = covariance)
# generate new responses
Xmat <- cbind(1, X)
y <- Xmat %*% betas + e

# fit the SLR and MLE models and extract coefficients
model1e <- lm(y ~ X[,1])
model2e <- lm(y ~ X[,2])
model3e <- lm(y ~ X[,3])
MLRe <- lm(y ~ X[,-1])

coefs <- c(coef(model1e)[2], coef(model2e)[2], coef(model3e)[2], coef(MLRe)[2:3])
names(coefs) <- c("X1", "X2", "X3", "X2", "X3")

# extract confidence intervals of slopes in all 4 models
confints <- rbind(confint(model1e)[2,], confint(model2e)[2,], confint(model3e)[2,],
                  confint(MLRe)[2:3,])

# print the model summaries for model1d and model3d
summary(model2e)
summary(model3e)
```

```{r q13-check}
grade_this_code()

```

```{r q13-hint-1}
#---

# When creating that Xmat matrix, remember to add an intercept column using cbind() function
Xmat <- cbind(1, X)

#---
```

```{r q13-hint-2}
#---

# When fitting the SLR and MLE models 
# 1. Remember to select the column in X corresponding to the predictor you want to use
# 2. Recall that we don't want to use X1 in the MLR model

#---

# When creating that Xmat matrix, remember to add an intercept column using cbind() function
Xmat <- cbind(1, X)

#---
```


```{r q13MCQ, include=TRUE}
question("Which of the below is true regarding these two simple linear models?",
         answer("The slope of $X_3$ is significantly different from 0. "),
         answer("The slopes $X_3$ seems to indicate the same association (more or less) with $Y$ as $X_1$ should have."),
         answer("The slope of $X_2$ seems to be an reasonable estimate while the slope of $X_3$ seems to be biased."),
         answer("More than one of these is true.", correct = TRUE), allow_retry = TRUE
)
```

To visualize the effect of this strong correlation between $X_1$ and $X_3$ in the situation when $X_1$ is not included in the multiple linear model, we can produce the same plot as before. Pay particular attention to the effect of this correlation paired with fitting the wrong model as a multiple linear model.

```{r q14-setup}
n = 50
betas <- c(1,2,3,0)
e <- rnorm(n, 0, sqrt(2.25))
# edit the covariance matrix to introduce the correlation
covariance <- diag(c(1,1,1))
# edit the covariance matrix to introduce the correlation
covariance <- diag(c(1,1,1))
covariance[1,3] <- covariance[3,1] <- 0.9

# re-generate our predictors with this new covariance matrix
X <- mvrnorm(n, mu = c(0,0,0), Sigma = covariance)
# generate new responses
Xmat <- cbind(1, X)
y <- Xmat %*% betas + e

# fit the SLR and MLE models and extract coefficients
model1e <- lm(y ~ X[,1])
model2e <- lm(y ~ X[,2])
model3e <- lm(y ~ X[,3])
MLRe <- lm(y ~ X[,-1])

coefs <- c(coef(model1e)[2], coef(model2e)[2], coef(model3e)[2], coef(MLRe)[2:3])
names(coefs) <- c("X1", "X2", "X3", "X2", "X3")

# extract confidence intervals of slopes in all 4 models
confints <- rbind(confint(model1e)[2,], confint(model2e)[2,], confint(model3e)[2,],
                  confint(MLRe)[2:3,])

# print the model summaries for model1d and model3d
summary(model2e)
summary(model3e)
```

```{r q14, exercise = TRUE, exercise.lines = 30}
#plot the point estimates on a plot, where the position on the x-axis will be 
# 1, 2, 3 for the SLR values and 6,7 for the MLR values
xposition <- ____
plot(coefs ~ xposition,
     type="p", ylim=c(-3, 6), xlim=c(0,7), bty="n", xaxt="n",
     ylab="", xlab="")

# add the horizontal line of no effect at 0
abline(h=0, lty="dashed")

# adds a label to distinguish the points
text(2, 6, "Coefficients in SLRs")
text(6, 6, "Coefficients in MLR")

# loop to add confidence intervals and labels to plot
for(i in 1:5){
    # get the x position to plot at
    x <- xposition[i]
    # adds a label to distinguish the points
    text(x, -2.75, names(coefs)[i])
    # plots a vertical lines for the CI of each point
    lines(c(x,x), confints[i,], lwd=3)
}
```

```{r q14-solution}
#plot the point estimates on a plot, where the position on the x-axis will be 
# 1, 2, 3 for the SLR values and 6,7 for the MLR values
xposition <- c(1,2,3,6,7)
plot(coefs ~ xposition,
     type="p", ylim=c(-3, 6), xlim=c(0,7), bty="n", xaxt="n",
     ylab="", xlab="")

# add the horizontal line of no effect at 0
abline(h=0, lty="dashed")

# adds a label to distinguish the points
text(2, 6, "Coefficients in SLRs")
text(6, 6, "Coefficients in MLR")

# loop to add confidence intervals and labels to plot
for(i in 1:5){
    # get the x position to plot at
    x <- xposition[i]
    # adds a label to distinguish the points
    text(x, -2.75, names(coefs)[i])
    # plots a vertical lines for the CI of each point
    lines(c(x,x), confints[i,], lwd=3)
}
```

```{r q14-hint-1}
#---

# For the xposition, since we aren't estimating x1, we don't need a position for it in MLR model
xposition <- c(1,2,3,6,7)

#---
```

```{r q14MCQ, include=TRUE}
question("Which true association does the slope of $X_3$ from the multiple linear model represent?",
         answer("The association between $X_1$ and $Y$.", correct = TRUE),
         answer("The association between $X_2$ and $Y$."),
         answer("The association between $X_3$ and $Y$."),
         answer("None of the above."), allow_retry = TRUE
)
```

What we have just seen is an effect called an **unmeasured confounder** effect. It occurs when there is a strong correlation between predictors, but the predictor that is truly associated with the response is not measured and therefore cannot be included in a model. Due to the correlation, the predictor that is correlated with this variable, when included in the model, will capture the effect of this variable. This is because the correlation between them implies that they represent something similar and thus by omitting one, the effect is still captured by the variable that is included. 

This can be misleading because, contextually, it may not make sense for the variable that is used as a predictor to have this effect/association with $Y$. When this is observed in a model (e.g. an association that contextually does not make sense or agree with existing literature), it usually highlights that this variable is correlated with something that was unmeasured. This is a separate issue compared to **multicollinearity**, which only happens when predictors that are actually included in the model together happen to be highly correlated.

## Conclusion

To summarize the work we have done so far in this module, we will construct a new population relationship and induce various amounts of correlation between the predicts. 

Consider a population relationship of $$Y=0.5 + 4X_1+10X_2+\epsilon, \epsilon \sim N(0, 1.5)$$.

Using the tool below, select different covariance matrices to induce different amounts of correlation between the predictors $X_1$ and $X_2$.


```{r, context="render", echo=FALSE}
withMathJax()
radioButtons("corr", "Select the correlation to induce", 
       c("$$\\Sigma = \\begin{pmatrix} 1 & 0 \\\\
                          0 & 1 \\end{pmatrix}$$", 
         "$$\\Sigma = \\begin{pmatrix} 1 & -0.9 \\\\
                        -0.9 & 1 \\end{pmatrix}$$", 
         "$$\\Sigma = \\begin{pmatrix} 1 & -0.5 \\\\
                          -0.5 & 1 \\end{pmatrix}$$", 
         "$$\\Sigma = \\begin{pmatrix} 1 & -0.2 \\\\
                          -0.2 & 1 \\end{pmatrix}$$", 
         "$$\\Sigma = \\begin{pmatrix} 1 & 0.2 \\\\
                          0.2 & 1 \\end{pmatrix}$$",
         "$$\\Sigma = \\begin{pmatrix} 1 & 0.5 \\\\
                          0.5 & 1 \\end{pmatrix}$$",
         "$$\\Sigma = \\begin{pmatrix} 1 & 0.9 \\\\
                          0.9 & 1 \\end{pmatrix}$$")
       )

plotOutput("plot")
```


```{r, context="server"}

  output$plot <- renderPlot({
    
    
plt_function <- function(corr_input){
    set.seed(302)

    n = 50
    betas <- c(0.5, 4, 10)
    e <- rnorm(n, 0, sqrt(1.5))
    # replace elements [1,2] and [2,1] with 0.5 to incorporate the correlation
    covariance <- diag(c(1,1))
    covariance[1,2] <- covariance[2,1] <- corr_input
    # re-generate our predictors with this new covariance matrix
    X <- mvrnorm(n, mu = c(0,0), Sigma = covariance)
    # generate new responses
    Xmat <- cbind(1, X)
    y <- Xmat %*% betas + e
    
    # fit the SLR and MLE models and extract coefficients
    model1c <- lm(y ~ X[,1])
    model2c <- lm(y ~ X[,2])
    MLRc <- lm(y ~ X)
    
    coefs <- c(coef(model1c)[2], coef(model2c)[2], coef(MLRc)[2:3])
    names(coefs) <- c("X1", "X2", "X1", "X2")
    
    # extract confidence intervals of slopes in all 4 models
    confints <- rbind(confint(model1c)[2,], confint(model2c)[2,],
                      confint(MLRc)[2:3,])
    
    
    #plot the point estimates on a plot, where the position on the x-axis will be 
    # 1, 2, 3 for the SLR values and 6,7,8 for the MLR values
    xposition <- c(1,2,6,7)
    plot(coefs ~ xposition,
         type="p", ylim=c(-10, 20), xlim=c(0,8), bty="n", xaxt="n",
         ylab="", xlab="")
    
    # add the horizontal line of no effect at 0
    abline(h=0, lty="dashed")
    
    # adds a label to distinguish the points
    text(2, 20, "Coefficients in SLRs")
    text(7, 20, "Coefficients in MLR")
    
    # loop to add confidence intervals and labels to plot
    for(i in 1:4){
        # get the x position to plot at
        x <- xposition[i]
        # adds a label to distinguish the points
        text(x, -2.75, names(coefs)[i])
        # plots a vertical lines for the CI of each point
        lines(c(x,x), confints[i,], lwd=3)
    }
}
   
   if(input$corr == "$$\\Sigma = \\begin{pmatrix} 1 & 0 \\\\
                          0 & 1 \\end{pmatrix}$$"){
     plt_function(0)
   }
   else if(input$corr == "$$\\Sigma = \\begin{pmatrix} 1 & -0.9 \\\\
                        -0.9 & 1 \\end{pmatrix}$$"){
     plt_function(-0.9)
   }
    else if(input$corr == "$$\\Sigma = \\begin{pmatrix} 1 & -0.5 \\\\
                          -0.5 & 1 \\end{pmatrix}$$"){
     plt_function(-0.5)
    }
    else if(input$corr == "$$\\Sigma = \\begin{pmatrix} 1 & -0.2 \\\\
                          -0.2 & 1 \\end{pmatrix}$$"){
     plt_function(-0.2)
    }
    else if(input$corr == "$$\\Sigma = \\begin{pmatrix} 1 & 0.2 \\\\
                          0.2 & 1 \\end{pmatrix}$$"){
     plt_function(0.2)
    }
    else if(input$corr == "$$\\Sigma = \\begin{pmatrix} 1 & 0.5 \\\\
                          0.5 & 1 \\end{pmatrix}$$"){
     plt_function(0.5)
    }
    else if(input$corr == "$$\\Sigma = \\begin{pmatrix} 1 & 0.9 \\\\
                          0.9 & 1 \\end{pmatrix}$$"){
     plt_function(0.9)
   }

})
```


```{r FinalMCQ, echo=FALSE}
question("Using the example above, which statement is true regarding how correlation impacts simple linear models and multiple linear models?",
      answer("The direction of the correlation impacts predicted coefficients in the simple linear model."), 
     answer("The strength of the correlation impacts predicted coefficients in the simple linear model."),
     answer("The confidence intervals for predictors in the multiple linear model are larger when the absolute value of the correlation is larger."),
     answer("The predictors in the multiple linear model are more robust against multicollinearity than the simple linear model."),
     answer("All of the above are correct.", correct=TRUE)
     allow_retry = T)
```
## Wrap-Up
Congratulations – You have successfully completed this learning module! By now, you should be familiar with:


