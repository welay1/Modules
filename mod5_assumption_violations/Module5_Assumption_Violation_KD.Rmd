---
title: 'STA302 Module 5: Assumption Violation'
output: learnr::tutorial
#css: "css/learnr_302.css"
runtime: shiny_prerendered
---

```{r setup, include = FALSE}
#install.packages("remotes", repos = "https://cloud.r-project.org/")

# These are the packages you will need for this activity
packages_needed <- c("tidyverse","learnr", "MASS")

package.check <- lapply(
  packages_needed,
  FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE, repos = "http://cran.us.r-project.org")
    }
  }
)

# Credit: package.check based on a helpful post from Vikram Baliga https://vbaliga.github.io/verify-that-r-packages-are-installed-and-loaded/

#remotes::install_github("rstudio/gradethis")


library(tidyverse)
library(shiny)
library(ggplot2)
options(repos = c(CRAN = "https://cloud.r-project.org"))
install.packages("broom")
library(broom)
library(learnr)
library(remotes)
library(gradethis)

gradethis::gradethis_setup()


knitr::opts_chunk$set(echo = FALSE)

options(tutorial.storage = list(
  save_object = function(...){},
  get_object = function(...) NULL,
  get_objects = function(...) list(),
  remove_all_objects = function(...){}))

#update.packages()
```


## Introduction

Welcome to Module 5: **Interpretation and Use of Indicators and Interactions**. In this module, you will learn: 

* To use R to generate data from a population with specified properties. 

* To use R to describe how a simulation is used to understand statistical properties of estimators

* To adapt existing simulation code in R to study violated assumptions of linear regression models. 

* To explain how violations of model assumptions change the coverage of confidence intervals of coefficients. 

Remember that if you get stuck on a coding exercise, you can click the **Hints** button for help. 

## Section 1 - Setting up a simulation

First, we will set up a simulation. We will set the simulation up such that all assumptions of linear regression will hold. We will then deliberately break each assumption and explore the effects. 

Usually, we do not know the true population relationship -- the whole purpose of linear regression is to *estimate* this relationship. However, for the purpose of this exercise, we will assume we do know this information, as we will be replicating drawing samples from some population. The steps of the simulation will be as follows: 

1. We will establish the "ground truth" -- the true population relationship between the predictors and response. 

2. Then, we take many samples from this population to obtain an estimate of the sampling distribution of each coefficient. 

3. Finally, we compute our confidence interval for each coefficient in each sample we take and calculate the **<mark>coverage</mark>**, or the proportion of intervals that contain the true parameter value. 

Why do we care about the coverage? Well, recall how we define a 95% confidence interval: *given that the assumptions of linear regression are met*, if we were to take many samples from a population and calculate a 95% confidence interval for each sample, then approximately 95% of the intervals would contain the true population parameter. Therefore, calculating the coverage for a simulation can tell us whether the linear regression assumptions are met. 

### Part 1.1: Sampling from the population

We will set up our first sample such that all linear regression assumptions are met. We will assume the following regression model is the true population relationship: 

$$Y = 1 + 2X_1 + 3X_2 + \epsilon, \text{     where   } \epsilon \sim N(0,1).$$
Our first step is generating a sample of size $n=50$ from this population and fitting a linear model to that sample. This will give us estimates of the coefficients of $X_1$ and $X_2$. 

Using the information above, fill in the blanks in the code to sample from the population and fit a linear model. 

```{r sim1-setup, include=F}
set.seed(302)
```

```{r sim1, exercise = TRUE, exercise.lines = 20}
# Set up parameters of the population errors
error_mean = 0
error_sd = 1
sample_size = 50

# Generate a sample from the true distribution of errors
e <- rnorm(___, ___, ___)

# Generate the predictors
x1<-  rnorm(sample_size, mean = 0, sd = 5)
x2<-  rnorm(sample_size, mean = 1, sd = 1)

# use the predictors and the errors to get your response
y <- ___

#Fit a linear regression model and summarize the output
model <- lm(___)
summary(model)
```

```{r sim1-solution, eval=FALSE}
# Set up parameters of the population errors
error_mean = 0
error_sd = 1
sample_size = 50

# Generate a sample from the true distribution of errors
e <- rnorm(sample_size, error_mean, error_sd)

# Generate the predictors
x1<-  rnorm(sample_size, mean = 0, sd = 5)
x2<-  rnorm(sample_size, mean = 1, sd = 1)

# use the predictors and the errors to get your response
y <- 1 + 2*x1 + 3*x2 + e

#Fit a linear regression model and summarize the output
model <- lm(y ~ x1 + x2)
summary(model)
```

```{r sim1-check}
grade_this_code()
```

```{r sim1-hint-1}
# ---

# The syntax of the rnorm function (for getting Normal errors) is
rnorm(sample size, mean, sd)

# It's a good idea to use the names we gave to the parameters in the first lines of the code (see "set up parameters...")

# ---
```

```{r sim1-hint-2}
# ---

# Note that Y is generated using the relationship noted in the instructions: 
Y = 1 + 2*x1 + 3*x2 + e

# ---

# The syntax of the rnorm function (for getting Normal errors) is
rnorm(sample size, mean, sd)

# It's a good idea to use the names we gave to the parameters in the first lines of the code (see "set up parameters...")

# ---
```

```{r sim1-hint-3}
# ---

# You may find the following function helpful when fitting linear regression: 
lm(response~predictors)

# ---

# Note that Y is generated using the relationship noted in the instructions: 
Y = 1 + 2*x1 + 3*x2 + e

# ---

# The syntax of the rnorm function (for getting Normal errors) is
rnorm(sample size, mean, sd)

# It's a good idea to use the names we gave to the parameters in the first lines of the code (see "set up parameters...")

# ---
```

```{r q1, include=TRUE}
question("Based on the summary for your model, which of the following statements is correct?",
         answer("The residual standard error has 48 degrees of freedom.", message="The degrees of freedome for the residual standard error is 50-2-1 or 47."),
         answer(sprintf("We have no evidence against the claim that the coefficient for $X_1$ is 0."), message = sprintf("Since the p-value for $X_1 < 2e-16$, this tells us that we have evidence that the slope of $X_1$ is not 0.")),
         answer(sprintf("We have strong evidence that the true coefficient for $X_1$ is 0. "), message = sprintf("Since the p-value for $X_1 < 2e-16$, this tells us that we have evidence that the slope of $X_1$ is not 0.")),
         answer(sprintf("We have strong evidence against the claim that the coefficient for $X_2$ is 0."), correct = TRUE,
         message = sprintf("Since the p-value for $X_2 < 2e-16$, this tells us that $X_2$ is linearly related to $Y$ and so the true slope of $X_2$ is not 0.")), allow_retry = TRUE
)
```

### Part 1.2: Repeat Sampling from the Population Many Times

To understand how confidence intervals *should* behave if all assumptions hold, we will take 2000 samples from the population. We will then compute confidence intervals for $X_1$ and $X_2$ for each sample, resulting in 2000 intervals for each predictor. We can then see the *coverage* of the interval. 

We will generate 2000 samples using a *for loop*. This allows us to repeat the same process for a desired number of times. In each loop, we will take a sample, fit a model, and save the 95\% confidence interval for each coefficient. 

Fill in the gaps below to run 2000 iterations of the simulation and save the resulting confidence interval of each coefficient for each iteration.

```{r confint1-setup, include=F}
set.seed(302)
```

```{r confint1, exercise = TRUE, exercise.lines = 26}
# set up the parameters like before
error_mean = 0
error_sd = 1
sample_size = 50
# include how many samples you want to take below
num_runs = _____

# Initialize how you will store your results
# Each row of the matrix will store an interval that contains 2 numbers
# We will have as many rows as times we are sampling our population
Beta0CI <- Beta1CI <- Beta2CI <- matrix(NA, nrow=____, ncol = ____)

# the for loop will run the code below as many times as we have samples
for (i in 1:____){
  e <- rnorm(sample_size, mean = error_mean, sd=error_sd)
  x1<-  rnorm(sample_size, mean = 0, sd = 5)
  x2<-  rnorm(sample_size, mean = 1, sd = 1)
  y <- 1 + 2*x1 + 3*x2 + e
  # Fit the model and compute the confidence interval
  model <- lm(y ~ x1 + x2)
  Beta0CI[i,] <- confint(model)[1,]
  Beta1CI[i,] <- ____
  Beta2CI[i,] <- ____
}
```

```{r confint1-solution, eval=FALSE}
# set up the parameters like before
error_mean = 0
error_sd = 1
sample_size = 50
# include how many samples you want to take below
num_runs = 2000

# Initialize how you will store your results
# Each row of the matrix will store an interval that contains 2 numbers
# We will have as many rows as times we are sampling our population
Beta0CI <- Beta1CI <- Beta2CI <- matrix(NA, nrow=num_runs, ncol = 2)

# the for loop will run the code below as many times as we have samples
for (i in 1:num_runs){
  e <- rnorm(sample_size, mean = error_mean, sd=error_sd)
  x1<-  rnorm(sample_size, mean = 0, sd = 5)
  x2<-  rnorm(sample_size, mean = 1, sd = 1)
  y <- 1 + 2*x1 + 3*x2 + e
  # Fit the model and compute the confidence interval
  model <- lm(y ~ x1 + x2)
  Beta0CI[i,] <- confint(model)[1,]
  Beta1CI[i,] <- confint(model)[2,]
  Beta2CI[i,] <- confint(model)[3,]
}
```

```{r confint1-check}
grade_this_code()
```

```{r confint1-hint-1}
# ---

# We gave the number of samples a name, num_runs. You can call this in other parts of the code:
num_runs = 2000
for(i in 1:num_runs){
}

# ---
```

```{r confint1-hint-2}
# ---

# The following hint might be helpful if you are stuck on nrow and ncol:

# Number of rows (nrow) is the number of times the simulation is repeated

# Number of columns (ncol) is how many values we need to store to represent a confidence interval 

# ---

# We gave the number of samples a name, num_runs. You can call this in other parts of the code:
num_runs = 2000
for(i in 1:num_runs){
}

# ---

```

```{r confint1-hint-3}
# ---

# The following command can be used to extract a confidence interval of a model
confint(model)
# and it will give us 2 values in each row, with as many rows as coefficients

# ---

# The following hint might be helpful if you are stuck on nrow and ncol:

# Number of rows (nrow) is the number of times the simulation is repeated

# Number of columns (ncol) is how many values we need to store to represent a confidence interval 

# ---

# We gave the number of samples a name, num_runs. You can call this in other parts of the code:
num_runs = 2000
for(i in 1:num_runs){
}

# ---
```



### Part 1.3: Estimating the Confidence Interval Coverage

As a reminder, coverage here refers to the **proportion of confidence intervals based on samples from this population that contain the true value of the coefficient.** If we simulate many 95% confidence intervals from a population, the coverage should be about 95% if all assumptions are met. If we get a percentage very different from our confidence level, that is a clue that assumptions may be violated. 

By taking the 2000 samples from this population and computing confidence intervals each time, we can check if our 95\% confidence intervals actually capture the true value of the coefficient 95\% of the time. So for a set of confidence intervals $[a_1, b_1], [a_2, b_2], [a_3, b_3], ... , [a_{2000}, b_{2000}]$, we find the coverage by 
$$\frac{\sum_{i=1}^{2000}1_{\{\beta \in [a_i, b_i]\}}}{2000}$$, where $1_{{\beta...}}$ is an indicator function taking $1$ if $\beta$ is in the interval, and $0$ otherwise. 

To code this process, we can break it up into four steps:

   1. Check if the true value of the coefficient is LARGER than the lower bound of the computed confidence interval (i.e. $a \leq \beta$ or equivalently $\beta \geq a$).
  2. Check if the true value of the coefficient is also SMALLER than the upper bound of the computed confidence interval (i.e. $b \geq \beta$ or equivalently $\beta \leq b$). 
   3. Count/Sum the number of simulated samples where the true value of the coefficient was BOTH larger than the lower bound and smaller than the upper bound of the confidence interval.
  4. Divide by the number of samples you took, i.e. the number of loops you did in your simulation.

Below is an example of how the coverage is computed for the confidence interval of the intercept. `print()` just prints the results, and `paste0()` just lets us add a label of what is being printed to the output. Use the code for the intercept to help you fill in the missing pieces for estimating the coverage for the slope of $X_1$ and the slope of $X_2$.

```{r confint1_1-setup, include=F}
set.seed(302)
error_mean = 0
error_sd = 1
sample_size = 50
# include how many samples you want to take below
num_runs = 2000

# Initialize how you will store your results
# Each row of the matrix will store an interval that contains 2 numbers
# We will have as many rows as times we are sampling our population
Beta0CI <- Beta1CI <- Beta2CI <- matrix(NA, nrow=2000, ncol = 2)

# the for loop will run the code below as many times as we have samples
for (i in 1:2000){
  e <- rnorm(sample_size, mean = error_mean, sd=error_sd)
  x1<-  rnorm(sample_size, mean = 0, sd = 5)
  x2<-  rnorm(sample_size, mean = 1, sd = 1)
  y <- 1 + 2*x1 + 3*x2 + e
  # Fit the model and compute the confidence interval
  model <- lm(y ~ x1 + x2)
  Beta0CI[i,] <- confint(model)[1,]
  Beta1CI[i,] <- confint(model)[2,]
  Beta2CI[i,] <- confint(model)[3,]
}
```

```{r confint1_1, exercise = TRUE, exercise.setup = "confint1_1-setup"}
# This is how we compute the coverage for the intercept:
print(paste0("Coverage for Intercept: ", sum(Beta0CI[,1] <=1 & Beta0CI[,2] >=1)/num_runs))

# use the same template as aboce to finish the next two lines
print(paste0("Coverage for Slope X1: ", sum(___[,1] <=2 & ___[,2] >=2)/num_runs))
print(paste0("Coverage for Slope X2: ", sum(Beta2CI[,1] <=___ & Beta2CI[,2] >=___)/num_runs))
```

```{r confint1_1-solution, eval=FALSE}
# This is how we compute the coverage for the intercept:
print(paste0("Coverage for Intercept: ", sum(Beta0CI[,1] <=1 & Beta0CI[,2] >=1)/num_runs))

# use the same template as above to finish the next two lines
print(paste0("Coverage for Slope X1: ", sum(Beta1CI[,1] <=2 & Beta1CI[,2] >=2)/num_runs))
print(paste0("Coverage for Slope X2: ", sum(Beta2CI[,1] <=3 & Beta2CI[,2] >=3)/num_runs))
```


```{r confint1_1-check}
grade_this_code()
```



```{r confint1_1-hint-1}
# ---

# To get the coverage of the intervals, remember that we are checking:

# 1. if the lower value of our interval is less than the true value of the parameter, for example
Beta0CI[,1] <=1 
# 2. AND (&) if it's also true that the upper value of our interval is greater than the true value of the parameter (this is equivalent to then asking is the truth in my interval), for example
Beta0CI[,2]
# 3. then we sum up how many times the interval contains the truth
sum(Beta0CI[,1] <=1 & Beta0CI[,2] >=1)
# 4. and lastly divide by the number of samples we collected to get a proportion
sum(Beta0CI[,1] <=1 & Beta0CI[,2] >=1)/num_runs

# ---

```

```{r confint1_1-hint-2}
# ---

# Remember what we called the objects that store our confidence intervals:
Beta0CI
Beta1CI
Beta2CI
# each one has 2000 rows of information and 2 columns

# ---

# To get the coverage of the intervals, remember that we are checking:

# 1. if the lower value of our interval is less than the true value of the parameter, for example
Beta0CI[,1] <=1 
# 2. AND (&) if it's also true that the upper value of our interval is greater than the true value of the parameter (this is equivalent to then asking is the truth in my interval), for example
Beta0CI[,2]
# 3. then we sum up how many times the interval contains the truth
sum(Beta0CI[,1] <=1 & Beta0CI[,2] >=1)
# 4. and lastly divide by the number of samples we collected to get a proportion
sum(Beta0CI[,1] <=1 & Beta0CI[,2] >=1)/num_runs

# ---
```

```{r confint1_1-hint-3}
# The true values of the parameters were used to get the response as
y <- 1 + 2*x1 + 3*x2 + e

# ---

# Remember what we called the objects that store our confidence intervals:
Beta0CI
Beta1CI
Beta2CI
# each one has 2000 rows of information and 2 columns

# ---

# To get the coverage of the intervals, remember that we are checking:

# 1. if the lower value of our interval is less than the true value of the parameter, for example
Beta0CI[,1] <=1 
# 2. AND (&) if it's also true that the upper value of our interval is greater than the true value of the parameter (this is equivalent to then asking is the truth in my interval), for example
Beta0CI[,2]
# 3. then we sum up how many times the interval contains the truth
sum(Beta0CI[,1] <=1 & Beta0CI[,2] >=1)
# 4. and lastly divide by the number of samples we collected to get a proportion
sum(Beta0CI[,1] <=1 & Beta0CI[,2] >=1)/num_runs

# ---
```


## Section 2 - Violated Homoscedasticity/Non-Constant Variance

In this part, we will examine the **<mark>homoscedasticity</mark>** (constant variance) assumption and how the violation of it could impact the confidence intervals we would build for the estimated coefficients.

Let's start by reviewing the definition for the homoscedasticity assumption.  

```{r q3, include=TRUE}
question("Which of the following statements is guaranteed to satisfy the homoscedasticity assumption?",
         answer(sprintf("$E(\\epsilon) = 0$"), message = "While the expectation of the error term being zero is a critical assumption for an unbiased regression model, it does not address homoscedasticity. Homoscedasticity concerns the spread or variability of the error terms, not their central tendency (expectation)."),
         answer(sprintf("$Var(\\epsilon \\mid X = 1) = Var(\\epsilon \\mid X = 2) = 1.$"), message = sprintf("This statement checks for equal variance at two specific points, which is not sufficient to establish homoscedasticity across the entire range of X. Homoscedasticity requires the variance of the error terms to be constant across all values of the independent variable, not just at individual points.")),
         answer(sprintf("$Var(\\epsilon \\mid X = x) = \\sigma^2 x^2$ where $\\sigma^2$ is a constant."), message = sprintf("This condition indicates heteroscedasticity, not homoscedasticity, as the variance of the error term is explicitly modeled as increasing with the square of X. Homoscedasticity is violated here because the variability of the error terms depends on the value of X.")),
         answer(sprintf("$Var(\\epsilon \\mid X = x) = σ^2.$ where $\\sigma^2$ is a constant."), correct = TRUE,
         message = "This statement encapsulates the essence of homoscedasticity, where the variance of the error terms remains constant across different levels of the independent variable, X."), allow_retry = TRUE
)
```


Now let's break the homoscedasticity assumption in our simulation. We will use the same simulation template as the previous section, but instead of having all assumptions hold, we will break constant variance. 

To do this, all we need to change is the distribution that dictates the population errors. Remember that, when all assumptions hold, the population relationship is $$Y = 1 + 2X_1 + 3X_2 + \epsilon, \text{     where   } \epsilon \sim N(0,1)$$ where the constant error variance is satisfied because the normal distribution has a variance of 1, a constant value that does *not* depend on the values of the predictors.

To create non-constant variance, we will make it so that the population relationship is now $$Y = 1 + 2X_1 + 3X_2 + \epsilon, \text{     where   } \epsilon \sim N(0,|X_1|).$$ Notice that as the values of $X_1$ change, so will the variance of the errors, thereby violating constant variance. We can add this into our simulation by setting `sd = abs(x1)` when we generate the errors below.

```{r confint2, exercise = TRUE, exercise.lines = 30}
# Set up parameters
error_mean = 0
num_runs = 2000
sample_size = 50

# Initialize how you will store your results
# Each row of the matrix represents 1 simulation loop
Beta0CI <- Beta1CI <- Beta2CI <- matrix(NA, nrow=num_runs, ncol = 2)

# repeat the commands num_run times
for (i in 1:num_runs){
  # sample the predictors
  x1<-  rnorm(sample_size, mean = 0, sd = 5)
  x2<-  rnorm(sample_size, mean = 1, sd = 1)
  # sample the errors
  # Let's break constant variance by making it depend on the value of x1
  e <- rnorm(sample_size, mean = error_mean, sd=____)
  # sample the responses
  y <- 1 + 2*x1 + 3*x2 + e
  # Fit the model and compute the confidence interval
  model <- lm(___ ~ ____)
  # Store the results so you won't lose it when we run the next run
  Beta0CI[i,] <- confint(model)[1,]
  Beta1CI[i,] <- confint(model)[2,]
  Beta2CI[i,] <- confint(model)[3,]
}
```


```{r confint2-solution, eval=FALSE}
# Set up parameters
error_mean = 0
num_runs = 2000
sample_size = 50

# Initialize how you will store your results
# Each row of the matrix represents 1 simulation loop
Beta0CI <- Beta1CI <- Beta2CI <- matrix(NA, nrow=num_runs, ncol = 2)

# repeat the commands num_run times
for (i in 1:num_runs){
  # sample the predictors
  x1<-  rnorm(sample_size, mean = 0, sd = 5)
  x2<-  rnorm(sample_size, mean = 1, sd = 1)
  # sample the errors
  # Let's break constant variance by making it depend on the value of x1
  e <- rnorm(sample_size, mean = error_mean, sd= abs(x1))
  # sample the responses
  y <- 1 + 2*x1 + 3*x2 + e
  # Fit the model and compute the confidence interval
  model <- lm(y ~ x1 + x2)
  # Store the results so you won't lose it when we run the next run
  Beta0CI[i,] <- confint(model)[1,]
  Beta1CI[i,] <- confint(model)[2,]
  Beta2CI[i,] <- confint(model)[3,]
}
```


```{r confint2-check}
grade_this_code()
```

```{r confint2-hint-1}
# ---

# to create non-constant variance, set
sd = abs(x1)
# in the sampling of the errors on line 17

# ---
```

Now that you have your 2000 samples from the population, have fit a linear model to each one, and computed the confidence intervals for each coefficient, we can look at whether the coverage of each confidence interval is changed due to the violation of constant variance. Fill in the blanks below to complete the coverage calculation for the CI of the slope of $X_1$ and $X_2$. 



```{r confint2_1-setup, include=F}
# Set up parameters
error_mean = 0
num_runs = 2000
sample_size = 50

# Initialize how you will store your results
# Each row of the matrix represents 1 simulation loop
Beta0CI <- Beta1CI <- Beta2CI <- matrix(NA, nrow=num_runs, ncol = 2)

# repeat the commands num_run times
for (i in 1:num_runs){
  # sample the predictors
  x1<-  rnorm(sample_size, mean = 0, sd = 5)
  x2<-  rnorm(sample_size, mean = 1, sd = 1)
  # sample the errors
  # Let's break constant variance by making it depend on the value of x1
  e <- rnorm(sample_size, mean = error_mean, sd=abs(x1))
  # sample the responses
  y <- 1 + 2*x1 + 3*x2 + e
  # Fit the model and compute the confidence interval
  model <- lm(y ~ x1+x2)
  # Store the results so you won't lose it when we run the next run
  Beta0CI[i,] <- confint(model)[1,]
  Beta1CI[i,] <- confint(model)[2,]
  Beta2CI[i,] <- confint(model)[3,]
}
```

```{r confint2_1, exercise=TRUE, exercise.setup="confint2_1-setup"}
# Here is how the coverage is checked for the intercept CI

print(paste0("Coverage for Intercept: ", sum(Beta0CI[,1] <=1 & Beta0CI[,2] >=1)/num_runs))

# fill in the coverage for the two slopes:
print(paste0("Coverage for Slope X1: ", sum(___[,1] <= ___ & ___[,2] >= ___)/num_runs))
print(paste0("Coverage for Slope X2: ", sum(Beta2CI[,___] <= ___ & Beta2CI[,___] >= ___)/num_runs))
```

```{r confint2_1-solution, eval=FALSE}
# Here is how the coverage is checked for the intercept CI

print(paste0("Coverage for Intercept: ", sum(Beta0CI[,1] <=1 & Beta0CI[,2] >=1)/num_runs))

# fill in the coverage for the two slopes:
print(paste0("Coverage for Slope X1: ", sum(Beta1CI[,1] <= 2 & Beta1CI[,2] >= 2)/num_runs))
print(paste0("Coverage for Slope X2: ", sum(Beta2CI[,1] <= 3 & Beta2CI[,2] >= 3)/num_runs))
```


```{r confint2_1-check}
grade_this_code()
```


```{r confint2_1-hint-1}
# --- 

# Remember what we called the objects that store our confidence intervals:
Beta0CI
Beta1CI
Beta2CI
# each one has 2000 rows of information and 2 columns

# --- 
```

```{r confint2_1-hint-2}
# --- 

# Remember that each storage object for the confidence intervals has
    # 2000 rows and 2 columns with lower bound in column 1 and upper in column 2
Beta0CI <- Beta1CI <- Beta2CI <- matrix(NA, nrow=num_runs, ncol = 2)

# --- 

# Remember what we called the objects that store our confidence intervals:
Beta0CI
Beta1CI
Beta2CI
# each one has 2000 rows of information and 2 columns

# --- 
```

```{r confint2_1-hint-3}
# ---

# The true values of the parameters were used to get the response as
y <- 1 + 2*x1 + 3*x2 + e

# --- 

# Remember that each storage object for the confidence intervals has
    # 2000 rows and 2 columns with lower bound in column 1 and upper in column 2
Beta0CI <- Beta1CI <- Beta2CI <- matrix(NA, nrow=num_runs, ncol = 2)

# --- 

# Remember what we called the objects that store our confidence intervals:
Beta0CI
Beta1CI
Beta2CI
# each one has 2000 rows of information and 2 columns

# --- 
```



Now that you've computed the coverage of each coefficient's confidence interval under non-constant variance, we need to compare that to what we saw when all assumptions held. Recall that when our assumptions held, we obtained approximately 95\% coverage when we computed a 95\% confidence interval.

```{r q4, include=TRUE}
question(sprintf("Compared to what you saw for the coverage of $X_1$ when all assumptions held, which of the following statements is true?"),
         answer(sprintf("An error occurs when we try to compute the confidence interval when assumptions don't hold."), message = "This statement is not entirely accurate. While it's not strictly an 'error' to compute a confidence interval when assumptions are violated, the resulting interval may not have the intended properties, giving us misleading results."),
         answer(sprintf("The coverage is higher when assumptions don't hold."),message = "In statistical terms, 'coverage' refers to the proportion of time that the confidence intervals contain the true parameter value. If the underlying assumptions are violated, the coverage is not guaranteed to be higher; in fact, it's more likely to be lower or different from the expected level."),
         answer(sprintf("The coverage is lower when assumptions don't hold."), correct = TRUE, message = "When the statistical assumptions underlying the computation of a confidence interval are not met, the actual coverage rate of the interval can be lower than the nominal coverage rate. This means that the interval will contain the true parameter value less often than expected."),
         answer(sprintf("The coverage is exactly the same when assumptions don't hold.", message = "Since the assumption no longer holds, we would expect to see some change in the coverage. Further, simulations inject randomness so it's very unlikely that the coverage would be exactly the same"),message = "The validity of a confidence interval is heavily dependent on the underlying assumptions of the statistical model, such as normality, independence, and homoscedasticity. When these assumptions are not met, the actual coverage of the interval is likely to deviate from the nominal coverage."),
         allow_retry = TRUE
)
```



## Section 3 - Violated Linearity/Model Misspecification

We saw that when we violate the constant variance assumption, the coverage was lower than it should have been. Now we will examine the linearity/model specification assumption and see how its violation impacts coverage. 

Let's start by reviewing the definition for the linearity assumption.  

```{r q5, include=TRUE}
question("Which ONE of the following statements regarding the linearity assumption is incorrect?",
         answer(sprintf("The linearity assumption says the conditional mean of  $Y |X = x$ does not depend on $X$."), correct = TRUE, message="This is the correct answer because it inaccurately reflects the linearity assumption. The linearity assumption actually posits that the conditional mean of Y given X is a linear function of X. This incorrect statement suggests a lack of relationship between X and Y, which is contrary to the assumption."),
         answer(sprintf("The linearity assumption says the conditional mean of  $Y |X = x$ is linear with respect to $X$. "), message="One phrasing of the linearity assumption is exactly this, that X is related to the conditional mean of Y in a linear fashion through the coefficients."),
         answer("The linearity assumption says that the errors in the population come from a distribution with a mean of zero.", message="This is an alternative phrasing of the linearity assumption, called the mean zero assumption, and is equivalent to option B through properties of linear combinations of Normal random variables."),
         answer("The linearity assumption states that the model as fitted to the sample is in fact the true relationship present in the population.", message="This is another alternative phrasing of the linearity assumption, which says that the fitted model is the true one too, and therefore is not missing any predictors, does not have any predictors that are not needed, and all predictors are in the correct format (e.g. none need to be squared to be the true relationship)."),
         allow_retry = TRUE
)
```

We will now break this assumption in the simulation. 

There are a few ways we can do this:

  1. Have the true relationship contain both predictors but the fitted model contain only one predictor.
  2. Change the true relationship to contain a predictor in a different functional form, e.g. as $X^2$ instead of just $X$.

We will not only look at the impact of this violation on the coverage of our confidence intervals for the coefficients (as before), but also the average value of the estimated coefficients across all runs to better understand the impact. 


### Part 3.1: Omitting a predictor

First, we will violate the assumption by omitting a predictor when we fit the model. We know (because we set it to be true) that the true relationship in the population is $E(Y | X) = 1 + 2X_1 + 3X_2$. 

To violate the assumption, we will only use $X_1$ when we fit the model. Then we can compute the estimated coverage as we did before. We will also store the estimated coefficients from each loop in our simulation so that we can compute the average $\beta$ from all 2000 samples.

First, set up the simulation with a true relationship of $y = 1 + 2*X_1 + 3*X_2 + e$ but a fitted linear relationship of $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x_2$.

```{r linearity1, exercise = TRUE, exercise.lines=26}
# Set up parameters
error_mean = 0
num_runs = 2000
sample_size = 50
error_sd = 1
betas <- matrix(NA, nrow=num_runs, ncol=2)
Beta0CI <- Beta1CI <- matrix(NA, nrow=num_runs, ncol = 2)

for (i in 1:num_runs){
  x1<-  rnorm(sample_size, mean = 1, sd = 5)
  x2<-  rnorm(sample_size, mean = 1, sd = 1)
  e <- rnorm(sample_size, mean = error_mean, sd = error_sd)
  # add the true population relationship here
  y <- _____
  #Let's break the linearity assumption by making it only depends on the value of x2
  model <- lm(__________)
  #Store the coefficients of the model in matrix betas
  betas[i,] <- as.numeric(__________)
  # Store the confidence intervals
  Beta0CI[i,] <- confint(model)[1,]
  Beta1CI[i,] <- confint(model)[2,]
}

```

```{r linearity1-solution, eval=FALSE}
# Set up parameters
error_mean = 0
num_runs = 2000
sample_size = 50
error_sd = 1
betas <- matrix(NA, nrow=num_runs, ncol=2)
Beta0CI <- Beta1CI <- matrix(NA, nrow=num_runs, ncol = 2)

for (i in 1:num_runs){
  x1<-  rnorm(sample_size, mean = 1, sd = 5)
  x2<-  rnorm(sample_size, mean = 1, sd = 1)
  e <- rnorm(sample_size, mean = error_mean, sd = error_sd)
  # add the true population relationship here
  y <- 1 + 2*x1 + 3*x2 +e
  #Let's break the linearity assumption by making it only depends on the value of x2
  model <- lm(y~x2)
  #Store the coefficients of the model in matrix betas
  betas[i,] <- as.numeric(coef(model))
  # Store the confidence intervals
  Beta0CI[i,] <- confint(model)[1,]
  Beta1CI[i,] <- confint(model)[2,]
}
```


```{r linearity1-check}
grade_this_code()
```


```{r linearity1-hint-1}
# ---

# remember that the true relationship is not changing and is
y <- 1 + 2*x1 + 3*x2 + e

# ---
```

```{r linearity1-hint-2}
# ---

# the model we fit using lm contains the violation and should only have one predictor:
lm(y ~ x2)

# ---

# remember that the true relationship is not changing and is
y <- 1 + 2*x1 + 3*x2 + e

# ---
```

```{r linearity1-hint-3}
# ---

# The following command can be used to obtain the coefficients of a model
coef(model)
# which will only give you the estimated coefficients from the model

# ---

# the model we fit using lm contains the violation and should only have one predictor:
lm(y ~ x2)

# ---

# remember that the true relationship is not changing and is
y <- 1 + 2*x1 + 3*x2 + e

# ---
```



Now we can compute the estimated coverage of the 95% confidence intervals that were calculated for each sample in our simulation. This is done the same way as the previous section:

 1. Check if the true value of the coefficient is LARGER than the lower bound of the computed confidence interval (i.e. $a \leq \beta$ or equivalently $\beta \geq a$).
  2. Check if the true value of the coefficient is also SMALLER than the upper bound of the computed confidence interval (i.e. $b \geq \beta$ or equivalently $\beta \leq b$). 
   3. Count/Sum the number of simulated samples where the true value of the coefficient was BOTH larger than the lower bound and smaller than the upper bound of the confidence interval.
  4. Divide by the number of samples you took, i.e. the number of loops you did in your simulation.
  
Let's do this below.

```{r confint3-setup, include=F}
# Set up parameters
error_mean = 0
num_runs = 2000
sample_size = 50
error_sd = 1
betas <- matrix(NA, nrow=num_runs, ncol=2)
Beta0CI <- Beta1CI <- matrix(NA, nrow=num_runs, ncol = 2)

for (i in 1:num_runs){
  x1<-  rnorm(sample_size, mean = 1, sd = 5)
  x2<-  rnorm(sample_size, mean = 1, sd = 1)
  e <- rnorm(sample_size, mean = error_mean, sd = error_sd)
  # add the true population relationship here
  y <- 1 + 2*x1 + 3*x2 + e
  #Let's break the linearity assumption by making it only depends on the value of x2
  model <- lm(y ~ x2)
  #Store the coefficients of the model in matrix betas
  betas[i,] <- as.numeric(coef(model))
  # Store the confidence intervals
  Beta0CI[i,] <- confint(model)[1,]
  Beta1CI[i,] <- confint(model)[2,]
}

```

```{r confint3_1, exercise=TRUE, exercise.setup="confint3-setup"}
# Here is how the coverage is checked for the intercept CI

print(paste0("Coverage for Intercept: ", sum(Beta0CI[,1] <=1 & Beta0CI[,2] >=1)/num_runs))

# Fill in the gaps for the coverage of the slope of X2
print(paste0("Coverage for Slope X2: ", sum(___[,___] <= 3 & ___[,___] >= 3)/___))
```

```{r confint3_1-solution, eval=FALSE}
# Here is how the coverage is checked for the intercept CI

print(paste0("Coverage for Intercept: ", sum(Beta0CI[,1] <=1 & Beta0CI[,2] >=1)/num_runs))

# Fill in the gaps for the coverage of the slope of X2
print(paste0("Coverage for Slope X2: ", sum(Beta1CI[,1] <= 3 & Beta1CI[,2] >= 3)/num_runs))

```

```{r confint3_1-check}
grade_this_code()
```

```{r confint3_1-hint-1}
# ---

# Remember what we called the objects that store our confidence intervals:
Beta0CI
Beta1CI
# each one has 2000 rows of information and 2 columns

# ---
```

```{r confint3_1-hint-2}
# ---

# Remember that each storage object for the confidence intervals has
    # 2000 rows and 2 columns with lower bound in column 1 and upper in column 2
Beta0CI <- Beta1CI <- matrix(NA, nrow=num_runs, ncol = 2)

# ---

# Remember what we called the objects that store our confidence intervals:
Beta0CI
Beta1CI
# each one has 2000 rows of information and 2 columns

# ---
```

```{r q6, include=TRUE}
question("Which confidence interval has reduced coverage (i.e. smaller than 95%)?",
         answer("Neither interval has reduced coverage", message="Check that you have computed the coverage correctly and that you have violated linearity as described."),
         answer(sprintf("The interval for the slope of $X_2$ has reduced coverage"), message="The coverage here should be close to 95%. Check that you have computed the coverage correctly and that you have violated linearity as described."),
         answer("The interval for the intercept has reduced coverage", correct = TRUE, message="You should see that the intercept CI is not quite capturing the truth as often as it should."),
         allow_retry = TRUE
)
```

Let's try to understand why we might be seeing reduced coverage in this situation. To do this, we will take the mean/average of the estimated coefficients from all samples in our simulation. This will give us an idea of whether any of our estimated coefficients appear to be **biased**.


```{r bias1, exercise=TRUE, exercise.setup="confint3-setup"}
# take the mean() of each COLUMN of our matrix of betas
print(paste0("Average value of the estimated intercept: ", mean(___)))
print(paste0("Average value of the estimated slope: ", mean(___)))
```

```{r bias1-solution}
# take the mean() of each COLUMN of our matrix of betas
print(paste0("Average value of the estimated intercept: ", mean(betas[1])))
print(paste0("Average value of the estimated slope: ", mean(betas[2])))
```

```{r bias1-check}
grade_this_code()
```

```{r bias1-hint-1}
# ---

# remember that we stored our estimated coefficients in
betas
# where column 1 stored all the intercepts and column 2 all the slopes

# ---
```

```{r q7, include=TRUE}
question("Based on these averages, which of the following statements is TRUE?",
         answer("The coefficients all appear to be unbiased, i.e. the average is roughly the same as the true parameter value.", message="Double check your simulation, as we should see an issue with bias since we saw an issue with coverage."),
         answer(sprintf("The average value of the estimated slope of $X_2$ is very different from the true value."), message="While it won't be exactly the same, you should see that it is somewhat near the true value of 3. If not, double check your simulation."),
         answer("The estimate of the intercept appears to be biased as it is not near the true value.", correct = TRUE, message="We saw an issue with the coverage of the 95% confidence interval for the intercept, and the cause of that is that the interval is not actually being centred at the correct place due to this bias in the estimates."),
        allow_retry = TRUE
)
```



### Part 3.2: Incorrect functional form

We can also violate this assumption by changing the functional form of at least one of the predictors. Recall that the linearity assumption states that a linear relationship exists between the response and predictors, and that it specifically includes predictors as they have been added to the fitted model. This means that if we fit a model assuming that the true relationship is $$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon$$ but in fact the true relationship is $$Y = \beta_0 + \beta_1 X_1^2 + \beta_2 X_2^2  + \epsilon,$$ we would violate the linearity assumption.

Therefore, we can also violate the linearity assumption by creating this situation. We will do this below by squaring each predictor when we generate the responses in our sample.

```{r linearity2, exercise = TRUE, exercise.lines = 25}
# Set up parameters
error_mean = 0
num_runs = 2000
sample_size = 50
error_sd = 1
betas <- matrix(NA, nrow=num_runs, ncol=3)

for (i in 1:num_runs){
  x1<-  rnorm(sample_size, mean = 1, sd = 5)
  x2<-  rnorm(sample_size, mean = 1, sd = 1)
  e <- rnorm(sample_size, mean = error_mean, sd = error_sd)
  # Break the linearity assumption by squaring each predictor
  y <- __________
  # fit a model as we have previously assuming that linearity holds
  model <- lm(__________)
  # Store the coefficients of the model in matrix betas
  betas[i,] <- as.numeric(__________)
}

# Print the mean of the estimated coefficients
print(paste0("Average value of the estimated intercept: ", mean(betas[,1])))
print(paste0("Average value of the estimated slope of X1: ", mean(betas[,2])))
print(paste0("Average value of the estimated slope of X2: ", mean(betas[,3])))
```

```{r linearity2-solution, eval=FALSE}
# Set up parameters
error_mean = 0
num_runs = 2000
sample_size = 50
error_sd = 1
betas <- matrix(NA, nrow=num_runs, ncol=3)

for (i in 1:num_runs){
  x1<-  rnorm(sample_size, mean = 1, sd = 5)
  x2<-  rnorm(sample_size, mean = 1, sd = 1)
  e <- rnorm(sample_size, mean = error_mean, sd = error_sd)
  # Break the linearity assumption by squaring each predictor
  y <- 1 + 2*x1**2 + 3*x2**2 + e
  # fit a model as we have previously assuming that linearity holds
  model <- lm(y~x1 + x2)
  # Store the coefficients of the model in matrix betas
  betas[i,] <- as.numeric(coef(model))
}

# Print the mean of the estimated coefficients
print(paste0("Average value of the estimated intercept: ", mean(betas[,1])))
print(paste0("Average value of the estimated slope of X1: ", mean(betas[,2])))
print(paste0("Average value of the estimated slope of X2: ", mean(betas[,3])))
```

```{r linearity2-check}
grade_this_code()
```

```{r linearity2-hint-1}
# ---

# we want to square the predictors in the true relationship like below
y <- 1 + 2*x1**2 + 3*x2**2 + e

# ---
```

```{r linearity2-hint-2}
# ---

# the fitted model would assume that the true relationship is
y <- 1 + 2*x1 + 3*x2 + e

# ---

# we want to square the predictors in the true relationship like below
y <- 1 + 2*x1**2 + 3*x2**2 + e

# ---
```

```{r linearity2-hint-3}
# ---

# we can extract the estimated coefficients using
coef(model)

# ---

# the fitted model would assume that the true relationship is
y <- 1 + 2*x1 + 3*x2 + e

# ---

# we want to square the predictors in the true relationship like below
y <- 1 + 2*x1**2 + 3*x2**2 + e

# ---
```


```{r q8, include=TRUE}
question("What do you notice about the average value of the estimated coefficients?",
         answer("On average, the estimated coefficients are similar to the true value", message="Double check how you've set up the true relationship that generates the data and the fitted model. They should be different."),
         answer("Only one of the estimated coefficients on average is different that the true value.", message="Double check how you've set up the true relationship that generates the data and the fitted model. They should be different."),
         answer("On average, the estimated coefficients are not estimating the correct true value.", correct = TRUE, message="This is because the true relationship has the preedictors squared but the fitted model doesn't. So the coefficients being estimated are not those that are part of the true relationship."),
         answer("The average values of the estimated coefficients suggest that the coverage of the confidence intervals should be close to 95%.", message="Because linearity is violated and thus the estimators are not unbiased, we would not expect the coverage of the intervals to align with 95%."),
         allow_retry = TRUE
)
```



## Section 4 - Violated Independent/Uncorrelated Errors
As we saw from the previous two sections, if we violate either the assumptions of linearity or the assumption of constant variance, we end up with estimates of coefficients that don't behave as we would expect them to. We end up with incorrect coverage of our confidence intervals and bias in our estimates of the coefficients.

In this part, we will examine the independence/uncorrelated errors assumption to see if violation of this assumptions will yield similar results. Let's start by reviewing the definition of the independence/uncorrelated errors assumption.  

```{r q9, include=TRUE}
question("Which statements is TRUE regarding the independent/uncorrelated errors assumption?",
         answer(sprintf("The relationship $\\epsilon_{k+1} = \\epsilon_{k}$ for $k=1,2,3,...$ satisfies the independence assumption."), message="The assumption does not require that all errors be equal, rather that they have no relationship to each other."),
         answer(sprintf("The independence assumption states that $Corr(\\epsilon_i, \\epsilon_j) = 0$ for all $i \\neq j$."), correct = TRUE, message="The assumption requires that all errors be uncorrelated with one another, namely have a correlation of 0."),
         answer(sprintf("The independence assumption states that the observed predictors $X_1, X_2, X_3...$ are independent/uncorrelated of each other."), message="The assumption is about the errors, not the predictors."),
         allow_retry=TRUE
)
```

### Part 4.1: Examining the Coverage when Assumptions Hold

We will study the impact of violating the uncorrelated/independent errors assumption by again looking at the coverage of the confidence intervals to understand what changes when this assumption no longer holds.

To begin, we can create a simulation when all assumptions hold where, for each run, we will save the confidence intervals. Fill in the blanks below to get an idea of how the sampling distribution behaves normally.

```{r corr1, exercise = TRUE, exercise.lines = 25}
# Set up parameters
num_runs = 2000
sample_size = 100
# setup storage
Beta0CI <- Beta1CI <- Beta2CI <- matrix(NA, nrow=num_runs, ncol = 2)

for(i in 1:num_runs){
  # generate the sample
  x1<- rnorm(sample_size, mean = 1, sd = 5)
  x2<- rnorm(sample_size, mean = 1, sd = 1)
  e <- ____
  y <- 1+2*x1+3*x2+e
  # fit the model
  model <- lm(y~x1+x2)
  # store each confidence interval
  Beta0CI[i,] <- confint(model)[1,]
  Beta1CI[i,] <- confint(model)[2,]
  Beta2CI[i,] <- confint(model)[3,]
}

# compute the coverage of the confidence intervals
print(paste0("Coverage for Intercept: ", sum(Beta0CI[,1] <=__ & Beta0CI[,2] >=__)/num_runs))
print(paste0("Coverage for Slope X1: ", sum(Beta1CI[,1] <=__ & Beta1CI[,2] >=__)/num_runs))
print(paste0("Coverage for Slope X2: ", sum(Beta2CI[,1] <=__ & Beta2CI[,2] >=__)/num_runs))
```

```{r corr1-solution, eval=FALSE}
# Set up parameters
num_runs = 2000
sample_size = 100
# setup storage
Beta0CI <- Beta1CI <- Beta2CI <- matrix(NA, nrow=num_runs, ncol = 2)

for(i in 1:num_runs){
  # generate the sample
  x1<- rnorm(sample_size, mean = 1, sd = 5)
  x2<- rnorm(sample_size, mean = 1, sd = 1)
  e <- rnorm(sample_size, mean = 0, sd = 1)
  y <- 1+2*x1+3*x2+e
  # fit the model
  model <- lm(y~x1+x2)
  # store each confidence interval
  Beta0CI[i,] <- confint(model)[1,]
  Beta1CI[i,] <- confint(model)[2,]
  Beta2CI[i,] <- confint(model)[3,]
}

# compute the coverage of the confidence intervals
print(paste0("Coverage for Intercept: ", sum(Beta0CI[,1] <=1 & Beta0CI[,2] >=1)/num_runs))
print(paste0("Coverage for Slope X1: ", sum(Beta1CI[,1] <=2 & Beta1CI[,2] >=2)/num_runs))
print(paste0("Coverage for Slope X2: ", sum(Beta2CI[,1] <=3 & Beta2CI[,2] >=3)/num_runs))

```

```{r corr1-check}
grade_this_code()
```

```{r corr1-hint-1}
# ---

# Remember that we are generating a sample of Normal errors with mean 0 and sd 1.
# we can use the same code as for generating the predictors, just with different values, e.g.
x2<- rnorm(sample_size, mean = 1, sd = 1)

# ---
```

```{r corr1-hint-2}
# ---

# remember that we had the following parameters set up in previous sections
error_mean = 0
error_sd = 1

# ---

# Remember that we are generating a sample of Normal errors with mean 0 and sd 1.
# we can use the same code as for generating the predictors, just with different values, e.g.
x2<- rnorm(sample_size, mean = 1, sd = 1)

# ---
```

```{r corr1-hint-3}
# ---

# our true relationship with the true intercept, slope of x1 and slope of x2 is
y <- 1+2*x1+3*x2+e

# ---

# remember that we had the following parameters set up in previous sections
error_mean = 0
error_sd = 1

# ---

# Remember that we are generating a sample of Normal errors with mean 0 and sd 1.
# we can use the same code as for generating the predictors, just with different values, e.g.
x2<- rnorm(sample_size, mean = 1, sd = 1)

# ---
```


```{r q10, include=TRUE}
question("Is the coverage of the confidence intervals about what you expected?",
         answer("Yes", correct = TRUE, message="All our assumptions hold, so we should observe no issues with the coverage. It should be around 95%."),
         answer("No", message="All our assumptions hold, so we should observe no issues with the coverage. It should be around 95%. If this is not what you saw, double check that you have filled in the gaps to ensure the assumptions hold."),
         allow_retry = TRUE
)
```

### Part 4.2: Adding Correlation to the Errors

To violate uncorrelated errors, we will need to modify how the random errors are generated. The function we have been using to get independent errors from a Normal distribution, `rnorm()` always gives us independent errors. We will need to use a different random normal generator which can generate correlated/dependent random errors to violate this assumption. The function that will let us do this is `mvrnorm()` which can take in a covariance matrix to create correlated variables.

This means that we need to create a **covariance matrix**. A covariance matrix is a way to concisely describe the variance of multiple random variables along with their covariance. Recall that when we have independent random variables, the covariance between any two will be $Cov(X_i, X_j) = 0, i \neq j$. So we could write the covariance matrix between 3 independent random variables as

$$\Sigma = \begin{pmatrix}
\sigma_1^2 & 0 & 0\\
0 & \sigma_2^2 & 0\\
0 & 0 & \sigma_3^2
\end{pmatrix},$$

where the diagonal elements are the variances of each random variable. If the random variables have a covariance, then the off-diagonal elements will no longer be zero and then the random variables are no longer independent.

Since we will be generating a sample of 100 errors, we don't want to create a covariance matrix with 100 rows and columns. Instead we can make a covariance matrix of size 4x4 and just sample random variables in sets of 4. By doing this, we create a dependence between 4 random variables/errors at a time in order to create a situation that violates the uncorrelated errors assumption.

Our covariance matrix will be

$$\Sigma = \begin{pmatrix}
1 & 0.75 & 0.5 & 0.25\\
0.75 & 1 & 0.75 & 0.5\\
0.5 & 0.75 & 1 & 0.75\\
0.25 & 0.5 & 0.75 & 1
\end{pmatrix},$$
and we will need to also set a mean for the multivariate normal distribution that will create these errors. Since we only want to violate correlated errors, we will set the mean to be 0. Then we will be generating errors from a multivariate distribution that looks like
$$\begin{pmatrix} \epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \epsilon_4 \end{pmatrix} \sim N_4 \left(\begin{pmatrix} 0 \\ 0 \\ 0 \\ 0 \end{pmatrix}, \begin{pmatrix}
1 & 0.75 & 0.5 & 0.25\\
0.75 & 1 & 0.75 & 0.5\\
0.5 & 0.75 & 1 & 0.75\\
0.25 & 0.5 & 0.75 & 1
\end{pmatrix}\right). $$

Since this only gives us 4 errors at a time, and we want a sample of 100, we will sample this distribution 25 times (25 times 4 = 100). In the code below, fill in the diagonal elements of the covariance matrix like above, add in a value for the mean of the errors in `mvrnorm()` and tell this function what the covariance matrix is called.

```{r corr2-setup, include=F}
set.seed(1234)
```

```{r corr2, exercise = TRUE, exercise.lines = 30, exercise.setup="corr2-setup"}
# Set up parameters
num_runs = 5000
sample_size = 100
# set up the storage
Beta0CI <- Beta1CI <- Beta2CI <- matrix(NA, nrow=num_runs, ncol = 2)
# Build the covariance matrix by row
row1 <- c(___, 0.75, 0.5, 0.25)
row2 <- c(0.75, ___, 0.75, 0.5)
row3 <- c(0.5, 0.75, ___, 0.75)
row4 <- c(0.25, 0.5, 0.75, ___)
# Create covariance matrix such that all errors corresponding to unique values of x1 will be correlated
Sigma = matrix(rbind(row1, row2, row3, row4), nrow=4, ncol=4)
library(MASS)
for(i in 1:num_runs){
  x1<-  rnorm(sample_size, mean = 1, sd = 5)
  x2<-  rnorm(sample_size, mean = 1, sd = 1)
  e <- as.vector(mvrnorm(n=25, mu = rep(___, times=4), Sigma = ____))
  y <- 1+2*x1+3*x2+e
  model <- lm(y~x1+x2)
  Beta0CI[i,] <- confint(model)[1,]
  Beta1CI[i,] <- confint(model)[2,]
  Beta2CI[i,] <- confint(model)[3,]
}
  
# compute the coverage of the confidence intervals
print(paste0("Coverage for Intercept: ", sum(Beta0CI[,1] <=1 & Beta0CI[,2] >=1)/num_runs))
print(paste0("Coverage for Slope X1: ", sum(Beta1CI[,1] <=2 & Beta1CI[,2] >=2)/num_runs))
print(paste0("Coverage for Slope X2: ", sum(Beta2CI[,1] <=3 & Beta2CI[,2] >=3)/num_runs))
```

```{r corr2-solution, eval=FALSE}
# Set up parameters
num_runs = 5000
sample_size = 100
# set up the storage
Beta0CI <- Beta1CI <- Beta2CI <- matrix(NA, nrow=num_runs, ncol = 2)
# Build the covariance matrix by row
row1 <- c(1, 0.75, 0.5, 0.25)
row2 <- c(0.75, 1, 0.75, 0.5)
row3 <- c(0.5, 0.75, 1, 0.75)
row4 <- c(0.25, 0.5, 0.75, 1)
# Create covariance matrix such that all errors corresponding to unique values of x1 will be correlated
Sigma = matrix(rbind(row1, row2, row3, row4), nrow=4, ncol=4)
library(MASS)
for(i in 1:num_runs){
  x1<-  rnorm(sample_size, mean = 1, sd = 5)
  x2<-  rnorm(sample_size, mean = 1, sd = 1)
  e <- as.vector(mvrnorm(n=25, mu = rep(0, times=4), Sigma = matrix(rbind(row1, row2, row3, row4), nrow=4, ncol=4)))
  y <- 1+2*x1+3*x2+e
  model <- lm(y~x1+x2)
  Beta0CI[i,] <- confint(model)[1,]
  Beta1CI[i,] <- confint(model)[2,]
  Beta2CI[i,] <- confint(model)[3,]
}
  
# compute the coverage of the confidence intervals
print(paste0("Coverage for Intercept: ", sum(Beta0CI[,1] <=1 & Beta0CI[,2] >=1)/num_runs))
print(paste0("Coverage for Slope X1: ", sum(Beta1CI[,1] <=2 & Beta1CI[,2] >=2)/num_runs))
print(paste0("Coverage for Slope X2: ", sum(Beta2CI[,1] <=3 & Beta2CI[,2] >=3)/num_runs))
```

```{r corr2-check}
grade_this_code()
```

```{r corr2-hint-1}
# ---

# the covariance matrix should have diagonal elements equal to 1
row1 <- c(1, 0.75, 0.5, 0.25)
row2 <- c(0.75, 1, 0.75, 0.5)
row3 <- c(0.5, 0.75, 1, 0.75)
row4 <- c(0.25, 0.5, 0.75, 1)

# ---
```

```{r corr2-hint-2}
# ---

# the mean we want for the errors is 0 and since we generate 4 errors at once, we want to repeat this value 4 times.
# to repeat something many times we use the code
rep(0, times=4)
# which repeats the value 0 four times.

# ---

# the covariance matrix should have diagonal elements equal to 1
row1 <- c(1, 0.75, 0.5, 0.25)
row2 <- c(0.75, 1, 0.75, 0.5)
row3 <- c(0.5, 0.75, 1, 0.75)
row4 <- c(0.25, 0.5, 0.75, 1)

# ---
```

```{r corr2-hint-3}
# ---

# see line 12 in the code chunk for the label we gave to our covariance matrix:
Sigma = matrix(rbind(row1, row2, row3, row4), nrow=4, ncol=4)

# ---

# the mean we want for the errors is 0 and since we generate 4 errors at once, we want to repeat this value 4 times.
# to repeat something many times we use the code
rep(0, times=4)
# which repeats the value 0 four times.

# ---

# the covariance matrix should have diagonal elements equal to 1
row1 <- c(1, 0.75, 0.5, 0.25)
row2 <- c(0.75, 1, 0.75, 0.5)
row3 <- c(0.5, 0.75, 1, 0.75)
row4 <- c(0.25, 0.5, 0.75, 1)

# ---
```

```{r q11, include=TRUE}
question("What do you notice about the coverage?",
         answer("At least one confidence interval does not have the correct coverage.", message="We should notice that the intercept has lower coverage than it should, indicating that having correlated errors impacts this property.", correct = TRUE),
         answer("All of the confidence intervals do not have the correct coverage.", message="This type of violation does not usually impact all confidence intervals, and this should be supported by the values you compare between section 4.1 and 4.2."),
         answer("All of the confidence intervals have the correct coverage.", message="This type of violation usually impacts at least one of the confidence intervals, and this should be supported by the values you compare between section 4.1 and 4.2."),
         answer("I'm not sure what is happening to the coverage", message="That's ok. Be sure to double check that you've filled in the code correrctly by looking at the hints. Then compare to what you saw in section 4.1."),
         allow_retry = TRUE
)
```


## Section 5 - Violated Normality
In our final section, we will examine how the normality assumption relates to the sampling distributions of our estimators of the coefficients. Recall that a **<mark>sampling distribution</mark>** describes how estimates of coefficients will change depending on the sample used. We can mimic the concept of a sampling distribution with our simulation, since we take many samples when we loop through our code.

### Part 5.1: Sampling Distribution of the Coefficient Estimators
Let's start by considering which elements of our linear regression model are assumed to be Normally distributed due to this assumption.  

```{r q12, include=TRUE}
question("Which quantity should NOT have a Normal distribution, assuming the Normality assumption is not violated?",
         answer("The population responses", message="The Normality assumption can equivalently be written for the responses or the errors."),
         answer("The population errors", message="The Normality assumption can equivalently be written for the responses or the errors."),
         answer("The estimators of the coefficients", message="If the Normality assumption holds, then by properties of Normal random variables, the distribution of the estimators will also be Normal."),
         answer("The population predictors", message="In linear regression, the predictors are fixed, not random, and therefore do not have a distribution.", correct=T),
         allow_retry = TRUE
)
```

To understand what the sampling distribution looks like when assumptions hold, we will modify our simulation to look at the different values of the coefficients that are estimated in each loop. We can store these, like we did the confidence intervals, and plot them to get a visual representation of an estimated sampling distribution.

```{r norm1, exercise = TRUE, exercise.lines = 25}
# Set up parameters
num_runs = 2000
sample_size = 25
# setup storage
betas <- matrix(NA, nrow=num_runs, ncol = ___)

for(i in 1:num_runs){
  # generate the sample
  x1<- rnorm(sample_size, mean = 1, sd = 5)
  x2<- rnorm(sample_size, mean = 1, sd = 1)
  e <- ____
  y <- 1+2*x1+3*x2+e
  # fit the model
  model <- lm(y~x1+x2)
  # store each coefficient
  betas[i,] <- as.numeric(___)
}

# plot a histogram of each coefficient
par(mfrow=c(1,3))
hist(betas[,1], main="Intercept Distribution", xlab="Estimated Intercepts")
hist(betas[,2], main="X1 Slope Distribution", xlab="Estimated X1 Slopes")
hist(betas[,3], main="X2 Slope Distribution", xlab="Estimated X2 Slopes")
```

```{r norm1-solution, eval=FALSE}
# Set up parameters
num_runs = 2000
sample_size = 25
# setup storage
betas <- matrix(NA, nrow=num_runs, ncol = 3)

for(i in 1:num_runs){
  # generate the sample
  x1<- rnorm(sample_size, mean = 1, sd = 5)
  x2<- rnorm(sample_size, mean = 1, sd = 1)
  e <- rnorm(sample_size, mean = 0, sd = 1)
  y <- 1+2*x1+3*x2+e
  # fit the model
  model <- lm(y~x1+x2)
  # store each coefficient
  betas[i,] <- as.numeric(coef(model))
}

# plot a histogram of each coefficient
par(mfrow=c(1,3))
hist(betas[,1], main="Intercept Distribution", xlab="Estimated Intercepts")
hist(betas[,2], main="X1 Slope Distribution", xlab="Estimated X1 Slopes")
hist(betas[,3], main="X2 Slope Distribution", xlab="Estimated X2 Slopes")
```

```{r norm1-check}
grade_this_code()
```

```{r norm1-hint-1}
# --- 

# the number of columns must be the same as the number of coefficients

# --- 
```

```{r norm1-hint-2}
# --- 

# remember that we had the following parameters set up in previous sections
error_mean = 0
error_sd = 1
# and that our assumption is that the errors are Normal like the predictors

# --- 

# the number of columns must be the same as the number of coefficients

# --- 
```

```{r norm1-hint-3}
# --- 

# To extract the estimated coefficients, use
coef(model)

# --- 

# remember that we had the following parameters set up in previous sections
error_mean = 0
error_sd = 1
# and that our assumption is that the errors are Normal like the predictors

# --- 

# the number of columns must be the same as the number of coefficients

# --- 
```

```{r normq, include=TRUE}
question("What do you notice about the sampling distributions?",
         answer("The sampling distributions appear to be non-Normal shaped.", message="By properties of Normals, they should look approximately Normal."),
         answer("The sampling distributions appear to be centred at the true values of the coefficients.", message="If all assumptions hold, our estimates should be unbiased, meaning that the expected value is the true value.", correct=T),
         answer("The estimators of the coefficients appear to be biased.", message="If all assumptions hold, our estimates should be unbiased, meaning that the expected value is the true value."),
         answer("We would expect the coverage of the confidence intervals to be much less than 95%.", message="If all assumptions hold, as they do here, the coverage should be approximately 95%."),
         allow_retry = TRUE
)
```

### Part 5.2: Violating the Normality Assumption

To violate Normality, we simply need to change the distribution that is generating the errors to that is not a Normal distribution, i.e. we need to use a function different from `rnorm()`. While any distribution that is not Normal will violate this assumption, the effect is more prominent if we pick a distribution that is far from Normal, such as the log-Normal distribution. The log-Normal distribution is a highly skewed distribution that is based off the Normal. If a random variable X is log-Normally distributed, then then $ln(X)$ will be Normally distributed.

Like the Normal distribution, the log-Normal distribution has both a mean and a standard deviation as parameters. We will use `rlnorm()` to generate errors from a log-Normal, and we will specify the mean to be 0 and the standard deviation to be 1. The `rlnorm()` function uses the same syntax as the `rnorm()` function, so you can copy the syntax from the code that generates the predictors. Fill in the blanks below to see how a violation of Normality affects the sampling distributions of the coefficients.

```{r norm2, exercise = TRUE, exercise.lines = 25}
# Set up parameters
error_mean = 0
num_run = 2000
sample_size = 25
error_sd = 1
betas <- matrix(NA, nrow=num_run, ncol=3)

for(i in 1:num_run){
  x1<- rnorm(sample_size, mean = 1, sd = 5)
  x2<- rnorm(sample_size, mean = 1, sd = 1)
  # Use a log-Normal distribution
  e <- __________
  y <- 1+2*x1+3*x2+e
  model <- lm(y~x1+x2)
  betas[i,] <- as.numeric(_____)
}

# plot the distribution of betas
par(mfrow=c(1,3))
hist(betas[,1], main="Intercept Distribution", xlab="Intercept")
hist(betas[,2], main="X1 Slope Distribution", xlab="X1 Slope")
hist(betas[,3], main="X2 Slope Distribution", xlab="X2 Slope")

```

```{r norm2-solution, eval=FALSE}
# Set up parameters
error_mean = 0
num_run = 2000
sample_size = 25
error_sd = 1
betas <- matrix(NA, nrow=num_run, ncol=3)

for(i in 1:num_run){
  x1<- rnorm(sample_size, mean = 1, sd = 5)
  x2<- rnorm(sample_size, mean = 1, sd = 1)
  # Use a log-Normal distribution
  e <- rlnorm(sample_size, meanlog = 0, sdlog = 1)
  y <- 1+2*x1+3*x2+e
  model <- lm(y~x1+x2)
  betas[i,] <- as.numeric(coef(model))
}

# plot the distribution of betas
par(mfrow=c(1,3))
hist(betas[,1], main="Intercept Distribution", xlab="Intercept")
hist(betas[,2], main="X1 Slope Distribution", xlab="X1 Slope")
hist(betas[,3], main="X2 Slope Distribution", xlab="X2 Slope")
```

```{r norm2-check}
grade_this_code()
```

```{r norm2-hint-1}
# ---

# The syntax of rlnorm function that generates log Normal data is
rlnorm(sample_size, meanlog = mean, sdlog = sd)
# and we want meanlog = 0 and sdlog = 1

# ---
```

```{r norm2-hint-2}
# ---

# To extract the estimated coefficients, use
coef(model)

# ---

# The syntax of rlnorm function that generates log Normal data is
rlnorm(sample_size, meanlog = mean, sdlog = sd)
# and we want meanlog = 0 and sdlog = 1

# ---
```


```{r q13, include=TRUE}
question("What about the sampling distribution has changed now that Normality is violated?",
         answer("The sampling distributions are more bell-shaped now than when Normality held.", message="The opposite should be true, as we are generating our errors from a very highly skewed distribution. So you should see a less bell-shaped appearance than before."),
         answer("The mean of all sampling distributions are still around the true value of the coefficients.", message="We should observe a slightly different mean in at least one of the sampling distributions, most likely the intercept."),
         answer("The sampling distributions exhibit more variability in the estimated coefficients.", correct = TRUE, message="The axes have changed dramatically due to sampling from a highly skewed distribution leading us to occasionally get some very extreme estimates of the coefficients."),
         allow_retry = TRUE
)
```



## Visualizing Assumption Violations

In this section, we use plots to visualize assumption violations, learning what to look out for when attempting to fit a linear model.

### Understanding Homoscedasticity
As we learnt, for homoscedasticity, the variance of the error term must be constant and consequently independent of the predictors. To determine if the assumption holds, we tend to use diagnostic plots such as Residual Plots where Residual values ( ${r_i = y_i - \hat{y}_i}$ ) are plotted against the Fitted values (X). If the assumption holds, you should expect:

1. The residuals should look random and display no clear trends.

2. No identifiable patterns in the plot. 

3. The vertical spread of the residuals should not increase or decrease as the fitted values increase.

In the plot below, the slider allows you to adjust the variance for the error term. For the purpose of this exercise, we define the variance as 

$$\sigma_e^2 = 1 + \frac{a(x - \bar{x})^2}{s_x}$$

such that for a variance multiplier i.e $a > 0$, the variance of the error term becomes dependent on the fitted values.  

```{r, context="server"}
# First interactive exercise (Homoscedasticity)
output$variancePlot <- renderPlot({
  set.seed(302) # Ensure reproducibility

  # Simulated data
  x <- rnorm(500)
  
  # Control the level of heteroscedasticity through input$varianceMultiplier
  variance <- 1 + input$varianceMultiplier * (x - mean(x))^2 / sd(x)^2
  
  # Generate errors based on the specified variance
  e <- rnorm(500, sd = sqrt(variance))
  
  y <- 1 + 4 * x + e

  model <- lm(y ~ x)
  residuals <- resid(model)
  x_range <- c(-10, 11)
  y_range <- c(-8, 8)
  
  # Plot residuals vs. fitted values
  ggplot(NULL, aes(fitted(model), residuals)) +
    geom_point() +
    geom_hline(yintercept = 0, col = 'red', linetype = "dashed") +
    xlim(x_range[1], x_range[2]) + # Set fixed x-axis limits
    ylim(y_range[1], y_range[2]) + # Set fixed y-axis limits
    labs(x = "Fitted Values (x)", y = "Residuals",
         title = "Residual Plot with Non-constant Variation in Error Terms") +
    theme_minimal()
})
```

   
```{r, context="render", echo=FALSE}
fluidPage(
   # titlePanel("Violating Homoscedasticity Assumption"),
  tags$h1("Violating Homoscedasticity Assumption", style = "font-size: 18px;"),
    
    fluidRow(
        plotOutput("variancePlot")
    ),
    fluidRow(
        sliderInput("varianceMultiplier", 
                    "Heteroscedasticity Multiplier:", 
                    min = 0, 
                    max = 10, 
                    value = 0, 
                    step = 2)
    ),
    
    hr())
```
In the plot above, note that while the observed hourglass pattern in the residual plot indicates heteroscedasticity, other patterns also indicate the violation e.g. a fanning pattern. 


### Understanding Linearity
As we learnt, it is important to ensure that the relationship between the predictor variable (X) and the response variable (Y) is linear when applying a linear model. This assumption underpins the reliability of our regression models. So, what should you be on the lookout for when evaluating the linearity assumption?

1. The Y-X scatter plot should show that X and Y follows a linear pattern, such that changes in X should result in proportional changes in Y. 

2. Alternatively, for Residuals vs. Fitted Values Plots, any curved or bending trends in the residuals may be an indication of ill-fitted relationship. Such patterns might suggest that our chosen model fails to capture the true relationship. 

In the interactive plot below, you can experiment with different model types (linear or quadratic) and observe how well the model fits the data. Keep note of the R-squared value and determine if they align with our expectations of linearity.


```{r, context="server"}

library(shiny)
#Second interactive exercise (Linearity)

set.seed(301)
x <- runif(100, 0, 10 )
data <- data.frame(
    x = x,
    y = 1 + 5 * x^3 + rnorm(100, 0, 200)  
)

fittedModel <- reactive({
  if (input$modelType == "Linear") {
    lm(y ~ x, data = data)
  } else if (input$modelType == "Quadratic") {
    lm(y ~ poly(x, 2), data = data)
  } else{
    lm(y ~ poly(x, 3), data = data)
  }
})

 y_range <- c(-800, 5000)

output$modelPlot <- renderPlot({
  model <- fittedModel()
  ggplot(data, aes(x = x, y = y)) +
    geom_point() +
    geom_smooth(method = "lm", formula = y ~ poly(x, 
      ifelse(input$modelType == "Linear", 1, 
        ifelse(input$modelType == "Quadratic", 2, 
          ifelse(input$modelType == "Cubic", 3, 1)))), 
      se = FALSE, colour = "red") +
    labs(title = paste("Model Type:", input$modelType), x = "X", y = "Y") +  ylim(y_range[1], y_range[2]) +
    theme_minimal()
})


output$modelStats <- renderTable({
  model <- fittedModel()
  req(input$modelType)
  df <- broom::glance(model)
  df[, c("r.squared", "adj.r.squared")]
})


```

```{r, context="render", echo = FALSE}
 fluidPage(
  # titlePanel("Violating Linearity Assumption"),
     tags$h1("Violating Linearity Assumption", style = "font-size: 18px;"),

    fluidRow(
        plotOutput("modelPlot")
    ),
    fluidRow(
        selectInput("modelType", "Select Model Type for Linearity Exercise:", 
                    choices = c("Linear", "Quadratic", "Cubic"))
    ),
    fluidRow(
        tableOutput("modelStats")
    )
)

```

In the plot above, we observe that the true relationship in non-linear, and fitting the data to a linear model is inappropriate.

### Understanding Independence Among Predictors
As we learnt, multicollinearity can influence coefficient estimates and affect the validity of the model. If predictors are correlated with one another, our model may fail to capture the unique contributions of eahc predictor. 

So, what should you be on the lookout for when evaluating the Independence among predictors assumption?

1. The X-X scatter plots, across all combinations of predictor pairings, should show little to no discernible patterns with respect to the apparent relationship. In particular, watch out for snake like patterns! They often indicate high levels of multicollinarity  

2. Alternatively, a low (near zero) correlation coefficient would indicate a weak or no linear relationship among the predictors. 

In the plot below, you may observe occurrences of multicollinearity.


```{r, context="server"}

output$independencePlot <- renderPlot({
  set.seed(302) # Ensure reproducibility
  
  # Simulated data
  x1 <- rnorm(500)
  x2 <- x1 + rnorm(500, sd = 0.5)  # correlated with x1
  x3 <- rnorm(500)  # independent
  data <- data.frame(x1, x2, x3)
  
  # Create a scatter plot matrix
  pairs(data, main = "Scatter Plot Matrix for Independence Violation",
        pch = 21, lower.panel = NULL)
})
```

```{r, context="render", echo = FALSE}
 fluidPage(
     tags$h1("Violating Independence Among Predictors Assumption (Multicollinearity)", style = "font-size: 18px;"),

    fluidRow(
        plotOutput("independencePlot")
    )
)

```

```{r q15, include=TRUE}
question("In the plot above, is there an occurance of multicollinearity?",
         answer("The relationship between Predictors X1 and X3 indicate multicollinearity.", message="Predictors X1 and X3 appear to be unrelated as expressed by the random scatter. Try again!"),
         answer("The relationship between Predictors X1 and X2 indicate multicollinearity.", correct = TRUE, message="We observe the descernible pattern of a linear relationship among predictors X1 and X2 such that they are influenced by one another."),
         answer("The relationship between Predictors X2 and X3 indicate multicollinearity.", message="Predictors X2 and X3 appear to be unrelated as expressed by the random scatter. Try again!"),
         answer("There is no occurance of multicollinearity.", message="Two of the predictors appear to influence one another. Try again!"),
         allow_retry = TRUE
)
```

Great job! Now that you know what to look for in the plots, you are better equipped to identify when an assumption is violated. Keep up the good work!




## Conclusion

With any statistical tool you might use, it's important to recognize that they all depend on certain assumptions being true regarding where the data originated. Since we only see a sample of that population, we always need to check that these assumptions hold, in order for valid inference about that population to be made. The goal of this module was to get you to see that when assumptions do not hold, our tools do not tell us the correct information, which can lead us to make mistakes in how we use those tools. We focused primarily on looking at the impact on coverage and on the sampling distribution, but other areas of inference would likewise be affected.

```{r q14, include=TRUE}
question("What types of mistakes could we make if we use inferential tools without checking assumptions?",
         answer("We might conclude an effect is significant when it isn't.", message="This is true since p-values are directly linked to the sampling distribution. But it's not the only true option."),
         answer("We might obtain an estimate of a quantity that is far from being true.", message="This is true since we've seen that violated assumptions can lead to biased estimation. But it's not the only true option."),
         answer("We might have more confidence in our results and process than we should.", message="This is true since we saw that violated assumptions lead to lower coverage. meaning we will have more confidence than we should. But it's not the only true option."),
         answer("All of the options are potential mistakes.", correct = TRUE, message="Bias means the centre of the sampling distribution isn't in the correct place, which means our confidence intervals are less likely to capture the truth. Further, if sampling distributions are not as they should be, we will compute incorrect p-values and thus make decisions regarding significance that may be incorrect."),
         allow_retry = TRUE
)
```

Hopefully, you see how important it is to verify that assumptions hold in all inferential tools. There are many ways to verify the assumptions and to correct or at minimum improve them if needed. But it is most important to know that statistical tools depend on assumptions and to recognize that if assumptions do not hold, the statistical tool should not be used. 

## Wrap-Up

Congratulations -- You have successfully completed this learning module! By now, you should be familiar with:


* How violating each of the linear regression assumptions impacts results -- specifically confidence intervals for coefficients

* How to set up a simulation in R